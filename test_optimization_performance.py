"""
Phase 2 ì„±ëŠ¥ ìµœì í™” í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬ ë° API ìµœì í™” ê²€ì¦

í…ŒìŠ¤íŠ¸ í•­ëª©:
1. BM25 + Vector ë³‘ë ¬ ì‹¤í–‰ ì„±ëŠ¥
2. ë°°ì¹˜ ì²˜ë¦¬ ì²˜ë¦¬ëŸ‰ ì¸¡ì •
3. ë©”ëª¨ë¦¬ ìµœì í™” íš¨ê³¼ í™•ì¸
4. ë™ì‹œì„± ì œì–´ í…ŒìŠ¤íŠ¸
5. ì „ì²´ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¹„êµ
"""

import asyncio
import time
import logging
import statistics
from typing import List, Dict, Any
import httpx
import json
import random
import sys
import os

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# ìµœì í™” ëª¨ë“ˆë“¤ import
try:
    from apps.api.optimization import (
        get_async_optimizer,
        get_batch_search_processor,
        get_memory_monitor,
        get_concurrency_controller,
        PERFORMANCE_TARGETS
    )
    OPTIMIZATION_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Optimization modules not available: {e}")
    OPTIMIZATION_AVAILABLE = False

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class PerformanceTestSuite:
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸"""

    def __init__(self, api_base_url: str = "http://localhost:8000"):
        self.api_base_url = api_base_url
        self.results = {}

        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
        self.test_queries = [
            "AI machine learning algorithms",
            "natural language processing",
            "computer vision deep learning",
            "reinforcement learning applications",
            "neural network architectures",
            "data preprocessing techniques",
            "model evaluation metrics",
            "feature engineering methods",
            "classification algorithms comparison",
            "regression analysis techniques",
            "clustering algorithms overview",
            "dimensionality reduction methods",
            "time series forecasting",
            "recommendation systems",
            "ensemble learning methods",
            "cross validation techniques",
            "hyperparameter optimization",
            "transfer learning applications",
            "generative adversarial networks",
            "transformer architecture explanation"
        ]

    async def test_parallel_search_performance(self) -> Dict[str, Any]:
        """BM25 + Vector ë³‘ë ¬ ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ”„ Testing parallel search performance...")

        if not OPTIMIZATION_AVAILABLE:
            return {"error": "Optimization modules not available"}

        try:
            # Database import
            from apps.api.database import SearchDAO, db_manager

            results = {
                "sequential_times": [],
                "parallel_times": [],
                "speedup_ratios": [],
                "memory_usage": []
            }

            async_optimizer = await get_async_optimizer()
            memory_monitor = await get_memory_monitor()

            # 10ê°œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë¡œ ì„±ëŠ¥ ì¸¡ì •
            for i, query in enumerate(self.test_queries[:10]):
                logger.info(f"Testing query {i+1}/10: {query[:30]}...")

                # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
                memory_before = await memory_monitor.check_memory_usage()

                # ìˆœì°¨ ì‹¤í–‰ ì‹œê°„ ì¸¡ì • (ë ˆê±°ì‹œ ë°©ì‹)
                start_time = time.time()
                async with db_manager.async_session() as session:
                    try:
                        # ë ˆê±°ì‹œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
                        legacy_results = await SearchDAO._execute_legacy_hybrid_search(
                            query, None, 5, 12, 12, 50
                        )
                    except Exception as e:
                        logger.warning(f"Legacy search failed: {e}")
                        legacy_results = []
                sequential_time = time.time() - start_time
                results["sequential_times"].append(sequential_time)

                # ë³‘ë ¬ ì‹¤í–‰ ì‹œê°„ ì¸¡ì • (ìµœì í™” ë°©ì‹)
                start_time = time.time()
                try:
                    optimized_results = await SearchDAO.hybrid_search(
                        query=query,
                        filters=None,
                        topk=5,
                        bm25_topk=12,
                        vector_topk=12,
                        rerank_candidates=50
                    )
                except Exception as e:
                    logger.warning(f"Optimized search failed: {e}")
                    optimized_results = []
                parallel_time = time.time() - start_time
                results["parallel_times"].append(parallel_time)

                # ì„±ëŠ¥ í–¥ìƒ ë¹„ìœ¨ ê³„ì‚°
                speedup = sequential_time / parallel_time if parallel_time > 0 else 1.0
                results["speedup_ratios"].append(speedup)

                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë¡
                memory_after = await memory_monitor.check_memory_usage()
                memory_usage = memory_after.get("current_usage", {}).get("rss_mb", 0)
                results["memory_usage"].append(memory_usage)

                # ì ì‹œ ëŒ€ê¸° (ì‹œìŠ¤í…œ ì•ˆì •í™”)
                await asyncio.sleep(0.1)

            # í†µê³„ ê³„ì‚°
            avg_sequential = statistics.mean(results["sequential_times"])
            avg_parallel = statistics.mean(results["parallel_times"])
            avg_speedup = statistics.mean(results["speedup_ratios"])
            avg_memory = statistics.mean(results["memory_usage"])

            performance_result = {
                "avg_sequential_time": avg_sequential,
                "avg_parallel_time": avg_parallel,
                "avg_speedup": avg_speedup,
                "avg_memory_mb": avg_memory,
                "p95_parallel_time": statistics.quantiles(results["parallel_times"], n=20)[18] if len(results["parallel_times"]) >= 20 else max(results["parallel_times"]),
                "target_p95_ms": PERFORMANCE_TARGETS["p95_latency_ms"],
                "meets_target": (avg_parallel * 1000) <= PERFORMANCE_TARGETS["p95_latency_ms"],
                "sample_size": len(results["parallel_times"])
            }

            logger.info(f"âœ… Parallel search test completed:")
            logger.info(f"   Average speedup: {avg_speedup:.2f}x")
            logger.info(f"   P95 latency: {performance_result['p95_parallel_time']*1000:.1f}ms")
            logger.info(f"   Target met: {performance_result['meets_target']}")

            return performance_result

        except Exception as e:
            logger.error(f"Parallel search test failed: {e}")
            return {"error": str(e)}

    async def test_batch_processing_throughput(self) -> Dict[str, Any]:
        """ë°°ì¹˜ ì²˜ë¦¬ ì²˜ë¦¬ëŸ‰ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ”„ Testing batch processing throughput...")

        if not OPTIMIZATION_AVAILABLE:
            return {"error": "Optimization modules not available"}

        try:
            batch_processor = get_batch_search_processor()

            # ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸°ë¡œ í…ŒìŠ¤íŠ¸
            batch_sizes = [1, 5, 10, 20, 50]
            results = {}

            for batch_size in batch_sizes:
                logger.info(f"Testing batch size: {batch_size}")

                # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì¤€ë¹„
                test_queries = self.test_queries[:batch_size]
                search_params = {
                    "filters": None,
                    "bm25_topk": 12,
                    "vector_topk": 12,
                    "final_topk": 5,
                    "rerank_candidates": 50
                }

                # ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
                start_time = time.time()

                # ë°°ì¹˜ ìš”ì²­ ë™ì‹œ ì œì¶œ
                tasks = []
                for query in test_queries:
                    task = batch_processor.submit_search_request(
                        query=query,
                        search_params=search_params,
                        priority=0
                    )
                    tasks.append(task)

                # ëª¨ë“  ê²°ê³¼ ëŒ€ê¸°
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)

                processing_time = time.time() - start_time

                # ì„±ê³µ/ì‹¤íŒ¨ ì¹´ìš´íŠ¸
                successful_results = sum(1 for r in batch_results if not isinstance(r, Exception))
                failed_results = len(batch_results) - successful_results

                # ì²˜ë¦¬ëŸ‰ ê³„ì‚°
                throughput = len(test_queries) / processing_time if processing_time > 0 else 0

                results[f"batch_{batch_size}"] = {
                    "batch_size": batch_size,
                    "processing_time": processing_time,
                    "throughput_qps": throughput,
                    "successful_requests": successful_results,
                    "failed_requests": failed_results,
                    "success_rate": successful_results / len(batch_results) if batch_results else 0
                }

                await asyncio.sleep(0.2)  # ë°°ì¹˜ ê°„ ëŒ€ê¸°

            # ìµœê³  ì²˜ë¦¬ëŸ‰ ì°¾ê¸°
            best_throughput = max(results.values(), key=lambda x: x["throughput_qps"])

            # ë°°ì¹˜ í”„ë¡œì„¸ì„œ ë©”íŠ¸ë¦­
            batch_metrics = batch_processor.get_metrics()

            throughput_result = {
                "batch_results": results,
                "best_performance": best_throughput,
                "target_qps": PERFORMANCE_TARGETS["throughput_qps"],
                "meets_target": best_throughput["throughput_qps"] >= PERFORMANCE_TARGETS["throughput_qps"],
                "batch_processor_metrics": batch_metrics.__dict__
            }

            logger.info(f"âœ… Batch processing test completed:")
            logger.info(f"   Best throughput: {best_throughput['throughput_qps']:.1f} QPS")
            logger.info(f"   Best batch size: {best_throughput['batch_size']}")
            logger.info(f"   Target met: {throughput_result['meets_target']}")

            return throughput_result

        except Exception as e:
            logger.error(f"Batch processing test failed: {e}")
            return {"error": str(e)}

    async def test_memory_optimization(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” íš¨ê³¼ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ”„ Testing memory optimization...")

        if not OPTIMIZATION_AVAILABLE:
            return {"error": "Optimization modules not available"}

        try:
            from apps.api.optimization.memory_optimizer import (
                get_embedding_quantizer,
                get_streaming_processor,
                get_gc_optimizer
            )

            memory_monitor = await get_memory_monitor()
            quantizer = get_embedding_quantizer()
            streaming_processor = get_streaming_processor()
            gc_optimizer = get_gc_optimizer()

            # ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ
            initial_memory = await memory_monitor.check_memory_usage()

            # 1. ì„ë² ë”© ì–‘ìí™” í…ŒìŠ¤íŠ¸
            import numpy as np
            test_embeddings = [np.random.rand(1536).astype(np.float32) for _ in range(100)]

            quantization_results = []
            for embedding in test_embeddings:
                quantized, metadata = quantizer.quantize_embedding(embedding)
                quantization_results.append({
                    "original_size": embedding.nbytes,
                    "quantized_size": quantized.nbytes,
                    "compression_ratio": embedding.nbytes / quantized.nbytes
                })

            avg_compression = statistics.mean([r["compression_ratio"] for r in quantization_results])

            # 2. ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸
            large_result_set = [
                {"chunk_id": f"chunk_{i}", "text": "dummy text " * 100, "score": random.random()}
                for i in range(1000)
            ]

            streaming_chunks = []
            async for chunk in streaming_processor.stream_search_results(large_result_set):
                streaming_chunks.append(len(chunk))

            # 3. GC ìµœì í™” í…ŒìŠ¤íŠ¸
            async with gc_optimizer.optimized_gc_context():
                # ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
                large_data = [list(range(10000)) for _ in range(100)]
                del large_data

            # ìµœì¢… ë©”ëª¨ë¦¬ ìƒíƒœ
            final_memory = await memory_monitor.check_memory_usage()

            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê³„ì‚°
            memory_saved = initial_memory.get("current_usage", {}).get("rss_mb", 0) - final_memory.get("current_usage", {}).get("rss_mb", 0)

            memory_result = {
                "quantization": {
                    "avg_compression_ratio": avg_compression,
                    "samples_tested": len(quantization_results),
                    "memory_saved_percent": (avg_compression - 1) / avg_compression * 100
                },
                "streaming": {
                    "original_items": len(large_result_set),
                    "streaming_chunks": len(streaming_chunks),
                    "avg_chunk_size": statistics.mean(streaming_chunks) if streaming_chunks else 0
                },
                "garbage_collection": {
                    "gc_stats": gc_optimizer.get_gc_stats()
                },
                "memory_usage": {
                    "initial_mb": initial_memory.get("current_usage", {}).get("rss_mb", 0),
                    "final_mb": final_memory.get("current_usage", {}).get("rss_mb", 0),
                    "saved_mb": memory_saved,
                    "efficiency_improvement": avg_compression
                },
                "target_efficiency": PERFORMANCE_TARGETS["memory_efficiency"],
                "meets_target": avg_compression >= (1 + PERFORMANCE_TARGETS["memory_efficiency"])
            }

            logger.info(f"âœ… Memory optimization test completed:")
            logger.info(f"   Compression ratio: {avg_compression:.2f}x")
            logger.info(f"   Memory saved: {memory_saved:.1f}MB")
            logger.info(f"   Target met: {memory_result['meets_target']}")

            return memory_result

        except Exception as e:
            logger.error(f"Memory optimization test failed: {e}")
            return {"error": str(e)}

    async def test_concurrency_control(self) -> Dict[str, Any]:
        """ë™ì‹œì„± ì œì–´ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ”„ Testing concurrency control...")

        if not OPTIMIZATION_AVAILABLE:
            return {"error": "Optimization modules not available"}

        try:
            concurrency_controller = get_concurrency_controller()

            # ë™ì‹œ ìš”ì²­ ì‹œë®¬ë ˆì´ì…˜
            concurrent_requests = 20
            request_tasks = []

            async def simulate_search_request(request_id: int):
                """ê°œë³„ ê²€ìƒ‰ ìš”ì²­ ì‹œë®¬ë ˆì´ì…˜"""
                async with concurrency_controller.controlled_execution("test_search"):
                    # ê²€ìƒ‰ ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
                    await asyncio.sleep(random.uniform(0.1, 0.3))
                    return {"request_id": request_id, "status": "completed"}

            # ë™ì‹œ ìš”ì²­ ìƒì„±
            start_time = time.time()
            for i in range(concurrent_requests):
                task = simulate_search_request(i)
                request_tasks.append(task)

            # ëª¨ë“  ìš”ì²­ ì™„ë£Œ ëŒ€ê¸°
            results = await asyncio.gather(*request_tasks, return_exceptions=True)
            total_time = time.time() - start_time

            # ê²°ê³¼ ë¶„ì„
            successful_requests = sum(1 for r in results if not isinstance(r, Exception))
            failed_requests = len(results) - successful_requests

            # ë™ì‹œì„± í†µê³„
            concurrency_stats = concurrency_controller.get_comprehensive_stats()

            concurrency_result = {
                "concurrent_requests": concurrent_requests,
                "total_time": total_time,
                "successful_requests": successful_requests,
                "failed_requests": failed_requests,
                "requests_per_second": concurrent_requests / total_time if total_time > 0 else 0,
                "concurrency_stats": concurrency_stats,
                "target_error_rate": PERFORMANCE_TARGETS["error_rate"],
                "actual_error_rate": failed_requests / concurrent_requests if concurrent_requests > 0 else 0,
                "meets_target": (failed_requests / concurrent_requests) <= PERFORMANCE_TARGETS["error_rate"]
            }

            logger.info(f"âœ… Concurrency control test completed:")
            logger.info(f"   Success rate: {successful_requests}/{concurrent_requests}")
            logger.info(f"   Error rate: {concurrency_result['actual_error_rate']:.3f}")
            logger.info(f"   Target met: {concurrency_result['meets_target']}")

            return concurrency_result

        except Exception as e:
            logger.error(f"Concurrency control test failed: {e}")
            return {"error": str(e)}

    async def test_api_endpoints(self) -> Dict[str, Any]:
        """API ì—”ë“œí¬ì¸íŠ¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ”„ Testing API endpoints...")

        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                results = {}

                # 1. ê¸°ë³¸ ê²€ìƒ‰ API í…ŒìŠ¤íŠ¸
                search_latencies = []
                for i in range(5):
                    start_time = time.time()
                    try:
                        response = await client.post(
                            f"{self.api_base_url}/search",
                            json={
                                "q": self.test_queries[i % len(self.test_queries)],
                                "final_topk": 5,
                                "bm25_topk": 12,
                                "vector_topk": 12
                            },
                            headers={"Content-Type": "application/json"}
                        )
                        latency = (time.time() - start_time) * 1000
                        search_latencies.append(latency)

                        if response.status_code == 200:
                            logger.info(f"Search API test {i+1}/5: {latency:.1f}ms")
                        else:
                            logger.warning(f"Search API test {i+1}/5 failed: {response.status_code}")

                    except Exception as e:
                        logger.warning(f"Search API test {i+1}/5 error: {e}")

                    await asyncio.sleep(0.1)

                # 2. ë°°ì¹˜ ê²€ìƒ‰ API í…ŒìŠ¤íŠ¸ (if available)
                batch_latencies = []
                try:
                    batch_queries = [
                        {"q": query, "final_topk": 5}
                        for query in self.test_queries[:5]
                    ]

                    start_time = time.time()
                    batch_response = await client.post(
                        f"{self.api_base_url}/api/v1/batch/search",
                        json={
                            "queries": batch_queries,
                            "response_format": "json",
                            "timeout_seconds": 30
                        },
                        headers={"Content-Type": "application/json"}
                    )
                    batch_latency = (time.time() - start_time) * 1000
                    batch_latencies.append(batch_latency)

                    if batch_response.status_code == 200:
                        logger.info(f"Batch API test: {batch_latency:.1f}ms")
                    else:
                        logger.warning(f"Batch API test failed: {batch_response.status_code}")

                except Exception as e:
                    logger.warning(f"Batch API test error: {e}")

                # í†µê³„ ê³„ì‚°
                avg_search_latency = statistics.mean(search_latencies) if search_latencies else 0
                p95_search_latency = statistics.quantiles(search_latencies, n=20)[18] if len(search_latencies) >= 20 else max(search_latencies) if search_latencies else 0

                api_result = {
                    "search_api": {
                        "tests_completed": len(search_latencies),
                        "avg_latency_ms": avg_search_latency,
                        "p95_latency_ms": p95_search_latency,
                        "all_latencies": search_latencies
                    },
                    "batch_api": {
                        "tests_completed": len(batch_latencies),
                        "avg_latency_ms": statistics.mean(batch_latencies) if batch_latencies else 0,
                        "all_latencies": batch_latencies
                    },
                    "target_p95_ms": PERFORMANCE_TARGETS["p95_latency_ms"],
                    "meets_target": p95_search_latency <= PERFORMANCE_TARGETS["p95_latency_ms"]
                }

                logger.info(f"âœ… API endpoint test completed:")
                logger.info(f"   Search API P95: {p95_search_latency:.1f}ms")
                logger.info(f"   Target met: {api_result['meets_target']}")

                return api_result

        except Exception as e:
            logger.error(f"API endpoint test failed: {e}")
            return {"error": str(e)}

    async def run_full_performance_test(self) -> Dict[str, Any]:
        """ì „ì²´ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸš€ Starting comprehensive performance test suite...")

        start_time = time.time()

        # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_results = {}

        # 1. ë³‘ë ¬ ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        test_results["parallel_search"] = await self.test_parallel_search_performance()

        # 2. ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        test_results["batch_processing"] = await self.test_batch_processing_throughput()

        # 3. ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸
        test_results["memory_optimization"] = await self.test_memory_optimization()

        # 4. ë™ì‹œì„± ì œì–´ í…ŒìŠ¤íŠ¸
        test_results["concurrency_control"] = await self.test_concurrency_control()

        # 5. API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸
        test_results["api_endpoints"] = await self.test_api_endpoints()

        total_time = time.time() - start_time

        # ì „ì²´ ì„±ëŠ¥ í‰ê°€
        performance_summary = self._generate_performance_summary(test_results, total_time)

        logger.info(f"ğŸ‰ Performance test suite completed in {total_time:.1f}s")
        logger.info(f"ğŸ“Š Overall performance grade: {performance_summary['overall_grade']}")

        return {
            "test_results": test_results,
            "performance_summary": performance_summary,
            "total_test_time": total_time,
            "timestamp": time.time()
        }

    def _generate_performance_summary(self, test_results: Dict[str, Any], total_time: float) -> Dict[str, Any]:
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìš”ì•½ ìƒì„±"""
        targets_met = 0
        total_targets = 0

        performance_scores = []

        # ê° í…ŒìŠ¤íŠ¸ ê²°ê³¼ í‰ê°€
        for test_name, result in test_results.items():
            if "error" in result:
                continue

            meets_target = result.get("meets_target", False)
            if meets_target is not None:
                total_targets += 1
                if meets_target:
                    targets_met += 1
                    performance_scores.append(100)
                else:
                    performance_scores.append(60)

        # ì „ì²´ ì ìˆ˜ ê³„ì‚°
        overall_score = statistics.mean(performance_scores) if performance_scores else 0

        # ë“±ê¸‰ ì‚°ì •
        if overall_score >= 90:
            grade = "A"
        elif overall_score >= 80:
            grade = "B"
        elif overall_score >= 70:
            grade = "C"
        else:
            grade = "D"

        return {
            "overall_score": overall_score,
            "overall_grade": grade,
            "targets_met": targets_met,
            "total_targets": total_targets,
            "success_rate": targets_met / total_targets if total_targets > 0 else 0,
            "performance_targets": PERFORMANCE_TARGETS,
            "recommendations": self._generate_recommendations(test_results)
        }

    def _generate_recommendations(self, test_results: Dict[str, Any]) -> List[str]:
        """ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []

        # ë³‘ë ¬ ê²€ìƒ‰ ì„±ëŠ¥ ë¶„ì„
        parallel_result = test_results.get("parallel_search", {})
        if not parallel_result.get("meets_target", True):
            recommendations.append("ë³‘ë ¬ ê²€ìƒ‰ ì„±ëŠ¥ ìµœì í™” í•„ìš” - BM25ì™€ Vector ê²€ìƒ‰ ê°„ ë¶€í•˜ ê· í˜• ì¡°ì •")

        # ë°°ì¹˜ ì²˜ë¦¬ ë¶„ì„
        batch_result = test_results.get("batch_processing", {})
        if not batch_result.get("meets_target", True):
            recommendations.append("ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸° ì¡°ì • ë° ì—°ê²° í’€ ìµœì í™” í•„ìš”")

        # ë©”ëª¨ë¦¬ ìµœì í™” ë¶„ì„
        memory_result = test_results.get("memory_optimization", {})
        if not memory_result.get("meets_target", True):
            recommendations.append("ë©”ëª¨ë¦¬ ì••ì¶•ë¥  ê°œì„  ë° ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ íŠœë‹ í•„ìš”")

        # ë™ì‹œì„± ì œì–´ ë¶„ì„
        concurrency_result = test_results.get("concurrency_control", {})
        if not concurrency_result.get("meets_target", True):
            recommendations.append("ë™ì‹œì„± ì œì–´ íŒŒë¼ë¯¸í„° ì¡°ì • ë° ì—ëŸ¬ í•¸ë“¤ë§ ê°œì„  í•„ìš”")

        # API ì—”ë“œí¬ì¸íŠ¸ ë¶„ì„
        api_result = test_results.get("api_endpoints", {})
        if not api_result.get("meets_target", True):
            recommendations.append("API ì‘ë‹µì‹œê°„ ìµœì í™” ë° ìºì‹± ì „ëµ ê°œì„  í•„ìš”")

        if not recommendations:
            recommendations.append("ëª¨ë“  ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± - í˜„ì¬ ìµœì í™” ìƒíƒœ ìœ ì§€")

        return recommendations

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("=" * 60)
    logger.info("Phase 2 ì„±ëŠ¥ ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info("=" * 60)

    if not OPTIMIZATION_AVAILABLE:
        logger.error("âŒ ìµœì í™” ëª¨ë“ˆì´ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
        logger.error("   ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ í•„ìš”í•œ ì¢…ì†ì„±ì„ ì„¤ì¹˜í•˜ì„¸ìš”:")
        logger.error("   pip install psutil numpy scikit-learn")
        return

    # í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰
    test_suite = PerformanceTestSuite()

    try:
        # ì „ì²´ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        results = await test_suite.run_full_performance_test()

        # ê²°ê³¼ íŒŒì¼ ì €ì¥
        results_file = "phase2_optimization_test_results.json"
        with open(results_file, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)

        logger.info(f"ğŸ“„ ê²°ê³¼ê°€ {results_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # ìš”ì•½ ì¶œë ¥
        summary = results["performance_summary"]
        logger.info("=" * 60)
        logger.info("ğŸ“Š Phase 2 ìµœì í™” ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
        logger.info("=" * 60)
        logger.info(f"ì „ì²´ ì ìˆ˜: {summary['overall_score']:.1f}/100")
        logger.info(f"ë“±ê¸‰: {summary['overall_grade']}")
        logger.info(f"ëª©í‘œ ë‹¬ì„±ë¥ : {summary['targets_met']}/{summary['total_targets']} ({summary['success_rate']:.1%})")
        logger.info("")
        logger.info("ğŸ“‹ ê¶Œì¥ì‚¬í•­:")
        for i, rec in enumerate(summary["recommendations"], 1):
            logger.info(f"  {i}. {rec}")
        logger.info("=" * 60)

    except Exception as e:
        logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())