좋아 오빠. “PromptCoT 2.0 스타일”을 **훈련 없이**도 프로젝트에 녹여 쓰는 **프레임워크형 설계**를 정리해줄게. 핵심은 “개념→사고흐름(rationale)→프롬프트”의 **EM 비슷한 반복 루프**를, 우리가 가진 도구(Claude Code, 스크립트, 테스트, 로그)로 **강제로 실행**하는 거야. 아래 설계는 전부 **모델 미세튜닝 없이** “프롬프트 엔지니어링 + 자동 평가 + 데이터 캐시”만으로 돌아가게 되어 있어.

---

# 0) 핵심 아이디어 (훈련 없이 합성-평가-선별 반복)

* **E-step(추정)**: 모델이 주어진 **Concept(개념/목표)**와 초기 **Prompt**를 보고 **Rationale(사고흐름/설명)** 후보를 여러 개 만든다.
* **M-step(최적화)**: 각 후보에 대해 **자동 평가(테스트/체커/스코어러)**로 점수를 매기고, 상위 후보들을 **다음 라운드 Prompt**로 재합성(리라이트/일반화)한다.
* **결과**: 시간이 갈수록 “좋은 프롬프트+사고흐름 묶음”이 축적되고, 다음 작업에 그대로 재사용(라이브러리화) 가능.

> 포인트: “Self-play + SFT” 대신 **Self-play + Selection(선별)**만으로도 품질을 점진 향상.

---

# 1) 최소 동작 루프 (언어불문/도메인불문)

### 1-1. 아티팩트 구조

```
/promptcot_runtime/
  concepts/                 # 도메인 개념 정의(JSON/YAML)
  seeds/                    # 초기 시드 프롬프트
  rationales/               # 생성된 사고흐름(버전링)
  prompts/                  # 합성된 프롬프트(버전링)
  checkers/                 # 자동평가 스크립트(언어별)
  scores/                   # 점수 로그(JSONL)
  winners/                  # 라운드 우승 프롬프트+라쇼널
  runs/                     # 전체 러닝 로그(메타데이터)
```

### 1-2. 메타 프롬프트(모델에 주는 시스템 지시)

```
[ROLE] 당신은 "Prompt Synthesizer".
[GOAL] 주어진 Concept로부터 고난도이지만 채점 가능한 과제를 만들고,
       해결 사고흐름(Rationale)과 정답/테스트를 포함한 프롬프트를 합성한다.
[OUTPUT] JSON only:
{
  "prompt": "...과제 본문...",
  "rationale": "...해결 사고흐름...",
  "solution": "...정답(또는 기준 출력)...",
  "judge": { "type":"unit_test|rule|rubric", "payload":"..." },
  "generalization": ["동일 패턴 변형 A", "변형 B", ...]
}
[CRITERIA] 채점 가능성, 난이도/다양성, 모호성 최소화, 재사용성.
```

### 1-3. 체크/채점(훈련 없이 필수)

* **코딩 과제**: 유닛테스트/샌드박스 실행 결과(통과 케이스 수)
* **텍스트 생성**: 규칙 기반 체커(길이/포맷/금칙어), BLEU/ROUGE 같은 간단 지표, “모델-에즈-저지” 다중합의(자기일관성)
* **분류/추론**: 정답 키, 혹은 체커 스크립트

---

# 2) Claude Code “hooks”로 묶는 방법 (강제 프롬프트 루프)

아래는 개념 예시야. (클로드코드 CLI에서 훅을 지원한다는 가정하에, **pre_run → generate → score → select → rewrite → next_round** 식 오케스트레이션)

### 2-1. 훅 배치 아이디어

* **pre_run hook**: concepts/* 와 seeds/* 읽어서 **라운드 작업 큐** 구성
* **generate hook**: 각 작업 큐 아이템에 대해 모델 호출 → N개 후보(JSON) 저장
* **score hook**: 후보별 judge를 실행하여 점수 계산 → scores/*.jsonl 기록
* **select hook**: 상위 K개 선별 → winners/ 저장
* **rewrite hook**: 우승 프롬프트를 기반으로 **일반화/난이도 조정** 리라이트
* **next_round hook**: 새 후보 세트를 만들어 다음 라운드 generate로 전달
  (라운드 수, 수렴 조건, 성능 임계값으로 중단)

### 2-2. 로직 의사코드

```python
# run_loop.py
for round_idx in range(MAX_ROUNDS):
    batch = load_batch(concepts, seeds, winners, round_idx)

    candidates = []
    for item in batch:
        cand = claude_generate(item)  # prompt, rationale, solution, judge, generalization
        candidates.append(cand)
        save_json(cand, f"rationales/{round_idx}/cand_{uuid()}.json")

    scored = []
    for c in candidates:
        s = run_judge(c)  # run unit tests or rule-based checker
        scored.append((c, s))
        append_jsonl("scores/round_{round_idx}.jsonl", {"id": c["id"], "score": s})

    topk = select_topk(scored, k=K)
    save_all(topk, f"winners/round_{round_idx}/")

    seeds = rewrite_from_winners(topk)  # 난이도 ↑, 모호성 ↓, 일반화 조합
    save_all(seeds, f"prompts/round_{round_idx}/seeds.json")

    if is_converged(scored): break
```

### 2-3. Claude 호출용 프롬프트 루틴(핵심 페이로드)

```python
def claude_generate(item):
    system = META_PROMPT  # 1-2의 메타 프롬프트
    user = {
      "concept": item["concept"],
      "style": item["style"],            # 예: 함수형/수학적 증명/시나리오 등
      "format": item["format"],          # 예: "coding|qa|reasoning"
      "constraints": item["constraints"] # 길이/입출력 포맷 등
    }
    resp = call_claude(system=system, user=json.dumps(user))
    return json.loads(resp)  # 반드시 JSON만 수신
```

---

# 3) “훈련 없이” 고도화하는 6가지 기법

1. **다중 샘플 + 다중 채점 + 다중 합의**

   * N개 후보 생성 → 서로 다른 **체커 2~3종**으로 점수 → **정규화 평균**
   * 텍스트 과제는 **자기일관성(MC-SC)**: 모델이 스스로 동일 문제를 3회 풀이, 다수결/교집합

2. **프로그램적 난이도 스케줄링**

   * round별로 난이도 목표(입력 길이/제약 수/테스트 수)를 자동 조정
   * `rewrite_from_winners`에서 **난이도 ↑ 규칙 템플릿** 적용 (예: “엣지케이스 2개 추가”)

3. **실패 주도 리라이트(Failure-driven Rewrite)**

   * 통과 실패한 테스트/규칙을 **피드백**으로 모델에 주고,
     “모호성을 유발한 문장, 누락된 제약”을 제거하도록 리라이트

4. **개념 그래프(Concept Graph) 확장**

   * winners의 `generalization` 목록을 분석하여 **개념 관계(선행→후행)**를 자동 추출
   * 쉽게는 YAML:

     ```yaml
     graph:
       - from: "two-pointer"
         to:   "sliding-window"
       - from: "invariants"
         to:   "proof-by-contradiction"
     ```
   * 다음 라운드에서 **그래프 경로 조합**으로 프롬프트 다양화

5. **도메인 체커 플러그인**

   * 코딩: pytest/Node test/Go test 등 **언어별 실행기**
   * 데이터분석: Pandas 검증 스크립트(컬럼/분포/정답 통계)
   * LLM평가: **룰-프롬프트 고정**(편향 최소), **다중 모델-저지** 옵션(가능 시)

6. **캐시 & 재사용(“라이브러리화”)**

   * winners를 **태그+메타**로 카탈로그화 → “같은 도메인 새 작업”에서 즉시 재사용
   * 메타 예: `domain=algo, pattern=two-pointer, io=jsonl, level=hard`

---

# 4) 오빠 프로젝트 맥락에 맞춘 “적용 시나리오”

### A. DT-RAG / 멀티-에이전트(Guardian Workflow) 테스트 자동 합성

* **Concept**: “문서 질의응답 품질 향상용 ‘곤란 쿼리’ 만들기”
* 루프:

  1. 합성 프롬프트가 RAG를 난처하게 만드는 **엣지쿼리** 생성(라쇼널 포함)
  2. **체커**: RAG의 실제 응답과 **정답근거 문단** 일치율, 소스 인용 유무, 금칙(환각) 점검
  3. 점수 상위 ‘엣지쿼리’들을 winners로 보존 → 회귀테스트 스위트 자동 확장

### B. 코드 에이전트(Claude Code Subagents)용 과제 생성

* **Concept**: “파일 IO + 에러 복구 + 리트라이 정책”
* 루프:

  1. 프롬프트가 작은 CLI 유틸 문제를 만들고, **유닛테스트**를 함께 합성
  2. 서브에이전트가 코드를 작성 → 테스트 실행 → 점수
  3. 실패 로그를 모델에 주고 **모호성 제거 리라이트** → 재시도
  4. 우승 프롬프트를 패턴 라이브러리로 저장 → 이후 과제 자동 생산

### C. 콘텐츠 생성(블로그/쇼츠) 스타일 템플릿 진화

* **Concept**: “한 주제에서 톤/구성/훅 5종 다양화”
* 루프:

  * **체커**: 구조(후킹→문제→해결→CTA) 준수, 길이/키워드/금칙어, 표절 점검
  * 낮은 점수의 원인(중복/약한 훅)을 피드백해 **템플릿 강화** 리라이트

---

# 5) JSON 스키마(입·출력 명세)

### 5-1. 입력(작업 단위)

```json
{
  "id": "job-20251009-001",
  "concept": "two-pointer on nearly sorted arrays",
  "format": "coding",
  "constraints": {
    "language": "python",
    "io": "stdin-stdout",
    "time_limit_ms": 2000,
    "edge_cases": 3
  },
  "style": "interview-hard"
}
```

### 5-2. 모델 출력(필수 필드)

```json
{
  "prompt": "…문제 설명…",
  "rationale": "…출제 의도/풀이 핵심…",
  "solution": "…기준 정답(또는 기대 출력/예시)…",
  "judge": { "type": "unit_test", "payload": "pytest code string or json cases" },
  "generalization": ["입력 크기↑", "역순 케이스 추가", "중복 요소 포함"]
}
```

---

# 6) 체크/체점기 예시(코딩: 파이썬)

```python
# checkers/python_unit_test_runner.py
import subprocess, json, tempfile, os, sys

def run(code:str, tests:str) -> dict:
    with tempfile.TemporaryDirectory() as td:
        code_file = os.path.join(td, "solution.py")
        test_file = os.path.join(td, "test_solution.py")
        open(code_file, "w", encoding="utf-8").write(code)
        open(test_file, "w", encoding="utf-8").write(tests)

        p = subprocess.run(
            [sys.executable, "-m", "pytest", "-q", test_file],
            stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, cwd=td
        )
        passed = " passed" in p.stdout
        # 더 세밀하게: 통과 케이스 수/실패 목록 파싱
        return {"passed": passed, "raw": p.stdout}

if __name__ == "__main__":
    payload = json.load(sys.stdin)
    result = run(payload["code"], payload["tests"])
    print(json.dumps(result, ensure_ascii=False))
```

> LLM이 합성한 `solution`이 실제 정답 코드일 필요는 없어.
> 우리 측 서브에이전트(Claude Code)가 코드를 시도하고, 이 **체커**가 점수를 만든다.

---

# 7) EM 유사 루프를 “문장 하나”로 강제하는 리라이트 프롬프트

* **E-step 프롬프트 예시(라쇼널 생성 강화)**
  “이전 우승 프롬프트를 읽고, 실패 케이스 로그를 반영해 **모호성 제거·제약 추가** 중심의 **사고흐름 3개**를 만들어라. 각 사고흐름은 엣지케이스가 다르게 설계되어야 한다. JSON만.”

* **M-step 프롬프트 예시(프롬프트 최적화)**
  “선택된 사고흐름을 가이드로 **프롬프트 본문**을 다시 작성하라. 평가 가능한 테스트(judge)를 반드시 포함하고, **입출력 포맷을 첫머리에 명세**하라. JSON만.”

---

# 8) 중단 기준 & 안전장치

* **수렴 조건**: 최근 3라운드 우승 평균점수 상승 ≤ ε, 또는 상위 테스트 통과율 ≥ T
* **다양성 제한**: n-gram 중복률/템플릿 유사도 상한
* **환각 방지**: 룰-기반 포맷 강제(필수 섹션/키 누락 시 0점), 소스 인용 필수(정보성 과제)

---

# 9) 운영 팁 (오빠 환경 기준)

* **로컬 우선**: 실행/체커·로그는 전부 로컬. 모델 호출만 API.
* **작은 배치로 시작**: round당 후보 N=3~5, 체커 가벼운 것부터.
* **승자 보관이 재산**: winners/는 곧 “테스트 스위트 + 템플릿 라이브러리”.
* **프로젝트별 폴더 분리**: DT-RAG, 코드에이전트, 콘텐츠 제작 각각 별도 러너.

---

# 10) 바로 써먹는 “Claude Code Hooks” 시나리오 요약

1. **pre_run**: `concepts/*.yaml` 읽어 배치 큐 구성
2. **generate**: 각 큐 아이템→모델 JSON 생성(N개)
3. **score**: 도메인 체커 실행(유닛테스트/룰/저지)→점수 기록
4. **select**: 상위 K 저장(winners/), 메타 업데이트
5. **rewrite**: 우승 세트를 일반화/난이도↑ 리라이트 → 다음 라운드 시드
6. **stop**: 수렴 기준 만족 시 종료, winners를 **프로젝트 테스트/템플릿**으로 등록

---

필요하면 이걸 **윈도우 기준 실행 스크립트(배치/파워셸)**, 또는 **파이썬 패키지 구조로 바로 복붙 가능한 형태**로 정리해서 내줄게. 어느 도메인(코딩/RAG/콘텐츠)부터 셋업할지만 알려줘.
