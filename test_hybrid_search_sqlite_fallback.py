#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ SQLite í´ë°± í…ŒìŠ¤íŠ¸
PostgreSQL ì—°ê²°ì´ ì—†ëŠ” í™˜ê²½ì—ì„œë„ ëª¨ë“  ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸

ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ ëŒ€ì‹  SQLite ë©”ëª¨ë¦¬ DBë¥¼ ì‚¬ìš©í•˜ì—¬
BM25 + Vector Similarity + Cross-Encoder Reranking ê¸°ëŠ¥ì„ ê²€ì¦
"""

import asyncio
import time
import logging
import json
import traceback
import sqlite3
import tempfile
import os
from typing import List, Dict, Any, Tuple
from datetime import datetime
import uuid
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import sys

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from apps.api.database import BM25Scorer, EmbeddingService, CrossEncoderReranker

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('hybrid_search_sqlite_test.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class SQLiteSearchDAO:
    """SQLite ê¸°ë°˜ ê²€ìƒ‰ DAO (í…ŒìŠ¤íŠ¸ìš©)"""

    def __init__(self, db_path=":memory:"):
        self.db_path = db_path
        self.conn = None
        self._init_database()

    def _init_database(self):
        """SQLite ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)
        self.conn.row_factory = sqlite3.Row

        # í…Œì´ë¸” ìƒì„±
        self.conn.executescript("""
            CREATE TABLE IF NOT EXISTS documents (
                doc_id TEXT PRIMARY KEY,
                title TEXT,
                source_url TEXT,
                content_type TEXT DEFAULT 'text/plain',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );

            CREATE TABLE IF NOT EXISTS chunks (
                chunk_id TEXT PRIMARY KEY,
                doc_id TEXT,
                text TEXT NOT NULL,
                chunk_index INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (doc_id) REFERENCES documents (doc_id)
            );

            CREATE TABLE IF NOT EXISTS embeddings (
                embedding_id TEXT PRIMARY KEY,
                chunk_id TEXT,
                vec TEXT,  -- JSON ë¬¸ìì—´ë¡œ ì €ì¥
                model_name TEXT DEFAULT 'text-embedding-ada-002',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (chunk_id) REFERENCES chunks (chunk_id)
            );

            CREATE TABLE IF NOT EXISTS doc_taxonomy (
                mapping_id TEXT PRIMARY KEY,
                doc_id TEXT,
                path TEXT,  -- JSON ë¬¸ìì—´ë¡œ ì €ì¥
                confidence REAL DEFAULT 0.0,
                source TEXT DEFAULT 'manual',
                FOREIGN KEY (doc_id) REFERENCES documents (doc_id)
            );

            -- ì¸ë±ìŠ¤ ìƒì„±
            CREATE INDEX IF NOT EXISTS idx_chunks_text ON chunks(text);
            CREATE INDEX IF NOT EXISTS idx_chunks_doc_id ON chunks(doc_id);
            CREATE INDEX IF NOT EXISTS idx_embeddings_chunk_id ON embeddings(chunk_id);
        """)
        self.conn.commit()

    async def insert_test_data(self, test_documents: List[Dict]):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚½ì…"""
        try:
            for i, doc_data in enumerate(test_documents):
                doc_id = str(uuid.uuid4())
                chunk_id = str(uuid.uuid4())

                # ë¬¸ì„œ ì‚½ì…
                self.conn.execute("""
                    INSERT OR REPLACE INTO documents (doc_id, title, source_url)
                    VALUES (?, ?, ?)
                """, (doc_id, doc_data["title"], f"https://example.com/{i}"))

                # ì²­í¬ ì‚½ì…
                self.conn.execute("""
                    INSERT OR REPLACE INTO chunks (chunk_id, doc_id, text, chunk_index)
                    VALUES (?, ?, ?, ?)
                """, (chunk_id, doc_id, doc_data["text"], i))

                # ì„ë² ë”© ìƒì„± ë° ì‚½ì…
                embedding = await EmbeddingService.generate_embedding(doc_data["text"])
                if embedding:
                    self.conn.execute("""
                        INSERT OR REPLACE INTO embeddings (embedding_id, chunk_id, vec)
                        VALUES (?, ?, ?)
                    """, (str(uuid.uuid4()), chunk_id, json.dumps(embedding)))

                # ë¶„ë¥˜ ì •ë³´ ì‚½ì…
                if "category" in doc_data:
                    self.conn.execute("""
                        INSERT OR REPLACE INTO doc_taxonomy (mapping_id, doc_id, path, confidence)
                        VALUES (?, ?, ?, ?)
                    """, (str(uuid.uuid4()), doc_id, json.dumps(doc_data["category"]), 0.9))

            self.conn.commit()
            logger.info(f"{len(test_documents)}ê°œ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì‚½ì… ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‚½ì… ì‹¤íŒ¨: {e}")
            return False

    async def bm25_search(self, query: str, topk: int = 10) -> List[Dict[str, Any]]:
        """BM25 ê¸°ë°˜ ê²€ìƒ‰ (SQLite FTS ì‚¬ìš©)"""
        try:
            # ì¿¼ë¦¬ í† í°í™”
            query_tokens = BM25Scorer.preprocess_text(query)
            if not query_tokens:
                return []

            # ëª¨ë“  ì²­í¬ ì¡°íšŒ ë° BM25 ìŠ¤ì½”ì–´ ê³„ì‚°
            cursor = self.conn.execute("""
                SELECT c.chunk_id, c.text, d.title, d.source_url
                FROM chunks c
                JOIN documents d ON c.doc_id = d.doc_id
            """)

            results = []
            all_docs = []

            # ì½”í¼ìŠ¤ í†µê³„ ìˆ˜ì§‘
            for row in cursor.fetchall():
                doc_tokens = BM25Scorer.preprocess_text(row['text'])
                all_docs.append(doc_tokens)

            # ì½”í¼ìŠ¤ í†µê³„ ê³„ì‚°
            total_docs = len(all_docs)
            avg_doc_length = sum(len(doc) for doc in all_docs) / max(1, total_docs)
            term_doc_freq = {}

            for doc_tokens in all_docs:
                unique_terms = set(doc_tokens)
                for term in unique_terms:
                    term_doc_freq[term] = term_doc_freq.get(term, 0) + 1

            corpus_stats = {
                "total_docs": total_docs,
                "avg_doc_length": avg_doc_length,
                "term_doc_freq": term_doc_freq
            }

            # ë‹¤ì‹œ ì¿¼ë¦¬ ì‹¤í–‰í•˜ì—¬ ìŠ¤ì½”ì–´ ê³„ì‚°
            cursor = self.conn.execute("""
                SELECT c.chunk_id, c.text, d.title, d.source_url
                FROM chunks c
                JOIN documents d ON c.doc_id = d.doc_id
            """)

            for row in cursor.fetchall():
                doc_tokens = BM25Scorer.preprocess_text(row['text'])
                score = BM25Scorer.calculate_bm25_score(query_tokens, doc_tokens, corpus_stats)

                if score > 0:  # 0ì  ì´ˆê³¼ì¸ ê²ƒë§Œ í¬í•¨
                    results.append({
                        "chunk_id": row['chunk_id'],
                        "text": row['text'],
                        "title": row['title'],
                        "source_url": row['source_url'],
                        "taxonomy_path": [],
                        "score": score,
                        "metadata": {
                            "bm25_score": score,
                            "vector_score": 0.0,
                            "source": "bm25"
                        }
                    })

            # ì ìˆ˜ìˆœ ì •ë ¬ ë° ìƒìœ„ kê°œ ë°˜í™˜
            results.sort(key=lambda x: x['score'], reverse=True)
            return results[:topk]

        except Exception as e:
            logger.error(f"BM25 ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    async def vector_search(self, query: str, topk: int = 10) -> List[Dict[str, Any]]:
        """ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰"""
        try:
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = await EmbeddingService.generate_embedding(query)
            if not query_embedding:
                return []

            query_vec = np.array(query_embedding).reshape(1, -1)

            # ëª¨ë“  ì„ë² ë”© ì¡°íšŒ
            cursor = self.conn.execute("""
                SELECT e.chunk_id, e.vec, c.text, d.title, d.source_url
                FROM embeddings e
                JOIN chunks c ON e.chunk_id = c.chunk_id
                JOIN documents d ON c.doc_id = d.doc_id
            """)

            results = []
            for row in cursor.fetchall():
                try:
                    # JSONì—ì„œ ë²¡í„° ë³µì›
                    doc_embedding = json.loads(row['vec'])
                    doc_vec = np.array(doc_embedding).reshape(1, -1)

                    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                    similarity = cosine_similarity(query_vec, doc_vec)[0][0]

                    results.append({
                        "chunk_id": row['chunk_id'],
                        "text": row['text'],
                        "title": row['title'],
                        "source_url": row['source_url'],
                        "taxonomy_path": [],
                        "score": float(similarity),
                        "metadata": {
                            "bm25_score": 0.0,
                            "vector_score": float(similarity),
                            "source": "vector"
                        }
                    })
                except Exception as e:
                    logger.warning(f"ì„ë² ë”© ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue

            # ìœ ì‚¬ë„ìˆœ ì •ë ¬ ë° ìƒìœ„ kê°œ ë°˜í™˜
            results.sort(key=lambda x: x['score'], reverse=True)
            return results[:topk]

        except Exception as e:
            logger.error(f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    async def hybrid_search(
        self,
        query: str,
        filters: Dict = None,
        topk: int = 5,
        bm25_topk: int = 12,
        vector_topk: int = 12,
        rerank_candidates: int = 50
    ) -> List[Dict[str, Any]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (BM25 + Vector + Cross-encoder Reranking)"""
        try:
            logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œì‘: '{query}'")

            # 1. BM25 ê²€ìƒ‰
            bm25_results = await self.bm25_search(query, bm25_topk)
            logger.info(f"BM25 ê²°ê³¼: {len(bm25_results)}ê°œ")

            # 2. Vector ê²€ìƒ‰
            vector_results = await self.vector_search(query, vector_topk)
            logger.info(f"Vector ê²°ê³¼: {len(vector_results)}ê°œ")

            # 3. ê²°ê³¼ í•©ì„± (ì¤‘ë³µ ì œê±° ë° í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ ê³„ì‚°)
            combined_results = self._combine_search_results(
                bm25_results, vector_results, rerank_candidates
            )
            logger.info(f"í•©ì„± ê²°ê³¼: {len(combined_results)}ê°œ")

            # 4. Cross-encoder ì¬ë­í‚¹
            if combined_results:
                final_results = CrossEncoderReranker.rerank_results(
                    query, combined_results, topk
                )
                logger.info(f"ì¬ë­í‚¹ ê²°ê³¼: {len(final_results)}ê°œ")
                return final_results

            return []

        except Exception as e:
            logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def _combine_search_results(
        self,
        bm25_results: List[Dict],
        vector_results: List[Dict],
        max_candidates: int
    ) -> List[Dict]:
        """BM25ì™€ Vector ê²€ìƒ‰ ê²°ê³¼ í•©ì„±"""
        BM25_WEIGHT = 0.5
        VECTOR_WEIGHT = 0.5

        combined = {}

        # BM25 ê²°ê³¼ ì¶”ê°€
        for result in bm25_results:
            chunk_id = result["chunk_id"]
            combined[chunk_id] = result.copy()

        # Vector ê²°ê³¼ ì¶”ê°€/ì—…ë°ì´íŠ¸
        for result in vector_results:
            chunk_id = result["chunk_id"]
            if chunk_id in combined:
                # ê¸°ì¡´ ê²°ê³¼ì— vector ì •ë³´ ì¶”ê°€
                combined[chunk_id]["metadata"]["vector_score"] = result["metadata"]["vector_score"]
                # í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ ê³„ì‚°
                bm25_score = combined[chunk_id]["metadata"]["bm25_score"]
                vector_score = result["metadata"]["vector_score"]
                combined[chunk_id]["score"] = BM25_WEIGHT * bm25_score + VECTOR_WEIGHT * vector_score
                combined[chunk_id]["metadata"]["source"] = "hybrid"
            else:
                # ìƒˆë¡œìš´ vector ì „ìš© ê²°ê³¼
                combined[chunk_id] = result.copy()

        # ì ìˆ˜ìˆœ ì •ë ¬ ë° ìƒìœ„ í›„ë³´ ì„ íƒ
        sorted_results = sorted(
            combined.values(),
            key=lambda x: x["score"],
            reverse=True
        )

        return sorted_results[:max_candidates]

    def get_stats(self) -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì¡°íšŒ"""
        try:
            stats = {}

            # ê¸°ë³¸ í†µê³„
            cursor = self.conn.execute("SELECT COUNT(*) FROM documents")
            stats["total_docs"] = cursor.fetchone()[0]

            cursor = self.conn.execute("SELECT COUNT(*) FROM chunks")
            stats["total_chunks"] = cursor.fetchone()[0]

            cursor = self.conn.execute("SELECT COUNT(*) FROM embeddings")
            stats["embedded_chunks"] = cursor.fetchone()[0]

            cursor = self.conn.execute("SELECT COUNT(*) FROM doc_taxonomy")
            stats["taxonomy_mappings"] = cursor.fetchone()[0]

            return stats

        except Exception as e:
            logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

class HybridSearchSQLiteTester:
    """SQLite ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤í„°"""

    def __init__(self):
        self.search_dao = SQLiteSearchDAO()
        self.test_results = {}

        # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ë°ì´í„°
        self.test_documents = [
            {
                "title": "RAG ì‹œìŠ¤í…œ ê°œìš”",
                "text": "RAG(Retrieval-Augmented Generation)ëŠ” ê²€ìƒ‰ ê¸°ë°˜ ìƒì„± ëª¨ë¸ì…ë‹ˆë‹¤. ì´ ì‹œìŠ¤í…œì€ ì™¸ë¶€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤. ë²¡í„° ì„ë² ë”©ê³¼ ìœ ì‚¬ë„ ê³„ì‚°ì„ í†µí•´ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ìŠµë‹ˆë‹¤.",
                "category": ["AI", "RAG"]
            },
            {
                "title": "Machine Learning Classification",
                "text": "Machine learning classification algorithms include Support Vector Machines (SVM), Random Forest, and Neural Networks. These algorithms learn patterns from training data to classify new instances. Feature engineering and model selection are crucial for performance.",
                "category": ["AI", "ML"]
            },
            {
                "title": "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í™œìš©",
                "text": "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” ê³ ì°¨ì› ë²¡í„° ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥í•˜ê³  ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. pgvectorì™€ ê°™ì€ í™•ì¥ì„ í†µí•´ PostgreSQLì—ì„œë„ ë²¡í„° ì—°ì‚°ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì½”ì‚¬ì¸ ìœ ì‚¬ë„, ìœ í´ë¦¬ë“œ ê±°ë¦¬ ë“± ë‹¤ì–‘í•œ ê±°ë¦¬ ì¸¡ì • ë°©ë²•ì„ ì§€ì›í•©ë‹ˆë‹¤.",
                "category": ["AI", "Database"]
            },
            {
                "title": "Cross-Encoder Reranking",
                "text": "Cross-encoder models use BERT-like architectures to score query-document pairs directly. Unlike bi-encoders that create separate embeddings, cross-encoders process both inputs together for more accurate relevance scoring. This approach is commonly used for reranking search results.",
                "category": ["AI", "NLP"]
            },
            {
                "title": "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ",
                "text": "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì€ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰(BM25)ê³¼ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰(ë²¡í„° ìœ ì‚¬ë„)ì„ ê²°í•©í•©ë‹ˆë‹¤. ê° ë°©ë²•ì˜ ì¥ì ì„ í™œìš©í•˜ì—¬ ë” ì •í™•í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°€ì¤‘ì¹˜ ì¡°ì •ê³¼ ì¬ë­í‚¹ ê³¼ì •ì„ í†µí•´ ìµœì¢… ê²°ê³¼ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤.",
                "category": ["AI", "Search"]
            }
        ]

        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
        self.test_queries = [
            "RAG ì‹œìŠ¤í…œ ì„¤ëª…",
            "machine learning algorithms",
            "ë²¡í„° ê²€ìƒ‰ ë°©ë²•",
            "What is retrieval augmented generation?",
            "ì¸ê³µì§€ëŠ¥ ë¶„ë¥˜ ê¸°ë²•",
            "neural network architecture",
            "ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”",
            "embedding model comparison"
        ]

    async def setup_test_data(self) -> bool:
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¤ì •"""
        logger.info("SQLite í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¤ì • ì‹œì‘")
        return await self.search_dao.insert_test_data(self.test_documents)

    async def test_bm25_component(self) -> Dict[str, Any]:
        """BM25 ì»´í¬ë„ŒíŠ¸ ê°œë³„ í…ŒìŠ¤íŠ¸"""
        logger.info("BM25 ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")

        results = {"success": False, "test_cases": [], "performance": {}}

        try:
            start_time = time.time()

            for query in self.test_queries[:4]:  # ì²˜ìŒ 4ê°œ ì¿¼ë¦¬ë¡œ í…ŒìŠ¤íŠ¸
                search_start = time.time()
                bm25_results = await self.search_dao.bm25_search(query, topk=5)
                search_time = time.time() - search_start

                test_case = {
                    "query": query,
                    "result_count": len(bm25_results),
                    "search_time": search_time,
                    "has_results": len(bm25_results) > 0,
                    "top_score": bm25_results[0]["score"] if bm25_results else 0,
                    "performance_acceptable": search_time < 1.0
                }

                # ê²°ê³¼ í’ˆì§ˆ ê²€ì¦
                if bm25_results:
                    # ìŠ¤ì½”ì–´ê°€ ë‚´ë¦¼ì°¨ìˆœì¸ì§€ í™•ì¸
                    scores = [r["score"] for r in bm25_results]
                    test_case["scores_descending"] = all(
                        scores[i] >= scores[i+1] for i in range(len(scores)-1)
                    )
                    # ëª¨ë“  ê²°ê³¼ê°€ í•„ìˆ˜ í•„ë“œë¥¼ ê°€ì§€ëŠ”ì§€ í™•ì¸
                    test_case["results_valid"] = all(
                        "chunk_id" in r and "text" in r and "score" in r
                        for r in bm25_results
                    )
                else:
                    test_case["scores_descending"] = True
                    test_case["results_valid"] = True

                results["test_cases"].append(test_case)

            # ì„±ëŠ¥ ë©”íŠ¸ë¦­
            total_time = time.time() - start_time
            results["performance"] = {
                "total_time": total_time,
                "avg_search_time": total_time / len(results["test_cases"]),
                "searches_per_second": len(results["test_cases"]) / total_time
            }

            # ì„±ê³µ ì—¬ë¶€ íŒë‹¨
            successful_tests = sum(
                1 for test in results["test_cases"]
                if test["has_results"] and test["scores_descending"] and test["results_valid"]
            )
            results["success"] = successful_tests >= len(results["test_cases"]) * 0.75

        except Exception as e:
            logger.error(f"BM25 í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            results["error"] = str(e)

        self.test_results["bm25_component"] = results
        logger.info(f"BM25 ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ì„±ê³µ: {results['success']}")
        return results

    async def test_vector_component(self) -> Dict[str, Any]:
        """ë²¡í„° ê²€ìƒ‰ ì»´í¬ë„ŒíŠ¸ ê°œë³„ í…ŒìŠ¤íŠ¸"""
        logger.info("ë²¡í„° ê²€ìƒ‰ ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")

        results = {"success": False, "test_cases": [], "performance": {}}

        try:
            start_time = time.time()

            for query in self.test_queries[:4]:
                search_start = time.time()
                vector_results = await self.search_dao.vector_search(query, topk=5)
                search_time = time.time() - search_start

                test_case = {
                    "query": query,
                    "result_count": len(vector_results),
                    "search_time": search_time,
                    "has_results": len(vector_results) > 0,
                    "top_score": vector_results[0]["score"] if vector_results else 0,
                    "performance_acceptable": search_time < 2.0
                }

                # ìœ ì‚¬ë„ ì ìˆ˜ ê²€ì¦ (0~1 ë²”ìœ„)
                if vector_results:
                    scores = [r["score"] for r in vector_results]
                    test_case["scores_valid"] = all(0 <= score <= 1 for score in scores)
                    test_case["scores_descending"] = all(
                        scores[i] >= scores[i+1] for i in range(len(scores)-1)
                    )
                    test_case["results_valid"] = all(
                        "chunk_id" in r and "text" in r and "score" in r
                        for r in vector_results
                    )
                else:
                    test_case["scores_valid"] = True
                    test_case["scores_descending"] = True
                    test_case["results_valid"] = True

                results["test_cases"].append(test_case)

            # ì„±ëŠ¥ ë©”íŠ¸ë¦­
            total_time = time.time() - start_time
            results["performance"] = {
                "total_time": total_time,
                "avg_search_time": total_time / len(results["test_cases"]),
                "searches_per_second": len(results["test_cases"]) / total_time
            }

            # ì„±ê³µ ì—¬ë¶€ íŒë‹¨
            successful_tests = sum(
                1 for test in results["test_cases"]
                if test["has_results"] and test["scores_valid"] and
                   test["scores_descending"] and test["results_valid"]
            )
            results["success"] = successful_tests >= len(results["test_cases"]) * 0.75

        except Exception as e:
            logger.error(f"ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            results["error"] = str(e)

        self.test_results["vector_component"] = results
        logger.info(f"ë²¡í„° ê²€ìƒ‰ ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ì„±ê³µ: {results['success']}")
        return results

    async def test_hybrid_integration(self) -> Dict[str, Any]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í†µí•© í…ŒìŠ¤íŠ¸"""
        logger.info("í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")

        results = {"success": False, "test_cases": [], "performance": {}}

        try:
            start_time = time.time()

            for query in self.test_queries:
                search_start = time.time()
                hybrid_results = await self.search_dao.hybrid_search(
                    query, topk=5, bm25_topk=10, vector_topk=10
                )
                search_time = time.time() - search_start

                test_case = {
                    "query": query,
                    "result_count": len(hybrid_results),
                    "search_time": search_time,
                    "has_results": len(hybrid_results) > 0,
                    "performance_acceptable": search_time < 3.0,
                    "source_diversity": {}
                }

                # ê²°ê³¼ í’ˆì§ˆ ë¶„ì„
                if hybrid_results:
                    # ì†ŒìŠ¤ ë‹¤ì–‘ì„± (BM25, Vector, Hybrid)
                    sources = [r["metadata"]["source"] for r in hybrid_results]
                    test_case["source_diversity"] = {
                        source: sources.count(source) for source in set(sources)
                    }

                    # ìŠ¤ì½”ì–´ ê²€ì¦
                    scores = [r["score"] for r in hybrid_results]
                    test_case["scores_descending"] = all(
                        scores[i] >= scores[i+1] for i in range(len(scores)-1)
                    )

                    # ë©”íƒ€ë°ì´í„° ì™„ì„±ë„
                    test_case["metadata_complete"] = all(
                        "bm25_score" in r["metadata"] and
                        "vector_score" in r["metadata"] and
                        "source" in r["metadata"]
                        for r in hybrid_results
                    )

                    # í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ê°€ ì¡´ì¬í•˜ëŠ”ì§€ (hybrid ì†ŒìŠ¤ì¸ ê²½ìš°)
                    hybrid_results_exist = any(
                        r["metadata"]["source"] == "hybrid" for r in hybrid_results
                    )
                    test_case["hybrid_scoring_works"] = hybrid_results_exist

                else:
                    test_case["scores_descending"] = True
                    test_case["metadata_complete"] = True
                    test_case["hybrid_scoring_works"] = False

                results["test_cases"].append(test_case)

            # ì„±ëŠ¥ ë©”íŠ¸ë¦­
            total_time = time.time() - start_time
            search_times = [test["search_time"] for test in results["test_cases"]]

            results["performance"] = {
                "total_time": total_time,
                "avg_search_time": sum(search_times) / len(search_times),
                "p95_search_time": sorted(search_times)[int(len(search_times) * 0.95)],
                "searches_per_second": len(results["test_cases"]) / total_time,
                "latency_target_met": all(t < 3.0 for t in search_times)
            }

            # ì„±ê³µ ì—¬ë¶€ íŒë‹¨
            successful_tests = sum(
                1 for test in results["test_cases"]
                if test["has_results"] and test["scores_descending"] and
                   test["metadata_complete"] and test["performance_acceptable"]
            )
            results["success"] = successful_tests >= len(results["test_cases"]) * 0.8

        except Exception as e:
            logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            results["error"] = str(e)

        self.test_results["hybrid_integration"] = results
        logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ì„±ê³µ: {results['success']}")
        return results

    async def test_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸"""
        logger.info("ì„±ëŠ¥ ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸ ì‹œì‘")

        results = {"success": False, "metrics": {}, "benchmarks": {}}

        try:
            # ë²¤ì¹˜ë§ˆí¬ ì¿¼ë¦¬ë“¤
            benchmark_queries = [
                "ë‹¨ìˆœ í‚¤ì›Œë“œ",
                "complex machine learning algorithm optimization",
                "í•œêµ­ì–´ì™€ ì˜ì–´ê°€ í˜¼ì¬ëœ ë³µí•© ì¿¼ë¦¬ multilingual",
                "ë§¤ìš° ê¸´ ì¿¼ë¦¬ë¬¸ìœ¼ë¡œ ì‹œìŠ¤í…œì˜ ì²˜ë¦¬ ëŠ¥ë ¥ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” ì¼€ì´ìŠ¤ì…ë‹ˆë‹¤ very long query to test system processing capabilities"
            ]

            # ê° ê²€ìƒ‰ íƒ€ì…ë³„ ì„±ëŠ¥ ì¸¡ì •
            for search_type in ["bm25", "vector", "hybrid"]:
                search_times = []

                for query in benchmark_queries:
                    start_time = time.time()

                    if search_type == "bm25":
                        await self.search_dao.bm25_search(query, topk=10)
                    elif search_type == "vector":
                        await self.search_dao.vector_search(query, topk=10)
                    else:  # hybrid
                        await self.search_dao.hybrid_search(query, topk=5)

                    search_times.append(time.time() - start_time)

                results["benchmarks"][search_type] = {
                    "avg_latency": sum(search_times) / len(search_times),
                    "min_latency": min(search_times),
                    "max_latency": max(search_times),
                    "p95_latency": sorted(search_times)[int(len(search_times) * 0.95)],
                    "throughput": len(search_times) / sum(search_times)
                }

            # ë°ì´í„°ë² ì´ìŠ¤ í†µê³„
            db_stats = self.search_dao.get_stats()
            results["metrics"]["database"] = db_stats

            # ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
            targets = {
                "bm25_avg_latency": 0.5,  # 500ms
                "vector_avg_latency": 1.0,  # 1s
                "hybrid_avg_latency": 2.0,  # 2s
                "hybrid_p95_latency": 3.0   # 3s
            }

            results["metrics"]["target_achievement"] = {
                "bm25_target_met": results["benchmarks"]["bm25"]["avg_latency"] < targets["bm25_avg_latency"],
                "vector_target_met": results["benchmarks"]["vector"]["avg_latency"] < targets["vector_avg_latency"],
                "hybrid_avg_target_met": results["benchmarks"]["hybrid"]["avg_latency"] < targets["hybrid_avg_latency"],
                "hybrid_p95_target_met": results["benchmarks"]["hybrid"]["p95_latency"] < targets["hybrid_p95_latency"]
            }

            # ì „ì²´ ì„±ê³µ ì—¬ë¶€
            targets_met = sum(results["metrics"]["target_achievement"].values())
            results["success"] = targets_met >= 3  # 4ê°œ ì¤‘ 3ê°œ ì´ìƒ ë‹¬ì„±

        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            results["error"] = str(e)

        self.test_results["performance_metrics"] = results
        logger.info(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ì„±ê³µ: {results['success']}")
        return results

    async def test_encoding_compatibility(self) -> Dict[str, Any]:
        """UTF-8 ì¸ì½”ë”© í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
        logger.info("UTF-8 ì¸ì½”ë”© í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹œì‘")

        results = {"success": False, "test_cases": [], "character_tests": {}}

        try:
            # ë‹¤êµ­ì–´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
            multilingual_queries = [
                "í•œê¸€ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸",
                "English search test",
                "æ—¥æœ¬èªæ¤œç´¢ãƒ†ã‚¹ãƒˆ",
                "Ğ ÑƒÑÑĞºĞ¸Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ñ‚ĞµÑÑ‚",
                "BÃºsqueda en espaÃ±ol",
                "Recherche franÃ§aise",
                "í•œê¸€ê³¼ English í˜¼í•© query",
                "íŠ¹ìˆ˜ë¬¸ì @#$%^&*() í¬í•¨",
                "ì´ëª¨ì§€ ğŸ˜€ğŸš€ğŸ” í¬í•¨ ê²€ìƒ‰"
            ]

            for query in multilingual_queries:
                test_case = {
                    "query": query,
                    "encoding": "utf-8",
                    "tests": {}
                }

                try:
                    # BM25 í† í°í™” í…ŒìŠ¤íŠ¸
                    tokens = BM25Scorer.preprocess_text(query)
                    test_case["tests"]["tokenization"] = {
                        "success": True,
                        "token_count": len(tokens),
                        "sample_tokens": tokens[:3]
                    }
                except Exception as e:
                    test_case["tests"]["tokenization"] = {
                        "success": False,
                        "error": str(e)
                    }

                try:
                    # ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸
                    embedding = await EmbeddingService.generate_embedding(query)
                    test_case["tests"]["embedding"] = {
                        "success": len(embedding) > 0,
                        "embedding_length": len(embedding)
                    }
                except Exception as e:
                    test_case["tests"]["embedding"] = {
                        "success": False,
                        "error": str(e)
                    }

                try:
                    # ì‹¤ì œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
                    search_results = await self.search_dao.hybrid_search(query, topk=3)
                    test_case["tests"]["search"] = {
                        "success": True,
                        "result_count": len(search_results)
                    }
                except Exception as e:
                    test_case["tests"]["search"] = {
                        "success": False,
                        "error": str(e)
                    }

                # ì „ì²´ ì„±ê³µ ì—¬ë¶€
                test_case["overall_success"] = all(
                    test.get("success", False)
                    for test in test_case["tests"].values()
                )

                results["test_cases"].append(test_case)

            # íŠ¹ìˆ˜ ë¬¸ì ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
            special_chars = {
                "punctuation": ",.!?;:\"'",
                "math_symbols": "âˆ‘âˆ«âˆ†âˆ‡âˆÂ±",
                "currency": "$â‚¬Â¥â‚©",
                "emoji": "ğŸ˜€ğŸš€ğŸ”ğŸ’¡",
                "cjk": "ä¸­æ–‡ æ—¥æœ¬èª í•œê¸€"
            }

            for category, chars in special_chars.items():
                try:
                    test_query = f"Test {category}: {chars}"
                    tokens = BM25Scorer.preprocess_text(test_query)
                    results["character_tests"][category] = {
                        "success": True,
                        "processed_tokens": len(tokens)
                    }
                except Exception as e:
                    results["character_tests"][category] = {
                        "success": False,
                        "error": str(e)
                    }

            # ì „ì²´ ì„±ê³µë¥  ê³„ì‚°
            successful_cases = sum(1 for case in results["test_cases"] if case["overall_success"])
            successful_chars = sum(1 for test in results["character_tests"].values() if test["success"])

            case_success_rate = successful_cases / len(results["test_cases"])
            char_success_rate = successful_chars / len(results["character_tests"])

            results["success"] = case_success_rate >= 0.8 and char_success_rate >= 0.8

        except Exception as e:
            logger.error(f"ì¸ì½”ë”© í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            results["error"] = str(e)

        self.test_results["encoding_compatibility"] = results
        logger.info(f"ì¸ì½”ë”© í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ì„±ê³µ: {results['success']}")
        return results

    async def run_all_tests(self) -> Dict[str, Any]:
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("=== SQLite í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")

        overall_start = time.time()

        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¤ì •
        setup_success = await self.setup_test_data()
        if not setup_success:
            logger.error("í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¤ì • ì‹¤íŒ¨")
            return {"error": "í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¤ì • ì‹¤íŒ¨"}

        # ê°œë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_functions = [
            ("BM25 ì»´í¬ë„ŒíŠ¸", self.test_bm25_component),
            ("ë²¡í„° ê²€ìƒ‰ ì»´í¬ë„ŒíŠ¸", self.test_vector_component),
            ("í•˜ì´ë¸Œë¦¬ë“œ í†µí•©", self.test_hybrid_integration),
            ("ì„±ëŠ¥ ë©”íŠ¸ë¦­", self.test_performance_metrics),
            ("ì¸ì½”ë”© í˜¸í™˜ì„±", self.test_encoding_compatibility)
        ]

        for test_name, test_func in test_functions:
            logger.info(f"{test_name} í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
            try:
                await test_func()
            except Exception as e:
                logger.error(f"{test_name} í…ŒìŠ¤íŠ¸ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
                logger.error(traceback.format_exc())

        # ì „ì²´ ê²°ê³¼ ìƒì„±
        total_time = time.time() - overall_start

        # ì„±ê³µë¥  ê³„ì‚°
        success_counts = {}
        for test_category, test_result in self.test_results.items():
            success_counts[test_category] = test_result.get("success", False)

        overall_success_rate = sum(success_counts.values()) / len(success_counts) if success_counts else 0

        # ë°ì´í„°ë² ì´ìŠ¤ í†µê³„
        db_stats = self.search_dao.get_stats()

        final_results = {
            "test_summary": {
                "total_time": total_time,
                "setup_success": setup_success,
                "tests_completed": len(test_functions),
                "overall_success_rate": overall_success_rate,
                "timestamp": datetime.now().isoformat()
            },
            "database_stats": db_stats,
            "individual_results": self.test_results,
            "success_summary": success_counts,
            "recommendations": self._generate_recommendations(success_counts)
        }

        # ê²°ê³¼ ë¡œê¹…
        logger.info("=== í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ===")
        logger.info(f"ì „ì²´ ì„±ê³µë¥ : {overall_success_rate:.1%}")
        logger.info(f"ì „ì²´ ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ")
        logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ í†µê³„: {db_stats}")

        return final_results

    def _generate_recommendations(self, success_counts: Dict[str, bool]) -> List[str]:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []

        if not success_counts.get("bm25_component", False):
            recommendations.append("BM25 ì»´í¬ë„ŒíŠ¸ ê°œì„ : í† í°í™” ë¡œì§ê³¼ ìŠ¤ì½”ì–´ë§ ì•Œê³ ë¦¬ì¦˜ì„ ìµœì í™”í•˜ì„¸ìš”.")

        if not success_counts.get("vector_component", False):
            recommendations.append("ë²¡í„° ê²€ìƒ‰ ê°œì„ : ì„ë² ë”© ìƒì„±ê³¼ ìœ ì‚¬ë„ ê³„ì‚° ë¡œì§ì„ ì ê²€í•˜ì„¸ìš”.")

        if not success_counts.get("hybrid_integration", False):
            recommendations.append("í•˜ì´ë¸Œë¦¬ë“œ í†µí•© ê°œì„ : BM25ì™€ ë²¡í„° ê²€ìƒ‰ì˜ ê°€ì¤‘ì¹˜ ì¡°ì •ì„ ìµœì í™”í•˜ì„¸ìš”.")

        if not success_counts.get("performance_metrics", False):
            recommendations.append("ì„±ëŠ¥ ìµœì í™”: ê²€ìƒ‰ ì§€ì—°ì‹œê°„ì„ ì¤„ì´ê¸° ìœ„í•´ ì¸ë±ì‹±ê³¼ ìºì‹±ì„ ê°œì„ í•˜ì„¸ìš”.")

        if not success_counts.get("encoding_compatibility", False):
            recommendations.append("ì¸ì½”ë”© í˜¸í™˜ì„± ê°œì„ : ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë¡œì§ì„ ê°•í™”í•˜ì„¸ìš”.")

        if not recommendations:
            recommendations.append("ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µí–ˆìŠµë‹ˆë‹¤! ì‹œìŠ¤í…œì´ ì˜¬ë°”ë¥´ê²Œ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.")

        return recommendations

    def save_results(self, results: Dict[str, Any], filename: str = "sqlite_hybrid_search_results.json"):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2, default=str)
            logger.info(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("SQLite í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)

    tester = HybridSearchSQLiteTester()

    try:
        # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        results = await tester.run_all_tests()

        # ê²°ê³¼ ì €ì¥
        tester.save_results(results)

        # ìš”ì•½ ì¶œë ¥
        print("\n" + "=" * 60)
        print("í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"ì „ì²´ ì„±ê³µë¥ : {results['test_summary']['overall_success_rate']:.1%}")
        print(f"ì´ ì‹¤í–‰ì‹œê°„: {results['test_summary']['total_time']:.2f}ì´ˆ")

        if results.get('recommendations'):
            print("\nê¶Œì¥ì‚¬í•­:")
            for rec in results['recommendations']:
                print(f"  - {rec}")

        print(f"\nìƒì„¸ ê²°ê³¼: sqlite_hybrid_search_results.json")
        print("=" * 60)

        return results['test_summary']['overall_success_rate'] >= 0.7

    except Exception as e:
        logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.error(traceback.format_exc())
        return False

if __name__ == "__main__":
    # ë¹„ë™ê¸° ì‹¤í–‰
    import platform
    if platform.system() == "Windows":
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

    success = asyncio.run(main())
    sys.exit(0 if success else 1)