"""
RAGAS Evaluation System Demo

ì‹¤ì œ ë™ì‘í•˜ëŠ” RAGAS í‰ê°€ ì‹œìŠ¤í…œ ë°ëª¨ì…ë‹ˆë‹¤.
DT-RAG v1.8.1ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ëª¨ë“  ê¸°ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

ì‹¤í–‰ ë°©ë²•:
python ragas_evaluation_demo.py
"""

import asyncio
import json
import os
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any

# Import our RAGAS evaluation system
from apps.evaluation.ragas_engine import RAGASEvaluator
from apps.evaluation.quality_monitor import QualityMonitor
from apps.evaluation.experiment_tracker import ExperimentTracker
from apps.evaluation.sample_data import SampleDataGenerator
from apps.evaluation.models import EvaluationRequest, QualityThresholds, ExperimentConfig

class RAGASDemo:
    """RAGAS í‰ê°€ ì‹œìŠ¤í…œ ì‹¤ì œ ë°ëª¨"""

    def __init__(self):
        print("ğŸš€ DT-RAG RAGAS í‰ê°€ ì‹œìŠ¤í…œ v1.8.1 ë°ëª¨")
        print("=" * 60)

        # ì‹œìŠ¤í…œ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
        self.evaluator = RAGASEvaluator()
        self.quality_monitor = QualityMonitor()
        self.experiment_tracker = ExperimentTracker()
        self.sample_generator = SampleDataGenerator()

        # API í‚¤ í™•ì¸
        self.has_gemini = bool(os.getenv("GEMINI_API_KEY"))
        print(f"ğŸ”‘ Gemini API: {'âœ… Available' if self.has_gemini else 'âŒ Not available (using fallbacks)'}")
        print()

    async def demo_basic_evaluation(self):
        """ê¸°ë³¸ RAGAS í‰ê°€ ë°ëª¨"""
        print("ğŸ“Š 1. ê¸°ë³¸ RAGAS í‰ê°€ ë°ëª¨")
        print("-" * 40)

        # ì‹¤ì œ RAG ì‹œë‚˜ë¦¬ì˜¤ ì˜ˆì‹œ
        query = "RAG ì‹œìŠ¤í…œì—ì„œ Context Precisionì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"

        response = """Context Precisionì€ RAG ì‹œìŠ¤í…œì—ì„œ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì˜ ì •í™•ë„ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œì…ë‹ˆë‹¤.
        ì´ëŠ” ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ì¤‘ì—ì„œ ì‹¤ì œë¡œ ì¿¼ë¦¬ì™€ ê´€ë ¨ì´ ìˆëŠ” ì»¨í…ìŠ¤íŠ¸ì˜ ë¹„ìœ¨ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
        Context Precision = (ê´€ë ¨ìˆëŠ” ì»¨í…ìŠ¤íŠ¸ ìˆ˜) / (ì „ì²´ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ìˆ˜) ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤.
        ë†’ì€ Context Precisionì€ ê²€ìƒ‰ ì‹œìŠ¤í…œì´ ë¶ˆí•„ìš”í•œ ì •ë³´ë¥¼ ìµœì†Œí™”í•˜ê³  ê´€ë ¨ì„± ë†’ì€ ì •ë³´ë§Œì„ ì œê³µí•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤."""

        contexts = [
            "Context Precisionì€ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ì¤‘ ì‹¤ì œë¡œ ê´€ë ¨ìˆëŠ” ì»¨í…ìŠ¤íŠ¸ì˜ ë¹„ìœ¨ì„ ì¸¡ì •í•˜ëŠ” RAGAS ë©”íŠ¸ë¦­ì…ë‹ˆë‹¤.",
            "RAGASì—ì„œ Context Precisionì€ ê²€ìƒ‰ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” í•µì‹¬ ì§€í‘œ ì¤‘ í•˜ë‚˜ë¡œ, ë¶ˆí•„ìš”í•œ ì •ë³´ì˜ ê²€ìƒ‰ì„ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.",
            "Context Precisionì´ ë†’ì„ìˆ˜ë¡ RAG ì‹œìŠ¤í…œì´ ë” ì •í™•í•œ ì •ë³´ ê²€ìƒ‰ì„ ìˆ˜í–‰í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "ì‚¬ê³¼ëŠ” ë¹¨ê°„ìƒ‰ì´ê³  ë‹¬ì½¤í•œ ê³¼ì¼ì…ë‹ˆë‹¤."  # ê´€ë ¨ì„± ì—†ëŠ” ì»¨í…ìŠ¤íŠ¸ (í…ŒìŠ¤íŠ¸ìš©)
        ]

        print(f"ğŸ” ì§ˆë¬¸: {query}")
        print(f"ğŸ’¬ ë‹µë³€: {response[:100]}...")
        print(f"ğŸ“š ì»¨í…ìŠ¤íŠ¸ ìˆ˜: {len(contexts)}ê°œ")
        print("\nâ³ í‰ê°€ ì¤‘...")

        start_time = time.time()

        # RAGAS í‰ê°€ ì‹¤í–‰
        result = await self.evaluator.evaluate_rag_response(
            query=query,
            response=response,
            retrieved_contexts=contexts
        )

        end_time = time.time()
        evaluation_time = end_time - start_time

        # ê²°ê³¼ ì¶œë ¥
        print(f"âš¡ í‰ê°€ ì™„ë£Œ ({evaluation_time:.2f}ì´ˆ)")
        print("\nğŸ“ˆ RAGAS ë©”íŠ¸ë¦­ ê²°ê³¼:")
        print(f"  â€¢ Faithfulness (ì‚¬ì‹¤ì„±):     {result.metrics.faithfulness:.3f}")
        print(f"  â€¢ Context Precision (ì •ë°€ë„): {result.metrics.context_precision:.3f}")
        print(f"  â€¢ Context Recall (ì¬í˜„ìœ¨):    {result.metrics.context_recall:.3f}")
        print(f"  â€¢ Answer Relevancy (ê´€ë ¨ì„±):  {result.metrics.answer_relevancy:.3f}")

        if result.quality_flags:
            print(f"\nâš ï¸  í’ˆì§ˆ ê²½ê³ : {', '.join(result.quality_flags)}")
        else:
            print("\nâœ… í’ˆì§ˆ ê²€ì‚¬ í†µê³¼")

        if result.recommendations:
            print("\nğŸ’¡ ê°œì„  ê¶Œì¥ì‚¬í•­:")
            for i, rec in enumerate(result.recommendations, 1):
                print(f"  {i}. {rec}")

        return result

    async def demo_quality_monitoring(self):
        """í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ë°ëª¨"""
        print("\nğŸ” 2. í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ë°ëª¨")
        print("-" * 40)

        # ë‹¤ì–‘í•œ í’ˆì§ˆì˜ í‰ê°€ ë°ì´í„° ìƒì„±
        evaluation_requests = self.sample_generator.generate_evaluation_requests(15)

        print(f"ğŸ“Š {len(evaluation_requests)}ê°œì˜ í‰ê°€ ë°ì´í„° ì²˜ë¦¬ ì¤‘...")

        alert_count = 0
        processed_count = 0

        for i, request in enumerate(evaluation_requests):
            # í‰ê°€ ì‹¤í–‰
            result = await self.evaluator.evaluate_rag_response(
                query=request.query,
                response=request.response,
                retrieved_contexts=request.retrieved_contexts
            )

            # í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ì— ê¸°ë¡
            alerts = await self.quality_monitor.record_evaluation(result)
            alert_count += len(alerts)
            processed_count += 1

            # ì§„í–‰ìƒí™© í‘œì‹œ (ë§¤ 5ê°œë§ˆë‹¤)
            if (i + 1) % 5 == 0:
                print(f"  âœ“ {i + 1}ê°œ ì²˜ë¦¬ ì™„ë£Œ")

        # í’ˆì§ˆ ìƒíƒœ í™•ì¸
        quality_status = await self.quality_monitor.get_quality_status()

        print(f"\nğŸ“‹ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ê²°ê³¼:")
        print(f"  â€¢ ì²˜ë¦¬ëœ í‰ê°€: {processed_count}ê°œ")
        print(f"  â€¢ ë°œìƒí•œ ì•Œë¦¼: {alert_count}ê°œ")

        current_metrics = quality_status.get('current_metrics', {})
        if current_metrics:
            print(f"\nğŸ“Š í˜„ì¬ í’ˆì§ˆ ì§€í‘œ:")
            for metric, value in current_metrics.items():
                if not metric.endswith('_trend') and not metric.endswith('_p95'):
                    print(f"  â€¢ {metric}: {value:.3f}")

        # í’ˆì§ˆ ê²Œì´íŠ¸ ìƒíƒœ
        quality_gates = quality_status.get('quality_gates', {})
        if quality_gates.get('gates'):
            print(f"\nğŸšª í’ˆì§ˆ ê²Œì´íŠ¸ ìƒíƒœ:")
            overall_passing = quality_gates.get('overall_passing', False)
            print(f"  â€¢ ì „ì²´ í†µê³¼ ì—¬ë¶€: {'âœ… í†µê³¼' if overall_passing else 'âŒ ì‹¤íŒ¨'}")

            for gate_name, gate_info in quality_gates['gates'].items():
                status = "âœ…" if gate_info.get('passing') else "âŒ"
                value = gate_info.get('current_value', 0)
                threshold = gate_info.get('threshold', 0)
                print(f"  â€¢ {gate_name}: {status} ({value:.3f} / {threshold:.3f})")

        # ê¶Œì¥ì‚¬í•­
        recommendations = quality_status.get('recommendations', [])
        if recommendations:
            print(f"\nğŸ’¡ ì‹œìŠ¤í…œ ê¶Œì¥ì‚¬í•­:")
            for i, rec in enumerate(recommendations, 1):
                print(f"  {i}. {rec}")

        return quality_status

    async def demo_ab_testing(self):
        """A/B í…ŒìŠ¤íŠ¸ ë°ëª¨"""
        print("\nğŸ§ª 3. A/B í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ ë°ëª¨")
        print("-" * 40)

        # ì‹¤í—˜ ì„¤ì •
        config = ExperimentConfig(
            experiment_id="demo_retrieval_test",
            name="ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¹„êµ",
            description="BM25 vs í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜",
            control_config={
                "search_type": "bm25_only",
                "top_k": 10,
                "rerank": False
            },
            treatment_config={
                "search_type": "hybrid",
                "bm25_weight": 0.3,
                "vector_weight": 0.7,
                "top_k": 10,
                "rerank": True
            },
            minimum_sample_size=30
        )

        print(f"ğŸ”¬ ì‹¤í—˜ ì„¤ì •: {config.name}")
        print(f"ğŸ“‹ ì„¤ëª…: {config.description}")
        print(f"ğŸ‘¥ ìµœì†Œ ìƒ˜í”Œ í¬ê¸°: {config.minimum_sample_size}")

        # ì‹¤í—˜ ìƒì„± ë° ì‹œì‘
        experiment_id = await self.experiment_tracker.create_experiment(config)
        await self.experiment_tracker.start_experiment(experiment_id)

        print(f"ğŸš€ ì‹¤í—˜ ì‹œì‘: {experiment_id}")

        # ê°€ìƒ ì‚¬ìš©ì ë°ì´í„° ìƒì„±
        print("\nğŸ‘¥ ê°€ìƒ ì‚¬ìš©ì ìƒí˜¸ì‘ìš© ì‹œë®¬ë ˆì´ì…˜...")

        for user_idx in range(60):
            user_id = f"demo_user_{user_idx}"

            # ì‚¬ìš©ì ê·¸ë£¹ í• ë‹¹
            group = self.experiment_tracker.assign_user_to_experiment(user_id, experiment_id)

            # ê·¸ë£¹ì— ë”°ë¥¸ ì„±ëŠ¥ ì‹œë®¬ë ˆì´ì…˜
            if group == "control":
                # ëŒ€ì¡°êµ°: ì•½ê°„ ë‚®ì€ ì„±ëŠ¥
                performance_modifier = 0.95
            else:
                # ì‹¤í—˜êµ°: ì•½ê°„ ë†’ì€ ì„±ëŠ¥
                performance_modifier = 1.05

            # ëª¨ì˜ í‰ê°€ ê²°ê³¼ ìƒì„±
            from apps.evaluation.models import EvaluationResult, EvaluationMetrics
            import random

            mock_result = EvaluationResult(
                evaluation_id=f"demo_{user_id}",
                query="demo query",
                metrics=EvaluationMetrics(
                    faithfulness=min(0.99, 0.82 * performance_modifier + random.gauss(0, 0.03)),
                    context_precision=min(0.99, 0.78 * performance_modifier + random.gauss(0, 0.04)),
                    context_recall=min(0.99, 0.74 * performance_modifier + random.gauss(0, 0.05)),
                    answer_relevancy=min(0.99, 0.80 * performance_modifier + random.gauss(0, 0.03))
                ),
                quality_flags=[],
                recommendations=[],
                timestamp=datetime.utcnow()
            )

            await self.experiment_tracker.record_experiment_result(
                experiment_id, user_id, mock_result
            )

            # ì§„í–‰ìƒí™© í‘œì‹œ
            if (user_idx + 1) % 20 == 0:
                print(f"  âœ“ {user_idx + 1}ëª… ì²˜ë¦¬ ì™„ë£Œ")

        # ì‹¤í—˜ ê²°ê³¼ ë¶„ì„
        print("\nğŸ“Š ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ì¤‘...")
        results = await self.experiment_tracker.analyze_experiment_results(experiment_id)

        if results:
            print(f"\nğŸ¯ ì‹¤í—˜ ê²°ê³¼:")
            print(f"  â€¢ ëŒ€ì¡°êµ° ìƒ˜í”Œ: {results.control_samples}ê°œ")
            print(f"  â€¢ ì‹¤í—˜êµ° ìƒ˜í”Œ: {results.treatment_samples}ê°œ")
            print(f"  â€¢ í†µê³„ì  ìœ ì˜ì„±: {'âœ… ìœ ì˜í•¨' if results.is_statistically_significant else 'âŒ ìœ ì˜í•˜ì§€ ì•ŠìŒ'}")
            print(f"  â€¢ ê¶Œì¥ì‚¬í•­: {results.recommendation}")

            print(f"\nğŸ“ˆ ë©”íŠ¸ë¦­ë³„ ë¹„êµ:")
            for metric, comparison in results.metric_comparisons.items():
                if comparison and 'control_mean' in comparison:
                    control_mean = comparison['control_mean']
                    treatment_mean = comparison['treatment_mean']
                    improvement = ((treatment_mean - control_mean) / control_mean * 100)
                    significance = "ğŸ“ˆ" if comparison.get('is_significant') else "ğŸ“Š"

                    print(f"  â€¢ {metric}:")
                    print(f"    - ëŒ€ì¡°êµ°: {control_mean:.3f}")
                    print(f"    - ì‹¤í—˜êµ°: {treatment_mean:.3f}")
                    print(f"    - ë³€í™”ìœ¨: {improvement:+.1f}% {significance}")

            print(f"\nğŸ“ ìš”ì•½: {results.summary}")

        # ì‹¤í—˜ ì¢…ë£Œ
        await self.experiment_tracker.stop_experiment(experiment_id, "ë°ëª¨ ì™„ë£Œ")
        print(f"â¹ï¸  ì‹¤í—˜ ì¢…ë£Œ")

        return results

    async def demo_golden_dataset(self):
        """ê³¨ë“  ë°ì´í„°ì…‹ ë°ëª¨"""
        print("\nğŸ† 4. ê³¨ë“  ë°ì´í„°ì…‹ ê´€ë¦¬ ë°ëª¨")
        print("-" * 40)

        # ê³¨ë“  ë°ì´í„°ì…‹ ìƒì„±
        golden_entries = self.sample_generator.generate_golden_dataset(20)
        print(f"ğŸ“š {len(golden_entries)}ê°œì˜ ê³¨ë“  ë°ì´í„°ì…‹ í•­ëª© ìƒì„±")

        # ë°ì´í„°ì…‹ ê²€ì¦
        validation_errors = []
        quality_scores = []

        for i, entry in enumerate(golden_entries):
            # ê¸°ë³¸ ê²€ì¦
            entry_score = 1.0

            if len(entry.query.split()) < 3:
                validation_errors.append(f"í•­ëª© {i}: ì§ˆë¬¸ì´ ë„ˆë¬´ ì§§ìŒ")
                entry_score -= 0.2

            if len(entry.ground_truth_answer.split()) < 10:
                validation_errors.append(f"í•­ëª© {i}: ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŒ")
                entry_score -= 0.2

            if len(entry.expected_contexts) < 2:
                validation_errors.append(f"í•­ëª© {i}: ì˜ˆìƒ ì»¨í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±")
                entry_score -= 0.3

            quality_scores.append(max(0, entry_score))

        # ê²€ì¦ ê²°ê³¼
        overall_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
        valid_entries = len([s for s in quality_scores if s > 0.8])

        print(f"\nâœ… ë°ì´í„°ì…‹ ê²€ì¦ ê²°ê³¼:")
        print(f"  â€¢ ì „ì²´ í’ˆì§ˆ ì ìˆ˜: {overall_quality:.2f}")
        print(f"  â€¢ ìœ íš¨í•œ í•­ëª©: {valid_entries}/{len(golden_entries)}")
        print(f"  â€¢ ê²€ì¦ ì˜¤ë¥˜: {len(validation_errors)}ê°œ")

        if validation_errors[:3]:  # ì²˜ìŒ 3ê°œ ì˜¤ë¥˜ë§Œ í‘œì‹œ
            print(f"  â€¢ ì£¼ìš” ì˜¤ë¥˜:")
            for error in validation_errors[:3]:
                print(f"    - {error}")

        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬
        category_dist = {}
        difficulty_dist = {}

        for entry in golden_entries:
            category = entry.category or "ë¯¸ë¶„ë¥˜"
            category_dist[category] = category_dist.get(category, 0) + 1

            difficulty = entry.difficulty_level
            difficulty_dist[difficulty] = difficulty_dist.get(difficulty, 0) + 1

        print(f"\nğŸ“Š ë°ì´í„°ì…‹ ë¶„í¬:")
        print(f"  â€¢ ì¹´í…Œê³ ë¦¬ë³„:")
        for category, count in category_dist.items():
            print(f"    - {category}: {count}ê°œ")

        print(f"  â€¢ ë‚œì´ë„ë³„:")
        for difficulty, count in difficulty_dist.items():
            print(f"    - {difficulty}: {count}ê°œ")

        # ìƒ˜í”Œ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        print(f"\nğŸ¯ ìƒ˜í”Œ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ (5ê°œ í•­ëª©)...")

        benchmark_results = []
        for entry in golden_entries[:5]:
            result = await self.evaluator.evaluate_rag_response(
                query=entry.query,
                response=entry.ground_truth_answer,
                retrieved_contexts=entry.expected_contexts
            )
            benchmark_results.append(result)

        # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
        if benchmark_results:
            avg_faithfulness = sum(r.metrics.faithfulness for r in benchmark_results if r.metrics.faithfulness) / len(benchmark_results)
            avg_precision = sum(r.metrics.context_precision for r in benchmark_results if r.metrics.context_precision) / len(benchmark_results)
            avg_recall = sum(r.metrics.context_recall for r in benchmark_results if r.metrics.context_recall) / len(benchmark_results)
            avg_relevancy = sum(r.metrics.answer_relevancy for r in benchmark_results if r.metrics.answer_relevancy) / len(benchmark_results)

            print(f"ğŸ“ˆ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ (í‰ê· ):")
            print(f"  â€¢ Faithfulness: {avg_faithfulness:.3f}")
            print(f"  â€¢ Context Precision: {avg_precision:.3f}")
            print(f"  â€¢ Context Recall: {avg_recall:.3f}")
            print(f"  â€¢ Answer Relevancy: {avg_relevancy:.3f}")

        return {
            "entries_count": len(golden_entries),
            "quality_score": overall_quality,
            "valid_entries": valid_entries,
            "benchmark_results": len(benchmark_results)
        }

    async def demo_performance_analysis(self):
        """ì„±ëŠ¥ ë¶„ì„ ë°ëª¨"""
        print("\nâš¡ 5. ì„±ëŠ¥ ë¶„ì„ ë°ëª¨")
        print("-" * 40)

        # ë‹¤ì–‘í•œ í¬ê¸°ì˜ í‰ê°€ ì‹¤í–‰
        test_sizes = [1, 5, 10, 20]
        performance_results = []

        for size in test_sizes:
            print(f"ğŸ“Š {size}ê°œ í‰ê°€ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸...")

            requests = self.sample_generator.generate_evaluation_requests(size)

            start_time = time.time()

            # ìˆœì°¨ ì‹¤í–‰
            for request in requests:
                await self.evaluator.evaluate_rag_response(
                    query=request.query,
                    response=request.response,
                    retrieved_contexts=request.retrieved_contexts
                )

            end_time = time.time()
            total_time = end_time - start_time
            avg_time = total_time / size

            performance_results.append({
                "size": size,
                "total_time": total_time,
                "avg_time": avg_time,
                "evals_per_second": size / total_time
            })

            print(f"  âœ“ ì™„ë£Œ: {total_time:.2f}ì´ˆ (í‰ê·  {avg_time:.2f}ì´ˆ/ê±´)")

        print(f"\nğŸ“ˆ ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼:")
        print(f"{'í¬ê¸°':<6} {'ì´ì‹œê°„':<10} {'í‰ê· ì‹œê°„':<10} {'ì²˜ë¦¬ëŸ‰(ê±´/ì´ˆ)':<12}")
        print("-" * 42)

        for result in performance_results:
            print(f"{result['size']:<6} {result['total_time']:<10.2f} {result['avg_time']:<10.2f} {result['evals_per_second']:<12.1f}")

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •
        import sys
        current_memory = sys.getsizeof(self.evaluator) + sys.getsizeof(self.quality_monitor)
        print(f"\nğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ~{current_memory / 1024:.1f} KB")

        return performance_results

    async def run_complete_demo(self):
        """ì „ì²´ ë°ëª¨ ì‹¤í–‰"""
        print("ğŸ¬ DT-RAG RAGAS í‰ê°€ ì‹œìŠ¤í…œ ì™„ì „ ë°ëª¨ ì‹œì‘!")
        print()

        demo_results = {}

        try:
            # 1. ê¸°ë³¸ í‰ê°€ ë°ëª¨
            demo_results['basic_evaluation'] = await self.demo_basic_evaluation()

            # 2. í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ë°ëª¨
            demo_results['quality_monitoring'] = await self.demo_quality_monitoring()

            # 3. A/B í…ŒìŠ¤íŠ¸ ë°ëª¨
            demo_results['ab_testing'] = await self.demo_ab_testing()

            # 4. ê³¨ë“  ë°ì´í„°ì…‹ ë°ëª¨
            demo_results['golden_dataset'] = await self.demo_golden_dataset()

            # 5. ì„±ëŠ¥ ë¶„ì„ ë°ëª¨
            demo_results['performance'] = await self.demo_performance_analysis()

            # ì „ì²´ ìš”ì•½
            print("\n" + "=" * 60)
            print("ğŸ‰ RAGAS í‰ê°€ ì‹œìŠ¤í…œ ë°ëª¨ ì™„ë£Œ!")
            print("=" * 60)
            print()
            print("âœ… êµ¬í˜„ëœ ì£¼ìš” ê¸°ëŠ¥:")
            print("  â€¢ RAGAS 4ëŒ€ ë©”íŠ¸ë¦­ í‰ê°€ (Faithfulness, Precision, Recall, Relevancy)")
            print("  â€¢ ì‹¤ì‹œê°„ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼")
            print("  â€¢ A/B í…ŒìŠ¤íŠ¸ ë° ì‹¤í—˜ ê´€ë¦¬")
            print("  â€¢ ê³¨ë“  ë°ì´í„°ì…‹ ê´€ë¦¬")
            print("  â€¢ ì„±ëŠ¥ ë¶„ì„ ë° ë²¤ì¹˜ë§ˆí‚¹")
            print("  â€¢ í’ˆì§ˆ ê²Œì´íŠ¸ ë° ìë™í™” ì›Œí¬í”Œë¡œìš°")
            print()
            print("ğŸš€ í”„ë¡œë•ì…˜ ì‚¬ìš© ì¤€ë¹„ ì™„ë£Œ!")
            print("   - API ì—”ë“œí¬ì¸íŠ¸: /evaluation/*")
            print("   - ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ: /evaluation/dashboard")
            print("   - í’ˆì§ˆ ëª¨ë‹ˆí„°ë§: ìë™ í™œì„±í™”")
            print()

            # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            results_file = f"ragas_demo_results_{timestamp}.json"

            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "timestamp": datetime.now().isoformat(),
                    "demo_completed": True,
                    "gemini_api_available": self.has_gemini,
                    "summary": "RAGAS evaluation system demo completed successfully",
                    "results_summary": {
                        "basic_evaluation_completed": bool(demo_results.get('basic_evaluation')),
                        "quality_monitoring_active": bool(demo_results.get('quality_monitoring')),
                        "ab_testing_functional": bool(demo_results.get('ab_testing')),
                        "golden_dataset_ready": bool(demo_results.get('golden_dataset')),
                        "performance_tested": bool(demo_results.get('performance'))
                    }
                }, f, indent=2, ensure_ascii=False)

            print(f"ğŸ“„ ë°ëª¨ ê²°ê³¼ê°€ {results_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

            return demo_results

        except Exception as e:
            print(f"\nâŒ ë°ëª¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return {"error": str(e)}

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸŒŸ DT-RAG v1.8.1 RAGAS í‰ê°€ ì‹œìŠ¤í…œ")
    print("ğŸ“… " + datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    print()

    # í™˜ê²½ í™•ì¸
    if not os.getenv("GEMINI_API_KEY"):
        print("ğŸ’¡ íŒ: GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ë©´ ë” ì •í™•í•œ LLM ê¸°ë°˜ í‰ê°€ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        print("    í˜„ì¬ëŠ” í´ë°± ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        print()

    # ë°ëª¨ ì‹¤í–‰
    demo = RAGASDemo()
    results = await demo.run_complete_demo()

    print("\nğŸ ë°ëª¨ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main())