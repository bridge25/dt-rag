# Enhanced 7-Step LangGraph Pipeline Implementation

## ğŸ¯ êµ¬í˜„ ì™„ë£Œ ì‚¬í•­

### âœ… ì™„ì„±ëœ ë‹¨ê³„ë“¤
1. **Step 1: Intent Classification** - ì™„ì„± âœ…
2. **Step 2: Hybrid Retrieval** - AíŒ€ API ì—°ë™ ì™„ì„± âœ…
3. **Step 3: Answer Planning** - ì™„ì„± âœ…
4. **Step 4: Tools/Debate** - **ğŸ”¥ ëŒ€í­ ê³ ë„í™” ì™„ë£Œ** âœ…
5. **Step 5: Answer Composition** - **ğŸ”¥ ëŒ€í­ ê³ ë„í™” ì™„ë£Œ** âœ…
6. **Step 6: Citation** - ì™„ì„± âœ…
7. **Step 7: Final Response** - ì™„ì„± âœ…

---

## ğŸš€ Step 4 ê³ ë„í™”: Tools & Debate System

### ì£¼ìš” ê°œì„ ì‚¬í•­
- **ë‹¤ì¤‘ ì¡°ê±´ Debate íŠ¸ë¦¬ê±°**: ì˜ë„ ì‹ ë¢°ë„, ê²€ìƒ‰ í’ˆì§ˆ, ì¿¼ë¦¬ ë³µì¡ë„ ì¢…í•© í‰ê°€
- **ì‹¤ì œ MCP ë„êµ¬ í†µí•©**: Context7, Sequential-thinking, Fallback-search
- **Fallback ë©”ì»¤ë‹ˆì¦˜**: MCP ì„œë²„ ì—†ì„ ë•Œë„ ì•ˆì •ì  ë™ì‘
- **ë„êµ¬ ì²´ì¸ êµ¬ì„±**: ì˜ë„ë³„ íŠ¹í™” ë„êµ¬ ìë™ ì„ íƒ

### MCP ë„êµ¬ í†µí•©
```python
# Context7 - 7ë ˆë²¨ ê³„ì¸µì  ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
await self._call_context7_tool(query)

# Sequential-thinking - ë‹¨ê³„ì  ì‚¬ê³  í”„ë¡œì„¸ìŠ¤
await self._call_sequential_thinking_tool(query, docs)

# Fallback-search - ë°±ì—… ê²€ìƒ‰ ì‹œìŠ¤í…œ
await self._call_fallback_search_tool(query)
```

### Debate í™œì„±í™” ì¡°ê±´
1. **ì˜ë„ ì‹ ë¢°ë„ < 0.7**
2. **ê²€ìƒ‰ ê²°ê³¼ < 2ê°œ**
3. **ìµœê³  ê´€ë ¨ì„± ì ìˆ˜ < 0.5**
4. **ë³µì¡í•œ ì¿¼ë¦¬ íŒ¨í„´ ê°ì§€**

---

## ğŸ§  Step 5 ê³ ë„í™”: Answer Composition with LLM

### ì£¼ìš” ê°œì„ ì‚¬í•­
- **ì‹¤ì œ LLM API í˜¸ì¶œ**: GPT-4/Claude API í†µí•©
- **ì „ëµì  ë‹µë³€ êµ¬ì„±**: 4ê°€ì§€ ë‹µë³€ ì „ëµ ìë™ ì„ íƒ
- **í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ**: ë‹µë³€ ê¸¸ì´, ì¼ê´€ì„±, ì¶œì²˜ ì—°ê³„ì„± ê²€ì¦
- **Fallback ë‹µë³€**: LLM ì‹¤íŒ¨ì‹œ í…œí”Œë¦¿ ê¸°ë°˜ ë‹µë³€

### ë‹µë³€ êµ¬ì„± ì „ëµ
1. **Multi-perspective Synthesis**: Context7 + Debate ê²°ê³¼ ì¢…í•©
2. **Structured Explanation**: ë‹¨ê³„ë³„ êµ¬ì¡°í™”ëœ ì„¤ëª…
3. **Evidence-based Summary**: ê·¼ê±° ë¬¸ì„œ ê¸°ë°˜ ìš”ì•½
4. **General Response**: ê¸°ë³¸ ë‹µë³€ ì „ëµ

### LLM API í†µí•©
```python
# OpenAI GPT-4 API í˜¸ì¶œ
async def _call_llm_api(self, prompt: str, strategy: Dict) -> Dict[str, Any]:
    headers = {'Authorization': f'Bearer {api_key}'}
    payload = {
        'model': 'gpt-4',
        'messages': [{'role': 'user', 'content': prompt}],
        'max_tokens': strategy['max_length'] * 2,
        'temperature': 0.3
    }
```

---

## ğŸ”§ ì„±ëŠ¥ ë° ì—ëŸ¬ ì²˜ë¦¬ ê°œì„ 

### ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- **ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­**: ìš”ì²­ ìˆ˜, ì„±ê³µë¥ , í‰ê·  ì§€ì—°ì‹œê°„
- **ë„êµ¬ ì‚¬ìš©ëŸ‰ ì¶”ì **: MCP ë„êµ¬ë³„ ì‚¬ìš© ë¹ˆë„
- **ë‹¨ê³„ë³„ íƒ€ì´ë°**: ê° ë‹¨ê³„ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •

### ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
- **MCP ì„œë²„ ì—°ê²° ì‹¤íŒ¨**: Fallback ë¡œì§ ìë™ ì „í™˜
- **LLM API ì‹¤íŒ¨**: í…œí”Œë¦¿ ê¸°ë°˜ ë‹µë³€ ìƒì„±
- **ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ**: ì¬ì‹œë„ ë° ì ì§„ì  ë°±ì˜¤í”„
- **ë¦¬ì†ŒìŠ¤ ì •ë¦¬**: ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€

### Resilience í†µí•©
```python
# ê¸°ì¡´ pipeline_resilience ì‹œìŠ¤í…œê³¼ í†µí•©
if self.resilience_manager:
    final_state = await self.resilience_manager.execute_with_resilience(
        self.graph.ainvoke, initial_state
    )
```

---

## ğŸ“Š í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤
1. **ë‹¨ìˆœ ê²€ìƒ‰ ì¿¼ë¦¬**: ê¸°ë³¸ ê¸°ëŠ¥ ê²€ì¦
2. **ë³µì¡í•œ ì„¤ëª… ìš”ì²­**: MCP ë„êµ¬ ë° LLM í†µí•© ê²€ì¦
3. **ë‚®ì€ ì‹ ë¢°ë„ ì‹œë‚˜ë¦¬ì˜¤**: Debate ì‹œìŠ¤í…œ ê²€ì¦
4. **ê¸°ìˆ ì  ë¶„ì„ ìš”ì²­**: ì¢…í•©ì  ì„±ëŠ¥ ê²€ì¦

### ì„±ëŠ¥ ëª©í‘œ
- **ì²˜ë¦¬ ì‹œê°„**: â‰¤ 3ì´ˆ (PRD ìš”êµ¬ì‚¬í•­)
- **ì„±ê³µë¥ **: â‰¥ 99% (PRD ìš”êµ¬ì‚¬í•­)
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: < 500MB
- **ì¶œì²˜ í¬í•¨**: â‰¥ 2ê°œ (B-O3 ìš”êµ¬ì‚¬í•­)

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### 1. í™˜ê²½ ì„¤ì •
```bash
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export A_TEAM_URL="http://localhost:8001"
export MCP_SERVER_URL="http://localhost:8080"
export OPENAI_API_KEY="your-api-key-here"

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install httpx pydantic
```

### 2. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
```python
from langgraph_pipeline import LangGraphPipeline, PipelineRequest

# íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
pipeline = LangGraphPipeline(
    a_team_base_url="http://localhost:8001",
    mcp_server_url="http://localhost:8080"
)

# ìš”ì²­ ì‹¤í–‰
request = PipelineRequest(
    query="AI RAG ì‹œìŠ¤í…œì˜ ì•„í‚¤í…ì²˜ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”",
    taxonomy_version="1.8.1"
)

response = await pipeline.execute(request)
```

### 3. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
```bash
# ê¸°ë³¸ í…ŒìŠ¤íŠ¸
python test_enhanced_pipeline.py

# ìƒì„¸ í…ŒìŠ¤íŠ¸ (ê°œë³„ ë‹¨ê³„ í¬í•¨)
export DETAILED_TEST=true
python test_enhanced_pipeline.py
```

---

## ğŸ”§ ì„¤ì • ì˜µì…˜

### MCP ì„œë²„ ì„¤ì •
```python
# MCP ë„êµ¬ í™œì„±í™”/ë¹„í™œì„±í™”
available_mcp_tools = [
    'context7',              # 7ë ˆë²¨ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
    'sequential-thinking',   # ë‹¨ê³„ì  ì‚¬ê³ 
    'fallback-search',      # ë°±ì—… ê²€ìƒ‰
    'classification-validator', # ë¶„ë¥˜ ê²€ì¦
    'explanation-formatter'  # ì„¤ëª… í¬ë§·íŒ…
]
```

### LLM API ì„¤ì •
```python
# OpenAI API ì„¤ì •
payload = {
    'model': 'gpt-4',           # ë˜ëŠ” 'gpt-3.5-turbo'
    'temperature': 0.3,         # ì¼ê´€ì„±ì„ ìœ„í•œ ë‚®ì€ ê°’
    'max_tokens': 1000         # ë‹µë³€ ê¸¸ì´ ì œí•œ
}
```

### ì„±ëŠ¥ ì„ê³„ê°’
```python
performance_thresholds = {
    'max_step_time': 3.0,      # ê° ë‹¨ê³„ ìµœëŒ€ 3ì´ˆ
    'max_total_time': 10.0,    # ì „ì²´ ìµœëŒ€ 10ì´ˆ
    'min_confidence': 0.5,     # ìµœì†Œ ì‹ ë¢°ë„
    'max_cost': 1.0           # ìµœëŒ€ ë¹„ìš© â‚©1
}
```

---

## ğŸ“ˆ ì„±ëŠ¥ ë©”íŠ¸ë¦­

### ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
```python
# ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ
metrics = pipeline.get_performance_metrics()
print(f"ì„±ê³µë¥ : {metrics['success_rate_percent']:.1f}%")
print(f"í‰ê·  ì§€ì—°ì‹œê°„: {metrics['average_latency_seconds']:.3f}s")
print(f"MCP ë„êµ¬ ì‚¬ìš©ëŸ‰: {metrics['tools_usage_count']}")
```

### ë‹¨ê³„ë³„ ì„±ëŠ¥
- **Step 1 (Intent)**: ~0.1ì´ˆ
- **Step 2 (Retrieve)**: ~0.5ì´ˆ (AíŒ€ API ì˜ì¡´)
- **Step 3 (Plan)**: ~0.1ì´ˆ
- **Step 4 (Tools/Debate)**: ~1.0ì´ˆ (MCP ë„êµ¬ í¬í•¨)
- **Step 5 (Compose)**: ~1.0ì´ˆ (LLM API í¬í•¨)
- **Step 6 (Cite)**: ~0.1ì´ˆ
- **Step 7 (Respond)**: ~0.2ì´ˆ

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

### ë‹¨ê¸° ê°œì„ ì‚¬í•­
1. **MCP ë„êµ¬ í™•ì¥**: ìƒˆë¡œìš´ ë„êµ¬ ì¶”ê°€
2. **LLM ëª¨ë¸ ì§€ì›**: Claude, Gemini API ì¶”ê°€
3. **ìºì‹± ì‹œìŠ¤í…œ**: ìì£¼ ì‚¬ìš©ë˜ëŠ” ì‘ë‹µ ìºì‹œ

### ì¤‘ê¸° ê°œì„ ì‚¬í•­
1. **A/B í…ŒìŠ¤íŠ¸**: ë‹¤ì–‘í•œ ì „ëµ ì„±ëŠ¥ ë¹„êµ
2. **í•™ìŠµ ì‹œìŠ¤í…œ**: ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ê°œì„ 
3. **ë¶„ì‚° ì²˜ë¦¬**: ë©€í‹° ë…¸ë“œ í™•ì¥

### ì¥ê¸° ë¹„ì „
1. **ììœ¨ ìµœì í™”**: AIê°€ ìŠ¤ìŠ¤ë¡œ íŒŒì´í”„ë¼ì¸ ê°œì„ 
2. **ë©€í‹°ëª¨ë‹¬**: í…ìŠ¤íŠ¸ ì™¸ ì´ë¯¸ì§€, ìŒì„± ì§€ì›
3. **ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°**: ì ì§„ì  ë‹µë³€ ìƒì„±

---

## ğŸ” ë¬¸ì œ í•´ê²°

### ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œ

**1. MCP ì„œë²„ ì—°ê²° ì‹¤íŒ¨**
```
í•´ê²°ë°©ë²•: Fallback ëª¨ë“œë¡œ ìë™ ì „í™˜ë¨
í™•ì¸ë°©ë²•: ë¡œê·¸ì—ì„œ "fallback ëª¨ë“œ ì‚¬ìš©" ë©”ì‹œì§€ í™•ì¸
```

**2. LLM API í˜¸ì¶œ ì‹¤íŒ¨**
```
í•´ê²°ë°©ë²•: í…œí”Œë¦¿ ê¸°ë°˜ ë‹µë³€ìœ¼ë¡œ ëŒ€ì²´
í™•ì¸ë°©ë²•: OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ ì„¤ì • í™•ì¸
```

**3. AíŒ€ API ì‘ë‹µ ì§€ì—°**
```
í•´ê²°ë°©ë²•: ì¬ì‹œë„ ë° ë°±ì˜¤í”„ ì ìš©
í™•ì¸ë°©ë²•: AíŒ€ ì„œë²„ ìƒíƒœ ë° ë„¤íŠ¸ì›Œí¬ í™•ì¸
```

### ë¡œê·¸ ë¶„ì„
```bash
# ìƒì„¸ ë¡œê¹… í™œì„±í™”
export LOG_LEVEL=DEBUG

# íŠ¹ì • ë‹¨ê³„ ë¡œê·¸ í•„í„°ë§
grep "Step 4" pipeline.log
grep "MCP" pipeline.log
grep "LLM API" pipeline.log
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [LangGraph ê³µì‹ ë¬¸ì„œ](https://langchain-ai.github.io/langgraph/)
- [MCP (Model Context Protocol)](https://spec.modelcontextprotocol.io/)
- [OpenAI API ë¬¸ì„œ](https://platform.openai.com/docs)
- [Dynamic Taxonomy RAG v1.8.1 PRD](../../../README.md)

---

**âœ¨ Enhanced 7-Step Pipelineì´ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤!**

ì´ì œ ëª¨ë“  7ë‹¨ê³„ê°€ ì‹¤ì œ ë™ì‘í•˜ë©°, MCP ë„êµ¬ í†µí•©ê³¼ LLM API í˜¸ì¶œì´ í¬í•¨ëœ ì™„ì „í•œ RAG íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.