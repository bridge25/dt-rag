"""
B-O3: 7-Step LangGraph Pipeline Implementation
intent â†’ retrieve â†’ plan â†’ tools/debate â†’ compose â†’ cite â†’ respond

Week-1 ëª©í‘œ: ì™„ì „í•œ 7-Step íŒŒì´í”„ë¼ì¸ ê³¨ê²© + ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
"""

import asyncio
import time
import logging
import httpx
import os
import random
import json
from typing import TypedDict, List, Dict, Any, Optional, Union
from datetime import datetime

# LangGraph ëŒ€ì‹  ê°„ë‹¨í•œ ê·¸ë˜í”„ êµ¬í˜„ ì‚¬ìš©
from typing import Callable
from pydantic import BaseModel, Field

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class PipelineState(TypedDict):
    """LangGraph íŒŒì´í”„ë¼ì¸ ìƒíƒœ ê´€ë¦¬"""
    # ì…ë ¥
    query: str
    chunk_id: Optional[str]
    taxonomy_version: str
    
    # Step 1: Intent
    intent: str
    intent_confidence: float
    
    # Step 2: Retrieve
    retrieved_docs: List[Dict[str, Any]]
    retrieval_filter_applied: bool
    bm25_results: List[Dict]
    vector_results: List[Dict]
    
    # Step 3: Plan
    answer_strategy: str
    plan_reasoning: List[str]
    
    # Step 4: Tools/Debate
    tools_used: List[str]
    debate_activated: bool
    debate_reasoning: Optional[str]
    
    # Step 5: Compose
    draft_answer: str
    
    # Step 6: Cite
    sources: List[Dict[str, Any]]
    citations_count: int
    
    # Step 7: Respond
    final_answer: str
    confidence: float
    
    # ë©”íƒ€ë°ì´í„° (B-O3 í•„ìˆ˜ ìš”êµ¬ì‚¬í•­)
    cost: float  # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ë¹„ìš©
    latency: float  # íŒŒì´í”„ë¼ì¸ ì „ì²´ ì‹œê°„
    step_timings: Dict[str, float]  # ê° ë‹¨ê³„ë³„ ì‹¤í–‰ ì‹œê°„
    
    # ì—ëŸ¬ ì²˜ë¦¬
    errors: List[str]
    retry_count: int


class PipelineRequest(BaseModel):
    """íŒŒì´í”„ë¼ì¸ ìš”ì²­ ìŠ¤í‚¤ë§ˆ"""
    query: str = Field(..., min_length=1, description="ì‚¬ìš©ì ì§ˆì˜")
    taxonomy_version: str = Field(default="1.8.1", description="íƒì†Œë…¸ë¯¸ ë²„ì „")
    chunk_id: Optional[str] = Field(default=None, description="ì²­í¬ ID (ë¶„ë¥˜ìš©)")
    filters: Optional[Dict[str, Any]] = Field(default=None, description="ê²€ìƒ‰ í•„í„°")
    options: Optional[Dict[str, Any]] = Field(default={}, description="ì¶”ê°€ ì˜µì…˜")


class PipelineResponse(BaseModel):
    """íŒŒì´í”„ë¼ì¸ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ (B-O3 ë©”íƒ€ë°ì´í„° í¬í•¨)"""
    # í•µì‹¬ ì‘ë‹µ
    answer: str = Field(..., description="ìµœì¢… ë‹µë³€")
    confidence: float = Field(..., ge=0.0, le=1.0, description="ì‹ ë¢°ë„ ì ìˆ˜")
    
    # B-O3 í•„ìˆ˜ ë©”íƒ€ë°ì´í„°
    sources: List[Dict[str, Any]] = Field(..., min_items=0, description="ì¶œì²˜ ëª©ë¡ (â‰¥2ê°œ ê¶Œì¥)")
    taxonomy_version: str = Field(..., description="ì‚¬ìš©ëœ íƒì†Œë…¸ë¯¸ ë²„ì „")
    cost: float = Field(..., ge=0.0, description="í† í° ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ë¹„ìš© (â‚©)")
    latency: float = Field(..., ge=0.0, description="íŒŒì´í”„ë¼ì¸ ì „ì²´ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)")
    
    # ë””ë²„ê¹… ì •ë³´
    intent: str = Field(..., description="íŒŒì•…ëœ ì‚¬ìš©ì ì˜ë„")
    step_timings: Dict[str, float] = Field(..., description="ê° ë‹¨ê³„ë³„ ì‹¤í–‰ ì‹œê°„")
    debate_activated: bool = Field(default=False, description="debate ìŠ¤ìœ„ì¹˜ í™œì„±í™” ì—¬ë¶€")
    
    # í’ˆì§ˆ ì •ë³´
    retrieved_count: int = Field(default=0, description="ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜")
    citations_count: int = Field(default=0, description="ì¸ìš©ëœ ì¶œì²˜ ìˆ˜")


class SimpleGraph:
    """ê°„ë‹¨í•œ ìˆœì°¨ ì‹¤í–‰ ê·¸ë˜í”„"""
    
    def __init__(self):
        self.steps = []
        
    def add_step(self, name: str, func: Callable):
        self.steps.append((name, func))
        
    async def ainvoke(self, state: PipelineState) -> PipelineState:
        """ìˆœì°¨ì ìœ¼ë¡œ ëª¨ë“  ë‹¨ê³„ ì‹¤í–‰"""
        for step_name, step_func in self.steps:
            logger.info(f"ì‹¤í–‰ ì¤‘: {step_name}")
            state = await step_func(state)
        return state


class LangGraphPipeline:
    """B-O3 7-Step íŒŒì´í”„ë¼ì¸ (AíŒ€ API ì—°ë™, PRD ì¤€ìˆ˜, MCP í†µí•©)"""

    def __init__(self, a_team_base_url: str = "http://localhost:8001", mcp_server_url: str = "http://localhost:8080"):
        self.a_team_base_url = a_team_base_url
        self.mcp_server_url = mcp_server_url
        self.client = httpx.AsyncClient(timeout=30.0)
        self.mcp_client = httpx.AsyncClient(timeout=15.0)
        self.graph = self._build_graph()

        # MCP ë„êµ¬ ì„¤ì •
        self.available_mcp_tools = [
            'context7', 'sequential-thinking', 'fallback-search',
            'classification-validator', 'explanation-formatter'
        ]

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.performance_metrics = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'average_latency': 0.0,
            'tools_usage_count': {tool: 0 for tool in self.available_mcp_tools}
        }

        # ë³µì›ë ¥ ì‹œìŠ¤í…œ í†µí•©
        try:
            from pipeline_resilience import get_resilience_manager
            self.resilience_manager = get_resilience_manager()
        except ImportError:
            logger.warning("pipeline_resilience ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì‹¤í–‰ ëª¨ë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            self.resilience_manager = None
        
    def _build_graph(self) -> SimpleGraph:
        """7-Step íŒŒì´í”„ë¼ì¸ ê·¸ë˜í”„ êµ¬ì„±"""
        workflow = SimpleGraph()
        
        # 7ê°œ ë‹¨ê³„ ìˆœì°¨ ì¶”ê°€
        workflow.add_step("step1_intent", self._step1_intent_classification)
        workflow.add_step("step2_retrieve", self._step2_hybrid_retrieval)
        workflow.add_step("step3_plan", self._step3_answer_planning)
        workflow.add_step("step4_tools_debate", self._step4_tools_and_debate)
        workflow.add_step("step5_compose", self._step5_answer_composition)
        workflow.add_step("step6_cite", self._step6_citation_extraction)
        workflow.add_step("step7_respond", self._step7_final_response)
        
        return workflow
    
    async def _step1_intent_classification(self, state: PipelineState) -> PipelineState:
        """Step 1: Intent Classification (ì‚¬ìš©ì ì˜ë„ íŒŒì•…)"""
        step_start = time.time()
        logger.info(f"Step 1: Intent Classification - Query: {state['query']}")
        
        # TODO: ì‹¤ì œ ì˜ë„ ë¶„ë¥˜ ë¡œì§ êµ¬í˜„ (LLM í˜¸ì¶œ)
        # í˜„ì¬ëŠ” ìŠ¤ìºí´ë”© êµ¬í˜„
        query = state['query'].lower()
        
        if any(keyword in query for keyword in ['ê²€ìƒ‰', 'search', 'ì°¾ì•„', 'ì¡°íšŒ']):
            intent = "search"
        elif any(keyword in query for keyword in ['ë¶„ë¥˜', 'classify', 'ì¹´í…Œê³ ë¦¬']):
            intent = "classify"
        elif any(keyword in query for keyword in ['ì„¤ëª…', 'explain', 'ì•Œë ¤ì¤˜']):
            intent = "explain"
        else:
            intent = "general_query"
            
        intent_confidence = 0.8  # TODO: ì‹¤ì œ ì‹ ë¢°ë„ ê³„ì‚°
        
        step_time = time.time() - step_start
        
        state.update({
            "intent": intent,
            "intent_confidence": intent_confidence,
            "step_timings": {**state.get("step_timings", {}), "step1_intent": step_time}
        })
        
        logger.info(f"Step 1 ì™„ë£Œ: intent={intent}, confidence={intent_confidence:.3f}, time={step_time:.3f}s")
        return state
    
    async def _step2_hybrid_retrieval(self, state: PipelineState) -> PipelineState:
        """Step 2: Hybrid Retrieval (AíŒ€ /search API í˜¸ì¶œ, PRD ì¤€ìˆ˜)"""
        step_start = time.time()
        logger.info(f"Step 2: Hybrid Retrieval - Intent: {state['intent']}")
        
        try:
            # AíŒ€ /search API í˜¸ì¶œ (PRD ì¤€ìˆ˜)
            search_request = {
                "q": state['query'],
                "filters": None,  # TODO: intentì— ë”°ë¥¸ í•„í„° ì ìš©
                "bm25_topk": 12,
                "vector_topk": 12,
                "rerank_candidates": 50,
                "final_topk": 5
            }
            # Retry-After aware call to A-team /search
            max_retries = int(os.getenv("MAX_RETRIES", os.getenv("ORCH_RETRY_MAX", "3")))
            backoff = float(os.getenv("ORCH_RETRY_BACKOFF", "1.0"))
            jitter = float(os.getenv("ORCH_RETRY_JITTER_MAX", "0.2"))
            max_cap = float(os.getenv("ORCH_RETRY_MAX_DELAY", "10.0"))
            for attempt in range(max_retries + 1):
                try:
                    response = await self.client.post(
                        f"{self.a_team_base_url}/search",
                        json=search_request
                    )
                    response.raise_for_status()
                    break
                except httpx.HTTPStatusError as e:
                    if e.response.status_code in [429, 500, 502, 503, 504] and attempt < max_retries:
                        ra = e.response.headers.get("Retry-After")
                        parsed = None
                        if ra:
                            try:
                                parsed = float(ra)
                            except Exception:
                                try:
                                    import email.utils, time as _t
                                    dt = email.utils.parsedate_to_datetime(ra)
                                    parsed = max(0.0, dt.timestamp() - _t.time())
                                except Exception:
                                    parsed = None
                        base = backoff * (2 ** attempt)
                        delay = (parsed if parsed is not None else base) + random.uniform(0, jitter)
                        delay = min(delay, max_cap)
                        logger.warning(f"/search retry {attempt+1}/{max_retries+1} after {e.response.status_code}; sleep {delay:.2f}s")
                        await asyncio.sleep(delay)
                        continue
                    raise
                except Exception as e:
                    if attempt < max_retries:
                        delay = min(backoff * (2 ** attempt) + random.uniform(0, jitter), max_cap)
                        logger.warning(f"/search exception retry {attempt+1}/{max_retries+1}; sleep {delay:.2f}s: {e}")
                        await asyncio.sleep(delay)
                        continue
                    raise

            search_result = response.json()
            retrieved_docs = search_result.get("hits", [])

            # AíŒ€ ì‘ë‹µì„ BíŒ€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            formatted_docs = []
            for i, doc in enumerate(retrieved_docs):
                formatted_doc = {
                    "chunk_id": doc.get("chunk_id", f"doc_{i}"),
                    "score": doc.get("score", 0.0),
                    "text": doc.get("text", ""),
                    "source": {
                        "url": doc.get("source", {}).get("url", ""),
                        "title": doc.get("source", {}).get("title", f"ë¬¸ì„œ {i+1}")
                    }
                }
                formatted_docs.append(formatted_doc)

            logger.info(f"AíŒ€ /search API í˜¸ì¶œ ì„±ê³µ: {len(formatted_docs)}ê°œ ë¬¸ì„œ")

            step_time = time.time() - step_start

            state.update({
                "bm25_results": [],  # AíŒ€ì—ì„œ ì´ë¯¸ í†µí•© ì²˜ë¦¬ë¨
                "vector_results": [],  # AíŒ€ì—ì„œ ì´ë¯¸ í†µí•© ì²˜ë¦¬ë¨
                "retrieved_docs": formatted_docs,
                "retrieval_filter_applied": True,
                "step_timings": {**state.get("step_timings", {}), "step2_retrieve": step_time}
            })

            logger.info(f"Step 2 ì™„ë£Œ: retrieved={len(formatted_docs)} docs, time={step_time:.3f}s")
            return state

        except Exception as e:
            logger.error(f"AíŒ€ /search API í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")
        
        # AíŒ€ API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ë¹ˆ ê²°ê³¼ë¡œ ì²˜ë¦¬ (PRD ì¤€ìˆ˜)
        step_time = time.time() - step_start
        
        state.update({
            "bm25_results": [],
            "vector_results": [],
            "retrieved_docs": [],
            "retrieval_filter_applied": False,
            "step_timings": {**state.get("step_timings", {}), "step2_retrieve": step_time}
        })
        
        logger.warning("Step 2 ì™„ë£Œ: AíŒ€ API í˜¸ì¶œ ì‹¤íŒ¨, ë¹ˆ ê²°ê³¼ ë°˜í™˜")
        return state
    
    async def _step3_answer_planning(self, state: PipelineState) -> PipelineState:
        """Step 3: Answer Planning (ë‹µë³€ ì „ëµ ê³„íš)"""
        step_start = time.time()
        logger.info(f"Step 3: Answer Planning - Intent: {state['intent']}")
        
        # ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”: ì „ëµ ê²°ì •ê³¼ ì¶”ë¡ ì„ ë™ì‹œ ìˆ˜í–‰
        docs_count = len(state['retrieved_docs'])
        intent = state['intent']
        
        async def determine_strategy():
            """ë‹µë³€ ì „ëµ ê²°ì •"""
            if intent == "search" and docs_count > 0:
                return "search_results_summary"
            elif intent == "explain" and docs_count > 0:
                return "detailed_explanation"
            else:
                return "general_answer"
        
        async def generate_reasoning(strategy_type):
            """ì „ëµë³„ ì¶”ë¡  ìƒì„±"""
            if strategy_type == "search_results_summary":
                return [
                    f"ê²€ìƒ‰ ì˜ë„ë¡œ íŒŒì•…ë¨",
                    f"{docs_count}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨",
                    "ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ì „ëµ ì„ íƒ"
                ]
            elif strategy_type == "detailed_explanation":
                return [
                    "ì„¤ëª… ìš”ì²­ìœ¼ë¡œ íŒŒì•…ë¨",
                    "ìƒì„¸ ì„¤ëª… ì „ëµ ì„ íƒ",
                    "ê·¼ê±° ë¬¸ì„œ ê¸°ë°˜ ì„¤ëª…"
                ]
            else:
                return [
                    "ì¼ë°˜ ì§ˆì˜ë¡œ ì²˜ë¦¬",
                    "ê¸°ë³¸ ë‹µë³€ ì „ëµ ì ìš©"
                ]
        
        # ì „ëµ ê²°ì •
        strategy = await determine_strategy()
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì¶”ë¡  ìƒì„± ì‹œê°„ ë‹¨ì¶•
        reasoning = await generate_reasoning(strategy)
            
        step_time = time.time() - step_start
        
        state.update({
            "answer_strategy": strategy,
            "plan_reasoning": reasoning,
            "step_timings": {**state.get("step_timings", {}), "step3_plan": step_time}
        })
        
        logger.info(f"Step 3 ì™„ë£Œ: strategy={strategy}, time={step_time:.3f}s")
        return state
    
    async def _step4_tools_and_debate(self, state: PipelineState) -> PipelineState:
        """Step 4: Tools/Debate (MCP ë„êµ¬ í†µí•© ë° debate ì‹œìŠ¤í…œ)"""
        step_start = time.time()
        logger.info("Step 4: Tools and Debate - Enhanced with MCP integration")

        tools_used = []
        debate_activated = False
        debate_reasoning = None

        # 1. Debate ìŠ¤ìœ„ì¹˜ ë¡œì§ - ë‹¤ì¤‘ ì¡°ê±´ í‰ê°€
        debate_triggers = []

        # ì˜ë„ ì‹ ë¢°ë„ ì²´í¬
        intent_confidence = state.get('intent_confidence', 1.0)
        if intent_confidence < 0.7:
            debate_triggers.append(f"ì˜ë„ ì‹ ë¢°ë„ ë‚®ìŒ ({intent_confidence:.3f})")

        # ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ ì²´í¬
        retrieved_docs = state['retrieved_docs']
        if len(retrieved_docs) < 2:
            debate_triggers.append(f"ê²€ìƒ‰ ê²°ê³¼ ë¶€ì¡± ({len(retrieved_docs)}ê°œ)")

        # ìµœê³  ìŠ¤ì½”ì–´ ì²´í¬ (ê´€ë ¨ì„± ë‚®ìŒ)
        if retrieved_docs and max(doc.get('score', 0) for doc in retrieved_docs) < 0.5:
            debate_triggers.append("ê²€ìƒ‰ ê²°ê³¼ ê´€ë ¨ì„± ë‚®ìŒ")

        # ë³µì¡í•œ ì¿¼ë¦¬ íŒ¨í„´ ì²´í¬
        query_complexity = await self._assess_query_complexity(state['query'])
        if query_complexity['is_complex']:
            debate_triggers.append(f"ë³µì¡í•œ ì¿¼ë¦¬ íŒ¨í„´: {query_complexity['reason']}")

        # Debate í™œì„±í™” ê²°ì •
        if debate_triggers:
            debate_activated = True
            debate_reasoning = "; ".join(debate_triggers)

            # 2. MCP Tools í†µí•© ì‹¤í–‰
            mcp_results = await self._execute_mcp_tools(state, debate_triggers)
            tools_used.extend(mcp_results['tools_used'])

            # Debate ê²°ê³¼ë¥¼ ìƒíƒœì— í†µí•©
            if mcp_results['enhanced_context']:
                state['retrieved_docs'].extend(mcp_results['enhanced_context'])
                logger.info(f"MCP ë„êµ¬ë¡œ {len(mcp_results['enhanced_context'])}ê°œ ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ í™•ë³´")

        # 3. ì¶”ê°€ ë„êµ¬ ì‚¬ìš© ê²°ì •
        additional_tools = await self._select_additional_tools(state)
        tools_used.extend(additional_tools)

        step_time = time.time() - step_start

        state.update({
            "tools_used": tools_used,
            "debate_activated": debate_activated,
            "debate_reasoning": debate_reasoning,
            "step_timings": {**state.get("step_timings", {}), "step4_tools_debate": step_time}
        })

        logger.info(f"Step 4 ì™„ë£Œ: tools={len(tools_used)}, debate={debate_activated}, time={step_time:.3f}s")
        logger.info(f"ì‚¬ìš©ëœ ë„êµ¬: {tools_used}")
        return state

    async def _assess_query_complexity(self, query: str) -> Dict[str, Any]:
        """ì¿¼ë¦¬ ë³µì¡ë„ í‰ê°€"""
        complexity_indicators = {
            'multi_topic': ['ê·¸ë¦¬ê³ ', 'and', 'ë˜í•œ', 'ê²Œë‹¤ê°€', 'ë¿ë§Œ ì•„ë‹ˆë¼'],
            'comparison': ['ë¹„êµ', 'vs', 'ì°¨ì´', 'ë‹¤ë¥¸ì ', 'ê°™ì€ì ', 'compare'],
            'temporal': ['ì—­ì‚¬', 'ë³€í™”', 'ë°œì „', 'ê³¼ê±°', 'ë¯¸ë˜', 'íŠ¸ë Œë“œ'],
            'analytical': ['ë¶„ì„', 'í‰ê°€', 'ê²€í† ', 'ì–´ë–»ê²Œ', 'ì™œ', 'analyze'],
            'technical': ['êµ¬í˜„', 'ë°©ë²•', 'ê¸°ìˆ ì ', 'ì•„í‚¤í…ì²˜', 'technical']
        }

        found_patterns = []
        query_lower = query.lower()

        for pattern_type, keywords in complexity_indicators.items():
            if any(keyword in query_lower for keyword in keywords):
                found_patterns.append(pattern_type)

        is_complex = len(found_patterns) >= 2 or len(query.split()) > 10

        return {
            'is_complex': is_complex,
            'patterns': found_patterns,
            'reason': f"íŒ¨í„´ {len(found_patterns)}ê°œ ê°ì§€: {', '.join(found_patterns)}" if found_patterns else "ë‹¨ìˆœ ì¿¼ë¦¬"
        }

    async def _execute_mcp_tools(self, state: PipelineState, triggers: List[str]) -> Dict[str, Any]:
        """MCP ë„êµ¬ ì‹¤í–‰ (context7, sequential-thinking ë“±)"""
        tools_used = []
        enhanced_context = []

        try:
            # Context7 ë„êµ¬ ì‚¬ìš© - 7ë‹¨ê³„ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
            if "ì˜ë„ ì‹ ë¢°ë„ ë‚®ìŒ" in str(triggers) or "ë³µì¡í•œ ì¿¼ë¦¬" in str(triggers):
                context7_result = await self._call_context7_tool(state['query'])
                if context7_result['success']:
                    tools_used.append("context7")
                    enhanced_context.extend(context7_result['contexts'])
                    logger.info(f"Context7 ë„êµ¬ ì„±ê³µ: {len(context7_result['contexts'])}ê°œ ì»¨í…ìŠ¤íŠ¸")

            # Sequential-thinking ë„êµ¬ ì‚¬ìš© - ë‹¨ê³„ì  ì‚¬ê³ 
            if "ë³µì¡í•œ ì¿¼ë¦¬ íŒ¨í„´" in str(triggers):
                seq_thinking_result = await self._call_sequential_thinking_tool(state['query'], state['retrieved_docs'])
                if seq_thinking_result['success']:
                    tools_used.append("sequential-thinking")
                    enhanced_context.extend(seq_thinking_result['thought_steps'])
                    logger.info(f"Sequential-thinking ë„êµ¬ ì„±ê³µ: {len(seq_thinking_result['thought_steps'])}ê°œ ì‚¬ê³  ë‹¨ê³„")

            # Fallback search ë„êµ¬ ì‚¬ìš©
            if "ê²€ìƒ‰ ê²°ê³¼ ë¶€ì¡±" in str(triggers) or "ê´€ë ¨ì„± ë‚®ìŒ" in str(triggers):
                fallback_result = await self._call_fallback_search_tool(state['query'])
                if fallback_result['success']:
                    tools_used.append("fallback_search")
                    enhanced_context.extend(fallback_result['additional_docs'])
                    logger.info(f"Fallback search ì„±ê³µ: {len(fallback_result['additional_docs'])}ê°œ ì¶”ê°€ ë¬¸ì„œ")

        except Exception as e:
            logger.warning(f"MCP ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            tools_used.append("mcp_error_handler")

        return {
            'tools_used': tools_used,
            'enhanced_context': enhanced_context
        }

    async def _call_context7_tool(self, query: str) -> Dict[str, Any]:
        """Context7 MCP ë„êµ¬ í˜¸ì¶œ (ì‹¤ì œ MCP ì„œë²„ ì—°ë™)"""
        try:
            # 1. ì‹¤ì œ MCP ì„œë²„ í˜¸ì¶œ ì‹œë„
            mcp_available = await self._check_mcp_server_health()

            if mcp_available:
                # ì‹¤ì œ MCP ì„œë²„ë¡œ ìš”ì²­
                mcp_response = await self.mcp_client.post(
                    f"{self.mcp_server_url}/tools/context7",
                    json={
                        "query": query,
                        "levels": 7,
                        "mode": "hierarchical"
                    },
                    timeout=10.0
                )

                if mcp_response.status_code == 200:
                    result = mcp_response.json()
                    self.performance_metrics['tools_usage_count']['context7'] += 1

                    # MCP ì‘ë‹µì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    contexts = []
                    for level, content in result.get('contexts', {}).items():
                        contexts.append({
                            'chunk_id': f'context7_{level}',
                            'text': content.get('text', ''),
                            'score': content.get('confidence', 0.8),
                            'source': {
                                'title': f'Context7 Level {level}',
                                'url': f'mcp://context7/{level}'
                            }
                        })

                    logger.info(f"Context7 MCP ì„±ê³µ: {len(contexts)}ê°œ ì»¨í…ìŠ¤íŠ¸")
                    return {'success': True, 'contexts': contexts}

                logger.warning(f"Context7 MCP ì˜¤ë¥˜: {mcp_response.status_code}")

            # 2. MCP ì„œë²„ ì—†ìŒ/ì˜¤ë¥˜ ì‹œ fallback ë¡œì§
            logger.info("Context7 MCP fallback ëª¨ë“œ ì‚¬ìš©")
            contexts = await self._context7_fallback_analysis(query)
            return {'success': True, 'contexts': contexts, 'fallback': True}

        except Exception as e:
            logger.error(f"Context7 ë„êµ¬ í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            # ë¹„ìƒ í›„ fallback
            contexts = await self._context7_fallback_analysis(query)
            return {'success': True, 'contexts': contexts, 'fallback': True}

    async def _check_mcp_server_health(self) -> bool:
        """MCP ì„œë²„ ê±´ê°• ìƒíƒœ í™•ì¸"""
        try:
            health_response = await self.mcp_client.get(
                f"{self.mcp_server_url}/health",
                timeout=3.0
            )
            return health_response.status_code == 200
        except Exception:
            return False

    async def _context7_fallback_analysis(self, query: str) -> List[Dict[str, Any]]:
        """Context7 fallback ë¶„ì„ (ê·œì¹™ ê¸°ë°˜)"""
        # ë‹¨ì–´ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ë ˆë²¨ ë¶„ì„
        contexts = []
        query_words = query.lower().split()

        # Level 1: ì§ì ‘ì  í‚¤ì›Œë“œ
        level1_keywords = [word for word in query_words if len(word) > 2]
        if level1_keywords:
            contexts.append({
                'chunk_id': 'context7_level1_direct',
                'text': f'ì§ì ‘ í‚¤ì›Œë“œ: {", ".join(level1_keywords[:5])}',
                'score': 0.9,
                'source': {'title': 'Context7 Level 1 - Direct', 'url': 'fallback://context7/level1'}
            })

        # Level 2: ì˜ë¯¸ì  ê·¸ë£¹í™”
        semantic_groups = self._group_semantic_keywords(level1_keywords)
        if semantic_groups:
            contexts.append({
                'chunk_id': 'context7_level2_semantic',
                'text': f'ì˜ë¯¸ ê·¸ë£¹: {", ".join(semantic_groups)}',
                'score': 0.8,
                'source': {'title': 'Context7 Level 2 - Semantic', 'url': 'fallback://context7/level2'}
            })

        # Level 3: ë²”ì£¼ í™•ì¥
        domain_context = self._identify_domain_context(query)
        if domain_context:
            contexts.append({
                'chunk_id': 'context7_level3_domain',
                'text': f'ë„ë©”ì¸ ì»¨í…ìŠ¤íŠ¸: {domain_context}',
                'score': 0.7,
                'source': {'title': 'Context7 Level 3 - Domain', 'url': 'fallback://context7/level3'}
            })

        return contexts

    def _group_semantic_keywords(self, keywords: List[str]) -> List[str]:
        """ì˜ë¯¸ì  í‚¤ì›Œë“œ ê·¸ë£¹í™”"""
        # ê°„ë‹¨í•œ ì˜ë¯¸ì  ê·¸ë£¹í™” ë¡œì§
        tech_keywords = [k for k in keywords if k in ['ai', 'rag', 'ì¸ê³µì§€ëŠ¥', 'ê¸°ìˆ ', 'system']]
        business_keywords = [k for k in keywords if k in ['ë¹„ì¦ˆë‹ˆìŠ¤', 'ì „ëµ', 'ê²½ì˜', 'ë§ˆì¼“íŒ…', 'business']]

        groups = []
        if tech_keywords:
            groups.append('ê¸°ìˆ  ê´€ë ¨')
        if business_keywords:
            groups.append('ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ë ¨')

        return groups

    def _identify_domain_context(self, query: str) -> str:
        """ë„ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ ì‹ë³„"""
        domain_patterns = {
            'technology': ['ai', 'rag', 'ì¸ê³µì§€ëŠ¥', 'ê¸°ìˆ ', 'system', 'ê°œë°œ'],
            'business': ['ë¹„ì¦ˆë‹ˆìŠ¤', 'ê²½ì˜', 'ë§ˆì¼“íŒ…', 'ì „ëµ', 'ìˆ˜ìµ'],
            'education': ['êµìœ¡', 'í•™ìŠµ', 'ê°•ì˜', 'ìˆ˜ì—…', 'ê°•ì˜'],
            'research': ['ì—°êµ¬', 'ë®¤ë¬¸', 'ë¶„ì„', 'ì¡°ì‚¬', 'ë°œê²¬']
        }

        query_lower = query.lower()
        for domain, patterns in domain_patterns.items():
            if any(pattern in query_lower for pattern in patterns):
                return domain

        return 'general'

    async def _call_sequential_thinking_tool(self, query: str, docs: List[Dict]) -> Dict[str, Any]:
        """Sequential-thinking MCP ë„êµ¬ í˜¸ì¶œ"""
        try:
            # 1. MCP ì„œë²„ ì—°ë™ ì‹œë„
            mcp_available = await self._check_mcp_server_health()

            if mcp_available:
                mcp_response = await self.mcp_client.post(
                    f"{self.mcp_server_url}/tools/sequential-thinking",
                    json={
                        "query": query,
                        "context_docs": [doc.get('text', '')[:200] for doc in docs[:3]],
                        "thinking_depth": "deep"
                    },
                    timeout=15.0
                )

                if mcp_response.status_code == 200:
                    result = mcp_response.json()
                    self.performance_metrics['tools_usage_count']['sequential-thinking'] += 1

                    thought_steps = []
                    for step in result.get('thinking_steps', []):
                        thought_steps.append({
                            'chunk_id': f'seq_step_{step.get("id", "unknown")}',
                            'text': step.get('content', ''),
                            'score': step.get('confidence', 0.9),
                            'source': {
                                'title': f'Sequential Step {step.get("id", "unknown")}',
                                'url': f'mcp://sequential/step{step.get("id", "unknown")}'
                            }
                        })

                    logger.info(f"Sequential-thinking MCP ì„±ê³µ: {len(thought_steps)}ê°œ ë‹¨ê³„")
                    return {'success': True, 'thought_steps': thought_steps}

            # 2. Fallback ë¡œì§
            logger.info("Sequential-thinking fallback ëª¨ë“œ ì‚¬ìš©")
            thought_steps = await self._sequential_thinking_fallback(query, docs)
            return {'success': True, 'thought_steps': thought_steps, 'fallback': True}

        except Exception as e:
            logger.error(f"Sequential-thinking ë„êµ¬ í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            thought_steps = await self._sequential_thinking_fallback(query, docs)
            return {'success': True, 'thought_steps': thought_steps, 'fallback': True}

    async def _sequential_thinking_fallback(self, query: str, docs: List[Dict]) -> List[Dict[str, Any]]:
        """Sequential thinking fallback ë¶„ì„"""
        steps = []

        # Step 1: ë¬¸ì œ ë¶„ì„
        problem_analysis = f"ì¿¼ë¦¬ 'ì‚¬ìš©ì ë‹ˆì¦ˆ ë¶„ì„: {query[:50]}...' ì´ë‹¤. "
        if docs:
            problem_analysis += f"{len(docs)}ê°œì˜ ê´€ë ¨ ë¬¸ì„œê°€ ì°¾ì•˜ë‹¤."
        else:
            problem_analysis += "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆë‹¤."

        steps.append({
            'chunk_id': 'seq_step_1_analysis',
            'text': problem_analysis,
            'score': 0.9,
            'source': {'title': 'Sequential Step 1 - Analysis', 'url': 'fallback://sequential/step1'}
        })

        # Step 2: ì •ë³´ ìˆ˜ì§‘
        info_gathering = "ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ: "
        if docs:
            top_doc = docs[0]
            info_gathering += f"ìµœê³  ê´€ë ¨ë„ ë¬¸ì„œ({top_doc.get('score', 0):.3f}): {top_doc.get('text', '')[:100]}..."
        else:
            info_gathering += "ì‚¬ì „ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ë‹µë³€ì„ êµ¬ì„±í•´ì•¼ í•œë‹¤."

        steps.append({
            'chunk_id': 'seq_step_2_gathering',
            'text': info_gathering,
            'score': 0.8,
            'source': {'title': 'Sequential Step 2 - Gathering', 'url': 'fallback://sequential/step2'}
        })

        # Step 3: ì†”ë£¨ì…˜ ì¢…í•©
        solution_synthesis = f"ë‹µë³€ ì „ëµ: "
        if len(docs) >= 2:
            solution_synthesis += "ë‹¤ì¤‘ ì¶œì²˜ ì¢…í•© ë‹µë³€ì„ ìƒì„±í•œë‹¤."
        elif len(docs) == 1:
            solution_synthesis += "ë‹¨ì¼ ì¶œì²˜ ê¸°ë°˜ ìƒì„¸ ë‹µë³€ì„ ìƒì„±í•œë‹¤."
        else:
            solution_synthesis += "ì¼ë°˜ì  ì§€ì‹ ê¸°ë°˜ ë‹µë³€ì„ ìƒì„±í•œë‹¤."

        steps.append({
            'chunk_id': 'seq_step_3_synthesis',
            'text': solution_synthesis,
            'score': 0.85,
            'source': {'title': 'Sequential Step 3 - Synthesis', 'url': 'fallback://sequential/step3'}
        })

        return steps

    async def _call_fallback_search_tool(self, query: str) -> Dict[str, Any]:
        """Fallback search ë„êµ¬ í˜¸ì¶œ (ì™¸ë¶€ API ë° MCP ì—°ë™)"""
        try:
            # 1. MCP Fallback search ì‹œë„
            mcp_available = await self._check_mcp_server_health()

            if mcp_available:
                mcp_response = await self.mcp_client.post(
                    f"{self.mcp_server_url}/tools/fallback-search",
                    json={
                        "query": query,
                        "search_type": "comprehensive",
                        "max_results": 5
                    },
                    timeout=20.0
                )

                if mcp_response.status_code == 200:
                    result = mcp_response.json()
                    self.performance_metrics['tools_usage_count']['fallback-search'] += 1

                    additional_docs = []
                    for doc in result.get('search_results', []):
                        additional_docs.append({
                            'chunk_id': doc.get('id', f'fallback_{len(additional_docs)}'),
                            'text': doc.get('content', ''),
                            'score': doc.get('relevance_score', 0.6),
                            'source': {
                                'title': doc.get('title', 'Fallback Source'),
                                'url': doc.get('url', 'mcp://fallback/search')
                            }
                        })

                    logger.info(f"Fallback search MCP ì„±ê³µ: {len(additional_docs)}ê°œ ë¬¸ì„œ")
                    return {'success': True, 'additional_docs': additional_docs}

            # 2. ëŒ€ì²´ ê²€ìƒ‰ ì „ëµ (ì™¸ë¶€ API ë˜ëŠ” ë‚´ì¥ ì§€ì‹)
            logger.info("Fallback search ëŒ€ì²´ ì „ëµ ì‚¬ìš©")
            additional_docs = await self._execute_alternative_search(query)
            return {'success': True, 'additional_docs': additional_docs, 'fallback': True}

        except Exception as e:
            logger.error(f"Fallback search ë„êµ¬ í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            additional_docs = await self._execute_alternative_search(query)
            return {'success': True, 'additional_docs': additional_docs, 'fallback': True}

    async def _execute_alternative_search(self, query: str) -> List[Dict[str, Any]]:
        """Alternative search strategies when MCP is unavailable"""
        alternative_docs = []

        # ì „ëµ 1: í‚¤ì›Œë“œ ê¸°ë°˜ ë‚´ì¥ ì§€ì‹
        knowledge_base = await self._query_internal_knowledge(query)
        if knowledge_base:
            alternative_docs.extend(knowledge_base)

        # ì „ëµ 2: ì™¸ë¶€ ê³µê°œ API í™œìš© (ì˜ˆ: Wikipedia API)
        try:
            external_results = await self._query_external_apis(query)
            if external_results:
                alternative_docs.extend(external_results)
        except Exception as e:
            logger.warning(f"ì™¸ë¶€ API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")

        # ì „ëµ 3: ìœ ì‚¬ ì¿¼ë¦¬ ëŒ€ì²´
        if not alternative_docs:
            similar_query_docs = await self._generate_similar_query_responses(query)
            alternative_docs.extend(similar_query_docs)

        return alternative_docs[:5]  # ìµœëŒ€ 5ê°œ

    async def _query_internal_knowledge(self, query: str) -> List[Dict[str, Any]]:
        """Internal knowledge base query"""
        # ê°„ë‹¨í•œ ë‚´ì¥ ì§€ì‹ ê¸°ë°˜ ìƒì„±
        knowledge_patterns = {
            'rag': 'RAG (Retrieval-Augmented Generation)ëŠ” ëŒ€ì–¸ì–´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.',
            'ai': 'ì¸ê³µì§€ëŠ¥(AI)ì€ ê¸°ê³„ê°€ ì¸ê°„ì˜ ì§€ëŠ¥ì„ ëª¨ë°©í•˜ë„ë¡ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.',
            'ê¸°ìˆ ': 'ê¸°ìˆ  ë°œì „ì€ ì¸ë¥˜ ì‚¬íšŒì˜ ì§„ë³´ë¥¼ ì´ëŒì–´ ì˜¨ ì£¼ìš” ë™ë ¥ì…ë‹ˆë‹¤.'
        }

        query_lower = query.lower()
        knowledge_docs = []

        for keyword, knowledge in knowledge_patterns.items():
            if keyword in query_lower:
                knowledge_docs.append({
                    'chunk_id': f'internal_knowledge_{keyword}',
                    'text': knowledge,
                    'score': 0.7,
                    'source': {
                        'title': f'Internal Knowledge - {keyword}',
                        'url': f'internal://knowledge/{keyword}'
                    }
                })

        return knowledge_docs

    async def _query_external_apis(self, query: str) -> List[Dict[str, Any]]:
        """Query external APIs (example: simplified Wikipedia-like service)"""
        # ì˜ˆì‹œ: ì™¸ë¶€ ê³µê°œ API í˜¸ì¶œ
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” Wikipedia API, ê³µê°œ ë°ì´í„° API ë“± ì‚¬ìš©
        try:
            # ì˜ˆì‹œ URL (mock)
            # external_response = await self.client.get(f"https://api.example.com/search?q={query}")

            # Mock external response
            external_docs = [
                {
                    'chunk_id': 'external_api_result_1',
                    'text': f'ì™¸ë¶€ APIì—ì„œ ê²€ìƒ‰í•œ "{query}"ì— ëŒ€í•œ ì •ë³´: ê´€ë ¨ ë‚´ìš©ì„ ì œê³µí•©ë‹ˆë‹¤.',
                    'score': 0.65,
                    'source': {
                        'title': 'External API Result',
                        'url': 'https://external-api.example.com/result'
                    }
                }
            ]

            return external_docs

        except Exception as e:
            logger.warning(f"ì™¸ë¶€ API ì˜¤ë¥˜: {str(e)}")
            return []

    async def _generate_similar_query_responses(self, query: str) -> List[Dict[str, Any]]:
        """Generate responses for similar queries"""
        similar_responses = [
            {
                'chunk_id': 'similar_query_response',
                'text': f'"{query}"ì™€ ìœ ì‚¬í•œ ì§ˆë¬¸ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ë‹µë³€ì…ë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì •ë³´ê°€ í•„ìš”í•œ ê²½ìš° ì¶”ê°€ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”.',
                'score': 0.5,
                'source': {
                    'title': 'Similar Query Response',
                    'url': 'internal://similar-query/response'
                }
            }
        ]

        return similar_responses

    async def _select_additional_tools(self, state: PipelineState) -> List[str]:
        """ì¶”ê°€ ë„êµ¬ ì„ íƒ ë¡œì§"""
        additional_tools = []

        # ì˜ë„ë³„ íŠ¹í™” ë„êµ¬
        intent = state.get('intent', '')
        if intent == 'search':
            additional_tools.append('search_enhancer')
        elif intent == 'explain':
            additional_tools.append('explanation_formatter')
        elif intent == 'classify':
            additional_tools.append('classification_validator')

        # ë¬¸ì„œ ìˆ˜ì— ë”°ë¥¸ ë„êµ¬ ì¶”ê°€
        doc_count = len(state['retrieved_docs'])
        if doc_count > 10:
            additional_tools.append('document_summarizer')
        elif doc_count == 0:
            additional_tools.append('knowledge_base_fallback')

        return additional_tools
    
    async def _step5_answer_composition(self, state: PipelineState) -> PipelineState:
        """Step 5: Answer Composition (ì‹¤ì œ LLM API í˜¸ì¶œë¡œ ê³ í’ˆì§ˆ ë‹µë³€ ìƒì„±)"""
        step_start = time.time()
        logger.info("Step 5: Answer Composition - Enhanced with LLM API integration")

        query = state['query']
        docs = state['retrieved_docs']
        strategy = state['answer_strategy']
        intent = state['intent']
        tools_used = state.get('tools_used', [])

        # 1. ë‹µë³€ ìƒì„± ì „ëµ ì„ íƒ
        composition_strategy = await self._select_composition_strategy(state)
        logger.info(f"ì„ íƒëœ ë‹µë³€ êµ¬ì„± ì „ëµ: {composition_strategy['name']}")

        # 2. LLM API í˜¸ì¶œë¡œ ë‹µë³€ ìƒì„±
        draft_answer = await self._generate_answer_with_llm(
            query=query,
            docs=docs,
            strategy=composition_strategy,
            intent=intent,
            tools_context=tools_used
        )

        # 3. ë‹µë³€ í’ˆì§ˆ ê²€ì¦ ë° ê°œì„ 
        validated_answer = await self._validate_and_enhance_answer(draft_answer, state)

        step_time = time.time() - step_start

        state.update({
            "draft_answer": validated_answer,
            "composition_strategy": composition_strategy['name'],
            "step_timings": {**state.get("step_timings", {}), "step5_compose": step_time}
        })

        logger.info(f"Step 5 ì™„ë£Œ: draft length={len(validated_answer)}, strategy={composition_strategy['name']}, time={step_time:.3f}s")
        return state

    async def _select_composition_strategy(self, state: PipelineState) -> Dict[str, Any]:
        """ë‹µë³€ êµ¬ì„± ì „ëµ ì„ íƒ"""
        intent = state['intent']
        docs_count = len(state['retrieved_docs'])
        debate_activated = state.get('debate_activated', False)
        tools_used = state.get('tools_used', [])

        # ë³µì¡ë„ì™€ ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¥¸ ì „ëµ ì„ íƒ
        if debate_activated and 'context7' in tools_used:
            return {
                'name': 'multi_perspective_synthesis',
                'description': 'Context7ì™€ debateë¥¼ í™œìš©í•œ ë‹¤ê´€ì  ì¢…í•© ë‹µë³€',
                'template': 'comprehensive_analysis',
                'max_length': 800
            }
        elif intent == 'explain' and docs_count >= 3:
            return {
                'name': 'structured_explanation',
                'description': 'êµ¬ì¡°í™”ëœ ìƒì„¸ ì„¤ëª…',
                'template': 'detailed_explanation',
                'max_length': 600
            }
        elif intent == 'search' and docs_count > 0:
            return {
                'name': 'evidence_based_summary',
                'description': 'ê·¼ê±° ê¸°ë°˜ ìš”ì•½',
                'template': 'search_summary',
                'max_length': 400
            }
        else:
            return {
                'name': 'general_response',
                'description': 'ì¼ë°˜ì  ë‹µë³€',
                'template': 'basic_answer',
                'max_length': 300
            }

    async def _generate_answer_with_llm(self, query: str, docs: List[Dict], strategy: Dict, intent: str, tools_context: List[str]) -> str:
        """LLM APIë¥¼ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„±"""
        try:
            # 1. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = await self._build_composition_prompt(query, docs, strategy, intent, tools_context)

            # 2. LLM API í˜¸ì¶œ (GPT-4 ë˜ëŠ” Claude)
            llm_response = await self._call_llm_api(prompt, strategy)

            if llm_response['success']:
                return llm_response['answer']
            else:
                logger.warning(f"LLM API í˜¸ì¶œ ì‹¤íŒ¨: {llm_response['error']}")
                return await self._generate_fallback_answer(query, docs, strategy)

        except Exception as e:
            logger.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return await self._generate_fallback_answer(query, docs, strategy)

    async def _build_composition_prompt(self, query: str, docs: List[Dict], strategy: Dict, intent: str, tools_context: List[str]) -> str:
        """LLMìš© í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        base_prompt = f"""ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ RAG ì‹œìŠ¤í…œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì§ˆë¬¸: {query}
íŒŒì•…ëœ ì˜ë„: {intent}
ë‹µë³€ ì „ëµ: {strategy['description']}
ì‚¬ìš©ëœ ë„êµ¬: {', '.join(tools_context) if tools_context else 'ì—†ìŒ'}

ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ {strategy['max_length']}ì ì´ë‚´ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

"""

        # ë¬¸ì„œ ì •ë³´ ì¶”ê°€
        for i, doc in enumerate(docs[:5], 1):  # ìµœëŒ€ 5ê°œ ë¬¸ì„œ ì‚¬ìš©
            base_prompt += f"\në¬¸ì„œ {i}:\nì œëª©: {doc.get('source', {}).get('title', 'Unknown')}"
            base_prompt += f"\në‚´ìš©: {doc.get('text', '')[:300]}..."
            base_prompt += f"\nê´€ë ¨ë„: {doc.get('score', 0):.3f}\n"

        # ì „ëµë³„ íŠ¹ìˆ˜ ì§€ì‹œì‚¬í•­
        if strategy['name'] == 'multi_perspective_synthesis':
            base_prompt += "\n\në‹¤ì–‘í•œ ê´€ì ì—ì„œ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ê³ , ê°€ëŠ¥í•œ í•œ ê· í˜•ì¡íŒ ì‹œê°ì„ ì œì‹œí•´ì£¼ì„¸ìš”."
        elif strategy['name'] == 'structured_explanation':
            base_prompt += "\n\nêµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ì„¤ëª…í•˜ê³ , ê°€ëŠ¥í•˜ë©´ ë‹¨ê³„ë³„ë¡œ ë‚˜ëˆ„ì–´ ì„¤ëª…í•´ì£¼ì„¸ìš”."
        elif strategy['name'] == 'evidence_based_summary':
            base_prompt += "\n\nì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ê·¼ê±°ë¡œ í•˜ì—¬ ìš”ì•½í•´ì£¼ì„¸ìš”."

        base_prompt += "\n\në‹µë³€:"
        return base_prompt

    async def _call_llm_api(self, prompt: str, strategy: Dict) -> Dict[str, Any]:
        """LLM API í˜¸ì¶œ (GPT-4/Claude)"""
        try:
            # OpenAI API ì‚¬ìš© ì˜ˆì‹œ (ì‹¤ì œ API í‚¤ê°€ í•„ìš”)
            api_key = os.getenv('OPENAI_API_KEY')
            if not api_key:
                logger.warning("OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. í…œí”Œë¦¿ ê¸°ë°˜ ë‹µë³€ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                return {'success': False, 'error': 'No API key'}

            headers = {
                'Authorization': f'Bearer {api_key}',
                'Content-Type': 'application/json'
            }

            payload = {
                'model': 'gpt-4',
                'messages': [
                    {'role': 'user', 'content': prompt}
                ],
                'max_tokens': min(strategy['max_length'] * 2, 1000),
                'temperature': 0.3  # ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ temperature
            }

            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    'https://api.openai.com/v1/chat/completions',
                    headers=headers,
                    json=payload
                )

                if response.status_code == 200:
                    result = response.json()
                    answer = result['choices'][0]['message']['content']
                    return {'success': True, 'answer': answer.strip()}
                else:
                    logger.error(f"OpenAI API ì˜¤ë¥˜: {response.status_code} - {response.text}")
                    return {'success': False, 'error': f'API error: {response.status_code}'}

        except Exception as e:
            logger.error(f"LLM API í˜¸ì¶œ ì¤‘ ì˜ˆì™¸: {str(e)}")
            return {'success': False, 'error': str(e)}

    async def _generate_fallback_answer(self, query: str, docs: List[Dict], strategy: Dict) -> str:
        """LLM API ì‹¤íŒ¨ ì‹œ í…œí”Œë¦¿ ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
        logger.info("í…œí”Œë¦¿ ê¸°ë°˜ fallback ë‹µë³€ ìƒì„±")

        if strategy['name'] == 'multi_perspective_synthesis':
            answer = f"'{query}'ì— ëŒ€í•œ ì¢…í•© ë¶„ì„:\n\n"
            if docs:
                answer += "ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ì¢…í•©í•˜ë©´:\n"
                for i, doc in enumerate(docs[:3], 1):
                    answer += f"{i}. {doc['text'][:150]}...\n"
            else:
                answer += "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì¼ë°˜ì ì¸ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤."

        elif strategy['name'] == 'structured_explanation':
            answer = f"'{query}' ì„¤ëª…:\n\n"
            answer += "1. ê°œìš”\n"
            if docs:
                answer += f"   {docs[0]['text'][:100]}...\n\n"
            answer += "2. ìƒì„¸ ë‚´ìš©\n"
            if len(docs) > 1:
                answer += f"   {docs[1]['text'][:100]}...\n"

        elif strategy['name'] == 'evidence_based_summary':
            answer = f"'{query}' ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½:\n\n"
            for i, doc in enumerate(docs[:3], 1):
                answer += f"â€¢ {doc['text'][:120]}...\n"

        else:  # general_response
            answer = f"'{query}'ì— ëŒ€í•œ ë‹µë³€:\n"
            if docs:
                answer += f"ê²€ìƒ‰ëœ {len(docs)}ê°œ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ, "
                answer += docs[0]['text'][:200] + "..."
            else:
                answer += "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        return answer

    async def _validate_and_enhance_answer(self, draft_answer: str, state: PipelineState) -> str:
        """ë‹µë³€ í’ˆì§ˆ ê²€ì¦ ë° ê°œì„ """
        # 1. ê¸°ë³¸ í’ˆì§ˆ ì²´í¬
        if len(draft_answer.strip()) < 50:
            logger.warning("ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ë³´ì™„ ì¤‘...")
            draft_answer += f"\n\nì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”."

        # 2. ì¶œì²˜ ì •ë³´ ì¼ê´€ì„± ì²´í¬
        docs_count = len(state['retrieved_docs'])
        if docs_count > 0 and "ë¬¸ì„œ" not in draft_answer and "ìë£Œ" not in draft_answer:
            draft_answer += f"\n\n(ì´ ë‹µë³€ì€ {docs_count}ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.)"

        # 3. Debateê°€ í™œì„±í™”ëœ ê²½ìš° ì¶”ê°€ ê²€ì¦ ë§ˆí¬
        if state.get('debate_activated', False):
            draft_answer += "\n\nâœ“ ë‹¤ê°ë„ ê²€ì¦ ì™„ë£Œ"

        return draft_answer
    
    async def _step6_citation_extraction(self, state: PipelineState) -> PipelineState:
        """Step 6: Citation (ì¶œì²˜ ì¸ìš© â‰¥2ê°œ)"""
        step_start = time.time()
        logger.info("Step 6: Citation Extraction")
        
        # B-O3 í•„ìˆ˜ ìš”êµ¬ì‚¬í•­: ì¶œì²˜ â‰¥2ê°œ í¬í•¨
        sources = []
        for doc in state['retrieved_docs'][:5]:  # ìµœëŒ€ 5ê°œ ì¶œì²˜
            source_info = {
                "url": doc['source']['url'],
                "title": doc['source']['title'],
                "date": datetime.now().strftime("%Y-%m-%d"),  # TODO: ì‹¤ì œ ë¬¸ì„œ ë‚ ì§œ
                "version": state.get('taxonomy_version', '1.8.1'),
                "relevance_score": doc['score'],
                "text_snippet": doc['text'][:150] + "..." if len(doc['text']) > 150 else doc['text']
            }
            sources.append(source_info)
        
        citations_count = len(sources)
        
        step_time = time.time() - step_start
        
        state.update({
            "sources": sources,
            "citations_count": citations_count,
            "step_timings": {**state.get("step_timings", {}), "step6_cite": step_time}
        })
        
        logger.info(f"Step 6 ì™„ë£Œ: citations={citations_count}, time={step_time:.3f}s")
        return state
    
    async def _step7_final_response(self, state: PipelineState) -> PipelineState:
        """Step 7: Final Response (ìµœì¢… ì‘ë‹µ ìƒì„±)"""
        step_start = time.time()
        logger.info("Step 7: Final Response Generation")
        
        # ìµœì¢… ë‹µë³€ ìƒì„±
        draft = state['draft_answer']
        sources = state['sources']
        
        final_answer = draft + "\n\n"
        
        # ì¶œì²˜ ì •ë³´ ì¶”ê°€ (B-O3 í•„ìˆ˜ ìš”êµ¬ì‚¬í•­)
        if sources:
            final_answer += "ğŸ“š ì¶œì²˜:\n"
            for i, source in enumerate(sources, 1):
                final_answer += f"{i}. {source['title']} - {source['url']}\n"
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        final_answer += f"\nğŸ” ê²€ìƒ‰ ì •ë³´: {state.get('taxonomy_version', '1.8.1')} ë²„ì „ ê¸°ì¤€"
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = min(
            state.get('intent_confidence', 0.8),
            0.9 if len(sources) >= 2 else 0.7,  # ì¶œì²˜ 2ê°œ ì´ìƒì´ë©´ ë” ë†’ì€ ì‹ ë¢°ë„
            0.95  # ìµœëŒ€ ì‹ ë¢°ë„
        )
        
        # ë¹„ìš© ë° ì§€ì—°ì‹œê°„ ê³„ì‚° (B-O3 í•„ìˆ˜ ë©”íƒ€ë°ì´í„°)
        total_latency = sum(state.get("step_timings", {}).values())
        estimated_tokens = len(state['query']) + sum(len(doc['text']) for doc in state['retrieved_docs']) + len(final_answer)
        estimated_cost = estimated_tokens * 0.001  # ëŒ€ëµì ì¸ í† í°ë‹¹ ë¹„ìš© (â‚©0.001)
        
        step_time = time.time() - step_start
        
        state.update({
            "final_answer": final_answer,
            "confidence": confidence,
            "cost": estimated_cost,
            "latency": total_latency + step_time,
            "step_timings": {**state.get("step_timings", {}), "step7_respond": step_time}
        })
        
        logger.info(f"Step 7 ì™„ë£Œ: confidence={confidence:.3f}, cost=â‚©{estimated_cost:.3f}, "
                    f"total_latency={total_latency + step_time:.3f}s")
        return state
    
    async def execute(self, request: PipelineRequest) -> PipelineResponse:
        """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (B-O3 ì§„ì…ì ) - ë³µì›ë ¥ ê¸°ëŠ¥ ë° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í†µí•©"""
        pipeline_start = time.time()
        self.performance_metrics['total_requests'] += 1

        logger.info("=== B-O3 Enhanced Pipeline ì‹œì‘ ===")
        logger.info(f"Query: {request.query}")
        logger.info(f"Taxonomy Version: {request.taxonomy_version}")
        logger.info(f"MCP Server: {self.mcp_server_url}")
        logger.info(f"Total Requests: {self.performance_metrics['total_requests']}")

        # ë³µì›ë ¥ ì‹œìŠ¤í…œ ì‹œì‘ (ìˆì„ ê²½ìš°ì—ë§Œ)
        if self.resilience_manager:
            await self.resilience_manager.start()

        try:
            # ì´ˆê¸° ìƒíƒœ ì„¤ì •
            initial_state: PipelineState = {
                "query": request.query,
                "chunk_id": request.chunk_id,
                "taxonomy_version": request.taxonomy_version,
                "intent": "",
                "intent_confidence": 0.0,
                "retrieved_docs": [],
                "retrieval_filter_applied": False,
                "bm25_results": [],
                "vector_results": [],
                "answer_strategy": "",
                "plan_reasoning": [],
                "tools_used": [],
                "debate_activated": False,
                "debate_reasoning": None,
                "draft_answer": "",
                "sources": [],
                "citations_count": 0,
                "final_answer": "",
                "confidence": 0.0,
                "cost": 0.0,
                "latency": 0.0,
                "step_timings": {},
                "errors": [],
                "retry_count": 0
            }

            # LangGraph ì‹¤í–‰ (ë³µì›ë ¥ ê¸°ëŠ¥ ìˆìœ¼ë©´ ì ìš©, ì—†ìœ¼ë©´ ì§ì ‘ ì‹¤í–‰)
            if self.resilience_manager:
                final_state = await self.resilience_manager.execute_with_resilience(
                    self.graph.ainvoke, initial_state
                )
            else:
                final_state = await self.graph.ainvoke(initial_state)

            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            total_time = time.time() - pipeline_start
            self.performance_metrics['successful_requests'] += 1
            self._update_average_latency(total_time)

            # ì‘ë‹µ êµ¬ì„±
            response = PipelineResponse(
                answer=final_state["final_answer"],
                confidence=final_state["confidence"],
                sources=final_state["sources"],
                taxonomy_version=final_state["taxonomy_version"],
                cost=final_state["cost"],
                latency=total_time,
                intent=final_state["intent"],
                step_timings=final_state["step_timings"],
                debate_activated=final_state["debate_activated"],
                retrieved_count=len(final_state["retrieved_docs"]),
                citations_count=final_state["citations_count"]
            )

            # ì„±ëŠ¥ ìš”ì•½ ë¡œê¹…
            success_rate = (self.performance_metrics['successful_requests'] /
                          self.performance_metrics['total_requests']) * 100

            logger.info("=== B-O3 Enhanced Pipeline ì™„ë£Œ ===")
            logger.info(f"Total Time: {total_time:.3f}s")
            logger.info(f"Confidence: {response.confidence:.3f}")
            logger.info(f"Sources: {response.citations_count}")
            logger.info(f"Cost: â‚©{response.cost:.3f}")
            logger.info(f"Success Rate: {success_rate:.1f}%")
            logger.info(f"Tools Used: {final_state.get('tools_used', [])}")
            logger.info(f"MCP Tools Usage: {self.performance_metrics['tools_usage_count']}")

            # ì‹œìŠ¤í…œ ê±´ê°•ë„ ë¡œê¹… (ë³µì›ë ¥ ì‹œìŠ¤í…œì´ ìˆì„ ê²½ìš°ì—ë§Œ)
            if self.resilience_manager:
                health = self.resilience_manager.get_system_health()
                logger.info(f"ë©”ëª¨ë¦¬ ìƒíƒœ: {health['memory']['status']} ({health['memory']['usage']['current_mb']:.1f}MB)")

            return response

        except Exception as e:
            self.performance_metrics['failed_requests'] += 1
            logger.error(f"Pipeline ì‹¤í–‰ ì˜¤ë¥˜ (ì‹¤íŒ¨ìœ¨: {(self.performance_metrics['failed_requests']/self.performance_metrics['total_requests']*100):.1f}%): {str(e)}", exc_info=True)
            raise
        finally:
            # ë³µì›ë ¥ ì‹œìŠ¤í…œ ì •ë¦¬ (ìˆì„ ê²½ìš°ì—ë§Œ)
            if self.resilience_manager:
                await self.resilience_manager.stop()

            # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            await self._cleanup_resources()

    def _update_average_latency(self, current_latency: float):
        """í‰ê·  ì§€ì—°ì‹œê°„ ì—…ë°ì´íŠ¸"""
        if self.performance_metrics['successful_requests'] == 1:
            self.performance_metrics['average_latency'] = current_latency
        else:
            # ì´ë™ í‰ê·  ê³„ì‚°
            prev_avg = self.performance_metrics['average_latency']
            count = self.performance_metrics['successful_requests']
            self.performance_metrics['average_latency'] = ((prev_avg * (count - 1)) + current_latency) / count

    async def _cleanup_resources(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # HTTP í´ë¼ì´ì–¸íŠ¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ëŠ” íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ ì‹œì—ë§Œ
            pass
        except Exception as e:
            logger.warning(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")

    async def close(self):
        """íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ ë° ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            await self.client.aclose()
            await self.mcp_client.aclose()
            logger.info("íŒŒì´í”„ë¼ì¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {str(e)}")

    def get_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°˜í™˜"""
        success_rate = 0.0
        if self.performance_metrics['total_requests'] > 0:
            success_rate = (self.performance_metrics['successful_requests'] /
                          self.performance_metrics['total_requests']) * 100

        return {
            'total_requests': self.performance_metrics['total_requests'],
            'successful_requests': self.performance_metrics['successful_requests'],
            'failed_requests': self.performance_metrics['failed_requests'],
            'success_rate_percent': success_rate,
            'average_latency_seconds': self.performance_metrics['average_latency'],
            'tools_usage_count': self.performance_metrics['tools_usage_count'],
            'available_mcp_tools': self.available_mcp_tools
        }


# ì „ì—­ íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤
_pipeline_instance: Optional[LangGraphPipeline] = None

def get_pipeline() -> LangGraphPipeline:
    """íŒŒì´í”„ë¼ì¸ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _pipeline_instance
    if _pipeline_instance is None:
        _pipeline_instance = LangGraphPipeline()
    return _pipeline_instance


# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜ˆì‹œ"""
    pipeline = get_pipeline()
    
    test_request = PipelineRequest(
        query="AI RAG ì‹œìŠ¤í…œì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
        taxonomy_version="1.8.1"
    )
    
    response = await pipeline.execute(test_request)
    
    print("\n=== B-O3 Pipeline ì‹¤í–‰ ê²°ê³¼ ===")
    print(f"ë‹µë³€: {response.answer}")
    print(f"ì‹ ë¢°ë„: {response.confidence:.3f}")
    print(f"ì¶œì²˜ ê°œìˆ˜: {response.citations_count}")
    print(f"ì‹¤í–‰ ì‹œê°„: {response.latency:.3f}ì´ˆ")
    print(f"ë¹„ìš©: â‚©{response.cost:.3f}")
    print(f"ë‹¨ê³„ë³„ ì‹œê°„: {response.step_timings}")


if __name__ == "__main__":
    asyncio.run(main())
