"""
B-O3: 7-Step LangGraph Pipeline Implementation
intent â†’ retrieve â†’ plan â†’ tools/debate â†’ compose â†’ cite â†’ respond

Week-1 ëª©í‘œ: ì™„ì „í•œ 7-Step íŒŒì´í”„ë¼ì¸ ê³¨ê²© + ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
"""

import asyncio
import time
import logging
import httpx
import os
from typing import TypedDict, List, Dict, Any, Optional
from datetime import datetime

# LangGraph ëŒ€ì‹  ê°„ë‹¨í•œ ê·¸ë˜í”„ êµ¬í˜„ ì‚¬ìš©
from typing import Dict, List, Callable, Any
from pydantic import BaseModel, Field

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class PipelineState(TypedDict):
    """LangGraph íŒŒì´í”„ë¼ì¸ ìƒíƒœ ê´€ë¦¬"""
    # ì…ë ¥
    query: str
    chunk_id: Optional[str]
    taxonomy_version: str
    
    # Step 1: Intent
    intent: str
    intent_confidence: float
    
    # Step 2: Retrieve
    retrieved_docs: List[Dict[str, Any]]
    retrieval_filter_applied: bool
    bm25_results: List[Dict]
    vector_results: List[Dict]
    
    # Step 3: Plan
    answer_strategy: str
    plan_reasoning: List[str]
    
    # Step 4: Tools/Debate
    tools_used: List[str]
    debate_activated: bool
    debate_reasoning: Optional[str]
    
    # Step 5: Compose
    draft_answer: str
    
    # Step 6: Cite
    sources: List[Dict[str, Any]]
    citations_count: int
    
    # Step 7: Respond
    final_answer: str
    confidence: float
    
    # ë©”íƒ€ë°ì´í„° (B-O3 í•„ìˆ˜ ìš”êµ¬ì‚¬í•­)
    cost: float  # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ë¹„ìš©
    latency: float  # íŒŒì´í”„ë¼ì¸ ì „ì²´ ì‹œê°„
    step_timings: Dict[str, float]  # ê° ë‹¨ê³„ë³„ ì‹¤í–‰ ì‹œê°„
    
    # ì—ëŸ¬ ì²˜ë¦¬
    errors: List[str]
    retry_count: int


class PipelineRequest(BaseModel):
    """íŒŒì´í”„ë¼ì¸ ìš”ì²­ ìŠ¤í‚¤ë§ˆ"""
    query: str = Field(..., min_length=1, description="ì‚¬ìš©ì ì§ˆì˜")
    taxonomy_version: str = Field(default="1.8.1", description="íƒì†Œë…¸ë¯¸ ë²„ì „")
    chunk_id: Optional[str] = Field(default=None, description="ì²­í¬ ID (ë¶„ë¥˜ìš©)")
    filters: Optional[Dict[str, Any]] = Field(default=None, description="ê²€ìƒ‰ í•„í„°")
    options: Optional[Dict[str, Any]] = Field(default={}, description="ì¶”ê°€ ì˜µì…˜")


class PipelineResponse(BaseModel):
    """íŒŒì´í”„ë¼ì¸ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ (B-O3 ë©”íƒ€ë°ì´í„° í¬í•¨)"""
    # í•µì‹¬ ì‘ë‹µ
    answer: str = Field(..., description="ìµœì¢… ë‹µë³€")
    confidence: float = Field(..., ge=0.0, le=1.0, description="ì‹ ë¢°ë„ ì ìˆ˜")
    
    # B-O3 í•„ìˆ˜ ë©”íƒ€ë°ì´í„°
    sources: List[Dict[str, Any]] = Field(..., min_items=0, description="ì¶œì²˜ ëª©ë¡ (â‰¥2ê°œ ê¶Œì¥)")
    taxonomy_version: str = Field(..., description="ì‚¬ìš©ëœ íƒì†Œë…¸ë¯¸ ë²„ì „")
    cost: float = Field(..., ge=0.0, description="í† í° ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ë¹„ìš© (â‚©)")
    latency: float = Field(..., ge=0.0, description="íŒŒì´í”„ë¼ì¸ ì „ì²´ ì‹¤í–‰ ì‹œê°„ (ì´ˆ)")
    
    # ë””ë²„ê¹… ì •ë³´
    intent: str = Field(..., description="íŒŒì•…ëœ ì‚¬ìš©ì ì˜ë„")
    step_timings: Dict[str, float] = Field(..., description="ê° ë‹¨ê³„ë³„ ì‹¤í–‰ ì‹œê°„")
    debate_activated: bool = Field(default=False, description="debate ìŠ¤ìœ„ì¹˜ í™œì„±í™” ì—¬ë¶€")
    
    # í’ˆì§ˆ ì •ë³´
    retrieved_count: int = Field(default=0, description="ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜")
    citations_count: int = Field(default=0, description="ì¸ìš©ëœ ì¶œì²˜ ìˆ˜")


class SimpleGraph:
    """ê°„ë‹¨í•œ ìˆœì°¨ ì‹¤í–‰ ê·¸ë˜í”„"""
    
    def __init__(self):
        self.steps = []
        
    def add_step(self, name: str, func: Callable):
        self.steps.append((name, func))
        
    async def ainvoke(self, state: PipelineState) -> PipelineState:
        """ìˆœì°¨ì ìœ¼ë¡œ ëª¨ë“  ë‹¨ê³„ ì‹¤í–‰"""
        for step_name, step_func in self.steps:
            logger.info(f"ì‹¤í–‰ ì¤‘: {step_name}")
            state = await step_func(state)
        return state


class LangGraphPipeline:
    """B-O3 7-Step íŒŒì´í”„ë¼ì¸ (AíŒ€ API ì—°ë™, PRD ì¤€ìˆ˜)"""
    
    def __init__(self, a_team_base_url: str = "http://localhost:8001"):
        self.a_team_base_url = a_team_base_url
        self.client = httpx.AsyncClient()
        self.graph = self._build_graph()
        # ë³µì›ë ¥ ì‹œìŠ¤í…œ í†µí•©
        try:
            from pipeline_resilience import get_resilience_manager
            self.resilience_manager = get_resilience_manager()
        except ImportError:
            logger.warning("pipeline_resilience ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì‹¤í–‰ ëª¨ë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            self.resilience_manager = None
        
    def _build_graph(self) -> SimpleGraph:
        """7-Step íŒŒì´í”„ë¼ì¸ ê·¸ë˜í”„ êµ¬ì„±"""
        workflow = SimpleGraph()
        
        # 7ê°œ ë‹¨ê³„ ìˆœì°¨ ì¶”ê°€
        workflow.add_step("step1_intent", self._step1_intent_classification)
        workflow.add_step("step2_retrieve", self._step2_hybrid_retrieval)
        workflow.add_step("step3_plan", self._step3_answer_planning)
        workflow.add_step("step4_tools_debate", self._step4_tools_and_debate)
        workflow.add_step("step5_compose", self._step5_answer_composition)
        workflow.add_step("step6_cite", self._step6_citation_extraction)
        workflow.add_step("step7_respond", self._step7_final_response)
        
        return workflow
    
    async def _step1_intent_classification(self, state: PipelineState) -> PipelineState:
        """Step 1: Intent Classification (ì‚¬ìš©ì ì˜ë„ íŒŒì•…)"""
        step_start = time.time()
        logger.info(f"Step 1: Intent Classification - Query: {state['query']}")
        
        # TODO: ì‹¤ì œ ì˜ë„ ë¶„ë¥˜ ë¡œì§ êµ¬í˜„ (LLM í˜¸ì¶œ)
        # í˜„ì¬ëŠ” ìŠ¤ìºí´ë”© êµ¬í˜„
        query = state['query'].lower()
        
        if any(keyword in query for keyword in ['ê²€ìƒ‰', 'search', 'ì°¾ì•„', 'ì¡°íšŒ']):
            intent = "search"
        elif any(keyword in query for keyword in ['ë¶„ë¥˜', 'classify', 'ì¹´í…Œê³ ë¦¬']):
            intent = "classify"
        elif any(keyword in query for keyword in ['ì„¤ëª…', 'explain', 'ì•Œë ¤ì¤˜']):
            intent = "explain"
        else:
            intent = "general_query"
            
        intent_confidence = 0.8  # TODO: ì‹¤ì œ ì‹ ë¢°ë„ ê³„ì‚°
        
        step_time = time.time() - step_start
        
        state.update({
            "intent": intent,
            "intent_confidence": intent_confidence,
            "step_timings": {**state.get("step_timings", {}), "step1_intent": step_time}
        })
        
        logger.info(f"Step 1 ì™„ë£Œ: intent={intent}, confidence={intent_confidence:.3f}, time={step_time:.3f}s")
        return state
    
    async def _step2_hybrid_retrieval(self, state: PipelineState) -> PipelineState:
        """Step 2: Hybrid Retrieval (AíŒ€ /search API í˜¸ì¶œ, PRD ì¤€ìˆ˜)"""
        step_start = time.time()
        logger.info(f"Step 2: Hybrid Retrieval - Intent: {state['intent']}")
        
        try:
            # AíŒ€ /search API í˜¸ì¶œ (PRD ì¤€ìˆ˜)
            search_request = {
                "q": state['query'],
                "filters": None,  # TODO: intentì— ë”°ë¥¸ í•„í„° ì ìš©
                "bm25_topk": 12,
                "vector_topk": 12,
                "rerank_candidates": 50,
                "final_topk": 5
            }
            # Retry-After aware call to A-team /search
            max_retries = int(os.getenv("MAX_RETRIES", os.getenv("ORCH_RETRY_MAX", "3")))
            backoff = float(os.getenv("ORCH_RETRY_BACKOFF", "1.0"))
            jitter = float(os.getenv("ORCH_RETRY_JITTER_MAX", "0.2"))
            max_cap = float(os.getenv("ORCH_RETRY_MAX_DELAY", "10.0"))
            for attempt in range(max_retries + 1):
                try:
                    response = await self.client.post(
                        f"{self.a_team_base_url}/search",
                        json=search_request
                    )
                    response.raise_for_status()
                    break
                except httpx.HTTPStatusError as e:
                    if e.response.status_code in [429, 500, 502, 503, 504] and attempt < max_retries:
                        ra = e.response.headers.get("Retry-After")
                        parsed = None
                        if ra:
                            try:
                                parsed = float(ra)
                            except Exception:
                                try:
                                    import email.utils, time as _t
                                    dt = email.utils.parsedate_to_datetime(ra)
                                    parsed = max(0.0, dt.timestamp() - _t.time())
                                except Exception:
                                    parsed = None
                        base = backoff * (2 ** attempt)
                        delay = (parsed if parsed is not None else base) + random.uniform(0, jitter)
                        delay = min(delay, max_cap)
                        logger.warning(f"/search retry {attempt+1}/{max_retries+1} after {e.response.status_code}; sleep {delay:.2f}s")
                        await asyncio.sleep(delay)
                        continue
                    raise
                except Exception as e:
                    if attempt < max_retries:
                        delay = min(backoff * (2 ** attempt) + random.uniform(0, jitter), max_cap)
                        logger.warning(f"/search exception retry {attempt+1}/{max_retries+1}; sleep {delay:.2f}s: {e}")
                        await asyncio.sleep(delay)
                        continue
                    raise

            search_result = response.json()
                retrieved_docs = search_result.get("hits", [])
                
                # AíŒ€ ì‘ë‹µì„ BíŒ€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                formatted_docs = []
                for i, doc in enumerate(retrieved_docs):
                    formatted_doc = {
                        "chunk_id": doc.get("chunk_id", f"doc_{i}"),
                        "score": doc.get("score", 0.0),
                        "text": doc.get("text", ""),
                        "source": {
                            "url": doc.get("source", {}).get("url", ""),
                            "title": doc.get("source", {}).get("title", f"ë¬¸ì„œ {i+1}")
                        }
                    }
                    formatted_docs.append(formatted_doc)
                
                logger.info(f"AíŒ€ /search API í˜¸ì¶œ ì„±ê³µ: {len(formatted_docs)}ê°œ ë¬¸ì„œ")
                
                step_time = time.time() - step_start
                
                state.update({
                    "bm25_results": [],  # AíŒ€ì—ì„œ ì´ë¯¸ í†µí•© ì²˜ë¦¬ë¨
                    "vector_results": [],  # AíŒ€ì—ì„œ ì´ë¯¸ í†µí•© ì²˜ë¦¬ë¨  
                    "retrieved_docs": formatted_docs,
                    "retrieval_filter_applied": True,
                    "step_timings": {**state.get("step_timings", {}), "step2_retrieve": step_time}
                })
                
                logger.info(f"Step 2 ì™„ë£Œ: retrieved={len(formatted_docs)} docs, time={step_time:.3f}s")
                return state
                
            else:
                logger.error(f"AíŒ€ /search API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
                
        except Exception as e:
            logger.error(f"AíŒ€ /search API í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")
        
        # AíŒ€ API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ë¹ˆ ê²°ê³¼ë¡œ ì²˜ë¦¬ (PRD ì¤€ìˆ˜)
        step_time = time.time() - step_start
        
        state.update({
            "bm25_results": [],
            "vector_results": [],
            "retrieved_docs": [],
            "retrieval_filter_applied": False,
            "step_timings": {**state.get("step_timings", {}), "step2_retrieve": step_time}
        })
        
        logger.warning("Step 2 ì™„ë£Œ: AíŒ€ API í˜¸ì¶œ ì‹¤íŒ¨, ë¹ˆ ê²°ê³¼ ë°˜í™˜")
        return state
    
    async def _step3_answer_planning(self, state: PipelineState) -> PipelineState:
        """Step 3: Answer Planning (ë‹µë³€ ì „ëµ ê³„íš)"""
        step_start = time.time()
        logger.info(f"Step 3: Answer Planning - Intent: {state['intent']}")
        
        # ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”: ì „ëµ ê²°ì •ê³¼ ì¶”ë¡ ì„ ë™ì‹œ ìˆ˜í–‰
        docs_count = len(state['retrieved_docs'])
        intent = state['intent']
        
        async def determine_strategy():
            """ë‹µë³€ ì „ëµ ê²°ì •"""
            if intent == "search" and docs_count > 0:
                return "search_results_summary"
            elif intent == "explain" and docs_count > 0:
                return "detailed_explanation"
            else:
                return "general_answer"
        
        async def generate_reasoning(strategy_type):
            """ì „ëµë³„ ì¶”ë¡  ìƒì„±"""
            if strategy_type == "search_results_summary":
                return [
                    f"ê²€ìƒ‰ ì˜ë„ë¡œ íŒŒì•…ë¨",
                    f"{docs_count}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨",
                    "ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ì „ëµ ì„ íƒ"
                ]
            elif strategy_type == "detailed_explanation":
                return [
                    "ì„¤ëª… ìš”ì²­ìœ¼ë¡œ íŒŒì•…ë¨",
                    "ìƒì„¸ ì„¤ëª… ì „ëµ ì„ íƒ",
                    "ê·¼ê±° ë¬¸ì„œ ê¸°ë°˜ ì„¤ëª…"
                ]
            else:
                return [
                    "ì¼ë°˜ ì§ˆì˜ë¡œ ì²˜ë¦¬",
                    "ê¸°ë³¸ ë‹µë³€ ì „ëµ ì ìš©"
                ]
        
        # ì „ëµ ê²°ì •
        strategy = await determine_strategy()
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì¶”ë¡  ìƒì„± ì‹œê°„ ë‹¨ì¶•
        reasoning = await generate_reasoning(strategy)
            
        step_time = time.time() - step_start
        
        state.update({
            "answer_strategy": strategy,
            "plan_reasoning": reasoning,
            "step_timings": {**state.get("step_timings", {}), "step3_plan": step_time}
        })
        
        logger.info(f"Step 3 ì™„ë£Œ: strategy={strategy}, time={step_time:.3f}s")
        return state
    
    async def _step4_tools_and_debate(self, state: PipelineState) -> PipelineState:
        """Step 4: Tools/Debate (ë„êµ¬ ì‚¬ìš© ë° debate ìŠ¤ìœ„ì¹˜)"""
        step_start = time.time()
        logger.info("Step 4: Tools and Debate")
        
        # TODO: ì‹¤ì œ ë„êµ¬ ì‚¬ìš© ë° debate ë¡œì§ êµ¬í˜„
        tools_used = []
        debate_activated = False
        debate_reasoning = None
        
        # debate ìŠ¤ìœ„ì¹˜ ë¡œì§: confidence < 0.7 ì‹œ í™œì„±í™”
        if state.get('intent_confidence', 1.0) < 0.7:
            debate_activated = True
            debate_reasoning = f"ì˜ë„ íŒŒì•… ì‹ ë¢°ë„ {state['intent_confidence']:.3f} < 0.7, debate í™œì„±í™”"
            tools_used.append("debate_module")
        
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì¡±í•œ ê²½ìš° ì¶”ê°€ ë„êµ¬ ì‚¬ìš©
        if len(state['retrieved_docs']) < 2:
            tools_used.append("fallback_search")
            
        step_time = time.time() - step_start
        
        state.update({
            "tools_used": tools_used,
            "debate_activated": debate_activated,
            "debate_reasoning": debate_reasoning,
            "step_timings": {**state.get("step_timings", {}), "step4_tools_debate": step_time}
        })
        
        logger.info(f"Step 4 ì™„ë£Œ: tools={tools_used}, debate={debate_activated}, time={step_time:.3f}s")
        return state
    
    async def _step5_answer_composition(self, state: PipelineState) -> PipelineState:
        """Step 5: Answer Composition (ë‹µë³€ êµ¬ì„±)"""
        step_start = time.time()
        logger.info("Step 5: Answer Composition")
        
        # TODO: ì‹¤ì œ ë‹µë³€ ìƒì„± ë¡œì§ êµ¬í˜„ (LLM í˜¸ì¶œ)
        query = state['query']
        docs = state['retrieved_docs']
        strategy = state['answer_strategy']
        
        if strategy == "search_results_summary":
            draft_answer = f"'{query}' ê²€ìƒ‰ ê²°ê³¼:\n"
            for i, doc in enumerate(docs[:3], 1):
                draft_answer += f"{i}. {doc['text'][:100]}...\n"
        elif strategy == "detailed_explanation":
            draft_answer = f"'{query}'ì— ëŒ€í•œ ì„¤ëª…:\n"
            draft_answer += f"ê²€ìƒ‰ëœ {len(docs)}ê°œ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n"
            draft_answer += docs[0]['text'][:200] + "..." if docs else "ê´€ë ¨ ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        else:
            draft_answer = f"'{query}'ì— ëŒ€í•œ ë‹µë³€ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤. ê²€ìƒ‰ëœ ë¬¸ì„œ: {len(docs)}ê°œ"
            
        step_time = time.time() - step_start
        
        state.update({
            "draft_answer": draft_answer,
            "step_timings": {**state.get("step_timings", {}), "step5_compose": step_time}
        })
        
        logger.info(f"Step 5 ì™„ë£Œ: draft length={len(draft_answer)}, time={step_time:.3f}s")
        return state
    
    async def _step6_citation_extraction(self, state: PipelineState) -> PipelineState:
        """Step 6: Citation (ì¶œì²˜ ì¸ìš© â‰¥2ê°œ)"""
        step_start = time.time()
        logger.info("Step 6: Citation Extraction")
        
        # B-O3 í•„ìˆ˜ ìš”êµ¬ì‚¬í•­: ì¶œì²˜ â‰¥2ê°œ í¬í•¨
        sources = []
        for doc in state['retrieved_docs'][:5]:  # ìµœëŒ€ 5ê°œ ì¶œì²˜
            source_info = {
                "url": doc['source']['url'],
                "title": doc['source']['title'],
                "date": datetime.now().strftime("%Y-%m-%d"),  # TODO: ì‹¤ì œ ë¬¸ì„œ ë‚ ì§œ
                "version": state.get('taxonomy_version', '1.8.1'),
                "relevance_score": doc['score'],
                "text_snippet": doc['text'][:150] + "..." if len(doc['text']) > 150 else doc['text']
            }
            sources.append(source_info)
        
        citations_count = len(sources)
        
        step_time = time.time() - step_start
        
        state.update({
            "sources": sources,
            "citations_count": citations_count,
            "step_timings": {**state.get("step_timings", {}), "step6_cite": step_time}
        })
        
        logger.info(f"Step 6 ì™„ë£Œ: citations={citations_count}, time={step_time:.3f}s")
        return state
    
    async def _step7_final_response(self, state: PipelineState) -> PipelineState:
        """Step 7: Final Response (ìµœì¢… ì‘ë‹µ ìƒì„±)"""
        step_start = time.time()
        logger.info("Step 7: Final Response Generation")
        
        # ìµœì¢… ë‹µë³€ ìƒì„±
        draft = state['draft_answer']
        sources = state['sources']
        
        final_answer = draft + "\n\n"
        
        # ì¶œì²˜ ì •ë³´ ì¶”ê°€ (B-O3 í•„ìˆ˜ ìš”êµ¬ì‚¬í•­)
        if sources:
            final_answer += "ğŸ“š ì¶œì²˜:\n"
            for i, source in enumerate(sources, 1):
                final_answer += f"{i}. {source['title']} - {source['url']}\n"
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        final_answer += f"\nğŸ” ê²€ìƒ‰ ì •ë³´: {state.get('taxonomy_version', '1.8.1')} ë²„ì „ ê¸°ì¤€"
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = min(
            state.get('intent_confidence', 0.8),
            0.9 if len(sources) >= 2 else 0.7,  # ì¶œì²˜ 2ê°œ ì´ìƒì´ë©´ ë” ë†’ì€ ì‹ ë¢°ë„
            0.95  # ìµœëŒ€ ì‹ ë¢°ë„
        )
        
        # ë¹„ìš© ë° ì§€ì—°ì‹œê°„ ê³„ì‚° (B-O3 í•„ìˆ˜ ë©”íƒ€ë°ì´í„°)
        total_latency = sum(state.get("step_timings", {}).values())
        estimated_tokens = len(state['query']) + sum(len(doc['text']) for doc in state['retrieved_docs']) + len(final_answer)
        estimated_cost = estimated_tokens * 0.001  # ëŒ€ëµì ì¸ í† í°ë‹¹ ë¹„ìš© (â‚©0.001)
        
        step_time = time.time() - step_start
        
        state.update({
            "final_answer": final_answer,
            "confidence": confidence,
            "cost": estimated_cost,
            "latency": total_latency + step_time,
            "step_timings": {**state.get("step_timings", {}), "step7_respond": step_time}
        })
        
        logger.info(f"Step 7 ì™„ë£Œ: confidence={confidence:.3f}, cost=â‚©{estimated_cost:.3f}, "
                   f"total_latency={total_latency + step_time:.3f}s")
        return state
    
    async def execute(self, request: PipelineRequest) -> PipelineResponse:
        """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (B-O3 ì§„ì…ì ) - ë³µì›ë ¥ ê¸°ëŠ¥ í†µí•©"""
        pipeline_start = time.time()
        logger.info(f"=== B-O3 Pipeline ì‹œì‘ (ë³µì›ë ¥ ê¸°ëŠ¥ ì ìš©) ===")
        logger.info(f"Query: {request.query}")
        logger.info(f"Taxonomy Version: {request.taxonomy_version}")
        
        # ë³µì›ë ¥ ì‹œìŠ¤í…œ ì‹œì‘ (ìˆì„ ê²½ìš°ì—ë§Œ)
        if self.resilience_manager:
            await self.resilience_manager.start()
        
        try:
            # ì´ˆê¸° ìƒíƒœ ì„¤ì •
            initial_state: PipelineState = {
                "query": request.query,
                "chunk_id": request.chunk_id,
                "taxonomy_version": request.taxonomy_version,
                "intent": "",
                "intent_confidence": 0.0,
                "retrieved_docs": [],
                "retrieval_filter_applied": False,
                "bm25_results": [],
                "vector_results": [],
                "answer_strategy": "",
                "plan_reasoning": [],
                "tools_used": [],
                "debate_activated": False,
                "debate_reasoning": None,
                "draft_answer": "",
                "sources": [],
                "citations_count": 0,
                "final_answer": "",
                "confidence": 0.0,
                "cost": 0.0,
                "latency": 0.0,
                "step_timings": {},
                "errors": [],
                "retry_count": 0
            }
            
            # LangGraph ì‹¤í–‰ (ë³µì›ë ¥ ê¸°ëŠ¥ ìˆìœ¼ë©´ ì ìš©, ì—†ìœ¼ë©´ ì§ì ‘ ì‹¤í–‰)
            if self.resilience_manager:
                final_state = await self.resilience_manager.execute_with_resilience(
                    self.graph.ainvoke, initial_state
                )
            else:
                final_state = await self.graph.ainvoke(initial_state)
            
            # ì‘ë‹µ êµ¬ì„±
            total_time = time.time() - pipeline_start
            response = PipelineResponse(
                answer=final_state["final_answer"],
                confidence=final_state["confidence"],
                sources=final_state["sources"],
                taxonomy_version=final_state["taxonomy_version"],
                cost=final_state["cost"],
                latency=total_time,
                intent=final_state["intent"],
                step_timings=final_state["step_timings"],
                debate_activated=final_state["debate_activated"],
                retrieved_count=len(final_state["retrieved_docs"]),
                citations_count=final_state["citations_count"]
            )
            
            logger.info(f"=== B-O3 Pipeline ì™„ë£Œ ===")
            logger.info(f"Total Time: {total_time:.3f}s")
            logger.info(f"Confidence: {response.confidence:.3f}")
            logger.info(f"Sources: {response.citations_count}")
            logger.info(f"Cost: â‚©{response.cost:.3f}")
            
            # ì‹œìŠ¤í…œ ê±´ê°•ë„ ë¡œê¹… (ë³µì›ë ¥ ì‹œìŠ¤í…œì´ ìˆì„ ê²½ìš°ì—ë§Œ)
            if self.resilience_manager:
                health = self.resilience_manager.get_system_health()
                logger.info(f"ë©”ëª¨ë¦¬ ìƒíƒœ: {health['memory']['status']} ({health['memory']['usage']['current_mb']:.1f}MB)")
            
            return response
            
        except Exception as e:
            logger.error(f"Pipeline ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}", exc_info=True)
            raise
        finally:
            # ë³µì›ë ¥ ì‹œìŠ¤í…œ ì •ë¦¬ (ìˆì„ ê²½ìš°ì—ë§Œ)
            if self.resilience_manager:
                await self.resilience_manager.stop()


# ì „ì—­ íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤
_pipeline_instance: Optional[LangGraphPipeline] = None

def get_pipeline() -> LangGraphPipeline:
    """íŒŒì´í”„ë¼ì¸ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _pipeline_instance
    if _pipeline_instance is None:
        _pipeline_instance = LangGraphPipeline()
    return _pipeline_instance


# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜ˆì‹œ"""
    pipeline = get_pipeline()
    
    test_request = PipelineRequest(
        query="AI RAG ì‹œìŠ¤í…œì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
        taxonomy_version="1.8.1"
    )
    
    response = await pipeline.execute(test_request)
    
    print("\n=== B-O3 Pipeline ì‹¤í–‰ ê²°ê³¼ ===")
    print(f"ë‹µë³€: {response.answer}")
    print(f"ì‹ ë¢°ë„: {response.confidence:.3f}")
    print(f"ì¶œì²˜ ê°œìˆ˜: {response.citations_count}")
    print(f"ì‹¤í–‰ ì‹œê°„: {response.latency:.3f}ì´ˆ")
    print(f"ë¹„ìš©: â‚©{response.cost:.3f}")
    print(f"ë‹¨ê³„ë³„ ì‹œê°„: {response.step_timings}")


if __name__ == "__main__":
    asyncio.run(main())
