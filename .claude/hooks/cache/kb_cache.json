{"kb_rag-evaluation-specialist": {"data": {"subagent": "rag-evaluation-specialist", "timestamp": "2025-09-14T15:26:33.595226", "search_results": [{"query": "RAGAS RAG evaluation metrics 2025 latest version", "url": "https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/", "title": "RAGAS v2.0 - Latest RAG Evaluation Framework 2025", "content": "RAGAS v2.0 released April 28, 2025. Reference-free evaluation with faithfulness, context precision, context recall, answer relevancy metrics. LLM-based evaluation framework.", "relevance_score": 0.98, "timestamp": "2025-09-14 15:26:33.595301", "subagent": "rag-evaluation-specialist", "category": "framework"}, {"query": "TruLens Phoenix Arize RAG evaluation 2025", "url": "https://arize.com/docs/phoenix/cookbook/evaluation/evaluate-rag", "title": "Arize Phoenix - Open-Source RAG Evaluation and Observability 2025", "content": "Phoenix designed for experimentation, data visualization, evaluation, troubleshooting. RAG triad metrics: context relevance, groundedness, answer relevance. UI focused on debugging.", "relevance_score": 0.96, "timestamp": "2025-09-14 15:26:34.598454", "subagent": "rag-evaluation-specialist", "category": "platform"}, {"query": "LangSmith LangChain RAG evaluation 2025 features", "url": "https://docs.smith.langchain.com/evaluation/tutorials/rag", "title": "LangSmith RAG Evaluation - 2025 Latest Features", "content": "LangSmith 2025 updates: Align Evals, Trace Mode in Studio, cross-framework support. Correctness, groundedness, relevance evaluators. Human feedback integration.", "relevance_score": 0.95, "timestamp": "2025-09-14 15:26:35.608141", "subagent": "rag-evaluation-specialist", "category": "platform"}, {"query": "RAG evaluation best practices 2025", "url": "https://www.pinecone.io/learn/series/vector-databases-in-production-for-busy-engineers/rag-evaluation/", "title": "RAG Evaluation Best Practices - Industry Standards 2025", "content": "Minimum 20 questions for personal projects, 100 for enterprise. Component-level evaluation: retrieval metrics, generation metrics. Production monitoring essential.", "relevance_score": 0.93, "timestamp": "2025-09-14 15:26:36.609200", "subagent": "rag-evaluation-specialist", "category": "best_practices"}, {"query": "DeepEval RAG evaluation metrics 2025", "url": "https://docs.confident-ai.com/docs/evaluation-introduction", "title": "DeepEval - 14+ LLM Evaluation Metrics for RAG 2025", "content": "DeepEval offers 14+ evaluation metrics for RAG and fine-tuning. Updated with latest research. Self-explaining metrics for debugging. Enterprise-ready evaluation.", "relevance_score": 0.92, "timestamp": "2025-09-14 15:26:37.610250", "subagent": "rag-evaluation-specialist", "category": "framework"}, {"query": "RAG evaluation enterprise deployment 2025", "url": "https://www.comet.com/site/blog/llm-evaluation-frameworks/", "title": "Enterprise RAG Evaluation Framework Comparison 2025", "content": "Head-to-head comparison of RAG evaluation frameworks. TruLens for domain-specific optimization, Phoenix for debugging, Arize for monitoring. Enterprise considerations.", "relevance_score": 0.9, "timestamp": "2025-09-14 15:26:38.611300", "subagent": "rag-evaluation-specialist", "category": "enterprise"}], "frameworks": {"ragas": {"name": "RAGAS (Retrieval Augmented Generation Assessment)", "version": "2.0", "release_date": "April 28, 2025", "key_features": ["Reference-free evaluation using LLMs", "Component-level metrics (retrieval + generation)", "Faithfulness, Context Precision, Context Recall, Answer Relevancy", "Extensible framework for custom metrics", "Cost-effective evaluation without ground truth labels"], "installation": "pip install ragas", "documentation_urls": ["https://docs.ragas.io/en/stable/", "https://github.com/explodinggradients/ragas"]}, "arize_phoenix": {"name": "Arize Phoenix", "version": "4.x", "type": "Open-source", "key_features": ["AI observability and evaluation platform", "RAG triad metrics (context relevance, groundedness, answer relevance)", "LLM trace logging and analytics", "UI focused on troubleshooting RAG scenarios", "Real-time debugging capabilities"], "installation": "pip install arize-phoenix", "documentation_urls": ["https://arize.com/docs/phoenix/", "https://phoenix.arize.com/"]}, "langsmith": {"name": "LangSmith", "version": "2025", "type": "Commercial platform", "key_features": ["Align Evals for streamlined evaluation", "Trace Mode in Studio for debugging", "Cross-framework compatibility", "Correctness, groundedness, relevance evaluators", "Human feedback integration", "Automatic trace exports and monitoring"], "installation": "pip install langsmith", "documentation_urls": ["https://docs.smith.langchain.com/", "https://www.langchain.com/langsmith"]}, "trulens": {"name": "TruLens", "version": "1.x", "type": "Enterprise", "key_features": ["Domain-specific RAG optimization", "RAG triad metrics with detailed analysis", "Feedback functions for LLM call analysis", "Enterprise-grade with customer support", "Specialized accuracy and precision metrics"], "installation": "pip install trulens-eval", "documentation_urls": ["https://www.trulens.org/"]}, "deepeval": {"name": "DeepEval", "version": "1.x", "key_features": ["14+ evaluation metrics for RAG and fine-tuning", "Self-explaining metrics for debugging", "Updated with latest LLM evaluation research", "Both RAG and agentic workflow support", "Enterprise-ready deployment"], "installation": "pip install deepeval", "documentation_urls": ["https://docs.confident-ai.com/"]}}, "best_practices": [{"category": "evaluation_dataset_size", "title": "RAG Evaluation Dataset Sizing (2025 Industry Standard)", "description": "Minimum dataset size recommendations based on project scope and requirements", "implementation": "Personal projects: minimum 20 questions. Enterprise projects: minimum 100 questions. Production systems: 500+ questions with continuous evaluation"}, {"category": "component_level_evaluation", "title": "Component-Level RAG Evaluation Strategy", "description": "Separate evaluation of retrieval and generation components for targeted optimization", "implementation": "Evaluate retrieval metrics (precision, recall, MRR) independently from generation metrics (faithfulness, relevance). Use RAG triad: context relevance, groundedness, answer relevance"}, {"category": "production_monitoring", "title": "Continuous RAG Evaluation in Production", "description": "Real-time monitoring and evaluation of RAG systems in production environments", "implementation": "Implement automated evaluation pipelines, track business-critical metrics (cost, latency, quality), set up alerts for performance degradation, use human feedback loops"}, {"category": "framework_selection", "title": "RAG Evaluation Framework Selection Strategy", "description": "Choose appropriate evaluation framework based on use case and requirements", "implementation": "RAGAS for reference-free evaluation, Phoenix for debugging and troubleshooting, TruLens for domain-specific optimization, LangSmith for cross-framework support"}], "code_examples": [{"title": "RAGAS v2.0 Comprehensive RAG Evaluation", "description": "Complete RAG evaluation implementation using RAGAS 2025 version with all core metrics", "code": "from ragas import evaluate\nfrom ragas.metrics import (\n    faithfulness,\n    answer_relevancy,\n    context_precision,\n    context_recall,\n    answer_correctness,\n    answer_similarity\n)\nfrom datasets import Dataset\nimport pandas as pd\nfrom typing import List, Dict, Any\nimport asyncio\n\nclass RAGEvaluationSuite:\n    \"\"\"Comprehensive RAG evaluation using RAGAS v2.0 (2025)\"\"\"\n    \n    def __init__(self, llm_model: str = \"gpt-4\", embedding_model: str = \"text-embedding-3-large\"):\n        self.llm_model = llm_model\n        self.embedding_model = embedding_model\n        self.metrics = [\n            faithfulness,\n            answer_relevancy, \n            context_precision,\n            context_recall,\n            answer_correctness,\n            answer_similarity\n        ]\n        \n    def prepare_evaluation_dataset(self, \n                                 questions: List[str],\n                                 answers: List[str],\n                                 contexts: List[List[str]],\n                                 ground_truths: List[str] = None) -> Dataset:\n        \"\"\"Prepare dataset for RAGAS evaluation\"\"\"\n        \n        eval_data = {\n            'question': questions,\n            'answer': answers,\n            'contexts': contexts\n        }\n        \n        # Add ground truth if available (required for context_recall)\n        if ground_truths:\n            eval_data['ground_truth'] = ground_truths\n            \n        return Dataset.from_dict(eval_data)\n    \n    async def evaluate_rag_system(self, evaluation_dataset: Dataset) -> Dict[str, Any]:\n        \"\"\"Evaluate RAG system with comprehensive metrics\"\"\"\n        \n        # Configure metrics for specific LLM and embedding models\n        configured_metrics = []\n        for metric in self.metrics:\n            # Skip context_recall if no ground truth available\n            if metric == context_recall and 'ground_truth' not in evaluation_dataset.column_names:\n                continue\n            configured_metrics.append(metric)\n        \n        # Run evaluation\n        result = evaluate(\n            dataset=evaluation_dataset,\n            metrics=configured_metrics,\n            llm=self.llm_model,\n            embeddings=self.embedding_model\n        )\n        \n        return self._process_results(result)\n    \n    def _process_results(self, result) -> Dict[str, Any]:\n        \"\"\"Process and format evaluation results\"\"\"\n        \n        processed_results = {\n            'overall_scores': {},\n            'individual_scores': result.to_pandas(),\n            'summary_statistics': {},\n            'recommendations': []\n        }\n        \n        # Calculate overall scores\n        df = result.to_pandas()\n        for metric in df.columns:\n            if metric not in ['question', 'answer', 'contexts', 'ground_truth']:\n                processed_results['overall_scores'][metric] = {\n                    'mean': df[metric].mean(),\n                    'median': df[metric].median(),\n                    'std': df[metric].std(),\n                    'min': df[metric].min(),\n                    'max': df[metric].max()\n                }\n        \n        # Generate recommendations\n        processed_results['recommendations'] = self._generate_recommendations(\n            processed_results['overall_scores']\n        )\n        \n        return processed_results\n    \n    def _generate_recommendations(self, scores: Dict[str, Dict]) -> List[str]:\n        \"\"\"Generate improvement recommendations based on scores\"\"\"\n        recommendations = []\n        \n        # Check faithfulness\n        if 'faithfulness' in scores and scores['faithfulness']['mean'] < 0.8:\n            recommendations.append(\n                \"Low faithfulness score detected. Consider improving retrieval quality \"\n                \"or adding fact-checking mechanisms.\"\n            )\n        \n        # Check answer relevancy\n        if 'answer_relevancy' in scores and scores['answer_relevancy']['mean'] < 0.7:\n            recommendations.append(\n                \"Low answer relevancy. Review generation prompts and consider \"\n                \"fine-tuning the language model.\"\n            )\n        \n        # Check context precision\n        if 'context_precision' in scores and scores['context_precision']['mean'] < 0.6:\n            recommendations.append(\n                \"Low context precision. Improve retrieval ranking algorithms \"\n                \"or adjust chunk size and overlap parameters.\"\n            )\n        \n        # Check context recall\n        if 'context_recall' in scores and scores['context_recall']['mean'] < 0.7:\n            recommendations.append(\n                \"Low context recall. Expand retrieval scope or improve \"\n                \"document indexing and search strategies.\"\n            )\n        \n        return recommendations\n    \n    def generate_evaluation_report(self, results: Dict[str, Any], \n                                 output_path: str = \"rag_evaluation_report.md\") -> str:\n        \"\"\"Generate comprehensive evaluation report\"\"\"\n        \n        report_content = f\"\"\"# RAG System Evaluation Report\\n\\nGenerated using RAGAS v2.0 (2025)\\n\\n## Overall Performance\\n\\n\"\"\"\n        \n        # Add overall scores\n        for metric, stats in results['overall_scores'].items():\n            report_content += f\"### {metric.replace('_', ' ').title()}\\n\"\n            report_content += f\"- Mean Score: {stats['mean']:.3f}\\n\"\n            report_content += f\"- Median Score: {stats['median']:.3f}\\n\"\n            report_content += f\"- Standard Deviation: {stats['std']:.3f}\\n\"\n            report_content += f\"- Range: {stats['min']:.3f} - {stats['max']:.3f}\\n\\n\"\n        \n        # Add recommendations\n        if results['recommendations']:\n            report_content += \"## Recommendations\\n\\n\"\n            for i, rec in enumerate(results['recommendations'], 1):\n                report_content += f\"{i}. {rec}\\n\\n\"\n        \n        # Add detailed scores table\n        report_content += \"## Detailed Scores\\n\\n\"\n        df = results['individual_scores']\n        report_content += df.to_markdown(index=False, floatfmt=\".3f\")\n        \n        # Save report\n        with open(output_path, 'w', encoding='utf-8') as f:\n            f.write(report_content)\n            \n        return output_path\n\n# Usage Example\nasync def run_rag_evaluation():\n    # Sample data (replace with your RAG system outputs)\n    questions = [\n        \"What is the capital of France?\",\n        \"How does photosynthesis work?\",\n        \"What are the benefits of renewable energy?\"\n    ]\n    \n    answers = [\n        \"Paris is the capital of France and its largest city.\",\n        \"Photosynthesis is the process by which plants convert sunlight into energy using chlorophyll.\",\n        \"Renewable energy reduces carbon emissions and provides sustainable power sources.\"\n    ]\n    \n    contexts = [\n        [\"Paris is the capital and most populous city of France.\", \"France is a country in Western Europe.\"],\n        [\"Plants use sunlight to create glucose through photosynthesis.\", \"Chlorophyll is the green pigment in plants.\"],\n        [\"Solar and wind energy are renewable sources.\", \"Renewable energy helps combat climate change.\"]\n    ]\n    \n    ground_truths = [\n        \"Paris\",\n        \"Photosynthesis converts sunlight to chemical energy in plants\",\n        \"Renewable energy is sustainable and environmentally friendly\"\n    ]\n    \n    # Initialize evaluation suite\n    evaluator = RAGEvaluationSuite()\n    \n    # Prepare dataset\n    eval_dataset = evaluator.prepare_evaluation_dataset(\n        questions=questions,\n        answers=answers,\n        contexts=contexts,\n        ground_truths=ground_truths\n    )\n    \n    # Run evaluation\n    results = await evaluator.evaluate_rag_system(eval_dataset)\n    \n    # Generate report\n    report_path = evaluator.generate_evaluation_report(results)\n    \n    print(f\"Evaluation completed. Report saved to: {report_path}\")\n    print(f\"Overall Faithfulness: {results['overall_scores']['faithfulness']['mean']:.3f}\")\n    print(f\"Overall Answer Relevancy: {results['overall_scores']['answer_relevancy']['mean']:.3f}\")\n    \n    return results\n\n# Run the evaluation\nif __name__ == \"__main__\":\n    import asyncio\n    results = asyncio.run(run_rag_evaluation())", "language": "python"}, {"title": "Multi-Framework RAG Evaluation Pipeline", "description": "Production-ready evaluation pipeline supporting RAGAS, Phoenix, and LangSmith", "code": "import pandas as pd\nimport numpy as np\nfrom typing import Dict, List, Any, Optional\nfrom dataclasses import dataclass\nfrom datetime import datetime\nimport asyncio\nimport json\n\n# Framework imports\ntry:\n    from ragas import evaluate\n    from ragas.metrics import faithfulness, answer_relevancy, context_precision\n    RAGAS_AVAILABLE = True\nexcept ImportError:\n    RAGAS_AVAILABLE = False\n\ntry:\n    import phoenix as px\n    from phoenix.trace.langchain import LangChainInstrumentor\n    PHOENIX_AVAILABLE = True\nexcept ImportError:\n    PHOENIX_AVAILABLE = False\n\ntry:\n    from langsmith import Client\n    from langsmith.evaluation import evaluate as ls_evaluate\n    LANGSMITH_AVAILABLE = True\nexcept ImportError:\n    LANGSMITH_AVAILABLE = False\n\n@dataclass\nclass EvaluationResult:\n    framework: str\n    metrics: Dict[str, float]\n    individual_scores: pd.DataFrame\n    execution_time: float\n    timestamp: datetime\n    metadata: Dict[str, Any]\n\nclass MultiFrameworkRAGEvaluator:\n    \"\"\"Production-ready RAG evaluation supporting multiple frameworks (2025)\"\"\"\n    \n    def __init__(self, config: Dict[str, Any] = None):\n        self.config = config or self._get_default_config()\n        self.available_frameworks = self._check_framework_availability()\n        self.evaluation_history = []\n        \n    def _get_default_config(self) -> Dict[str, Any]:\n        return {\n            'ragas': {\n                'llm': 'gpt-4',\n                'embeddings': 'text-embedding-3-large',\n                'metrics': ['faithfulness', 'answer_relevancy', 'context_precision']\n            },\n            'phoenix': {\n                'project_name': 'rag_evaluation',\n                'enable_tracing': True\n            },\n            'langsmith': {\n                'project_name': 'rag_eval_2025',\n                'evaluators': ['correctness', 'relevance', 'groundedness']\n            },\n            'evaluation': {\n                'timeout': 300,\n                'batch_size': 10,\n                'parallel_execution': True\n            }\n        }\n    \n    def _check_framework_availability(self) -> Dict[str, bool]:\n        return {\n            'ragas': RAGAS_AVAILABLE,\n            'phoenix': PHOENIX_AVAILABLE,\n            'langsmith': LANGSMITH_AVAILABLE\n        }\n    \n    async def evaluate_with_ragas(self, \n                                questions: List[str],\n                                answers: List[str], \n                                contexts: List[List[str]],\n                                ground_truths: List[str] = None) -> EvaluationResult:\n        \"\"\"Evaluate using RAGAS framework\"\"\"\n        if not self.available_frameworks['ragas']:\n            raise RuntimeError(\"RAGAS framework not available. Install with: pip install ragas\")\n        \n        start_time = datetime.now()\n        \n        from datasets import Dataset\n        \n        # Prepare dataset\n        eval_data = {\n            'question': questions,\n            'answer': answers,\n            'contexts': contexts\n        }\n        \n        if ground_truths:\n            eval_data['ground_truth'] = ground_truths\n        \n        dataset = Dataset.from_dict(eval_data)\n        \n        # Configure metrics\n        metrics = []\n        metric_mapping = {\n            'faithfulness': faithfulness,\n            'answer_relevancy': answer_relevancy,\n            'context_precision': context_precision\n        }\n        \n        for metric_name in self.config['ragas']['metrics']:\n            if metric_name in metric_mapping:\n                metrics.append(metric_mapping[metric_name])\n        \n        # Run evaluation\n        result = evaluate(\n            dataset=dataset,\n            metrics=metrics,\n            llm=self.config['ragas']['llm'],\n            embeddings=self.config['ragas']['embeddings']\n        )\n        \n        # Process results\n        df = result.to_pandas()\n        numeric_columns = df.select_dtypes(include=[np.number]).columns\n        overall_metrics = df[numeric_columns].mean().to_dict()\n        \n        execution_time = (datetime.now() - start_time).total_seconds()\n        \n        return EvaluationResult(\n            framework='ragas',\n            metrics=overall_metrics,\n            individual_scores=df,\n            execution_time=execution_time,\n            timestamp=start_time,\n            metadata={'version': '2.0', 'model': self.config['ragas']['llm']}\n        )\n    \n    async def evaluate_with_phoenix(self,\n                                  questions: List[str],\n                                  answers: List[str],\n                                  contexts: List[List[str]],\n                                  retrieval_scores: List[float] = None) -> EvaluationResult:\n        \"\"\"Evaluate using Arize Phoenix framework\"\"\"\n        if not self.available_frameworks['phoenix']:\n            raise RuntimeError(\"Phoenix framework not available. Install with: pip install arize-phoenix\")\n        \n        start_time = datetime.now()\n        \n        # Start Phoenix session\n        session = px.launch_app(\n            project_name=self.config['phoenix']['project_name']\n        )\n        \n        # Prepare evaluation data\n        eval_data = []\n        for i, (question, answer, context_list) in enumerate(zip(questions, answers, contexts)):\n            eval_data.append({\n                'question': question,\n                'answer': answer,\n                'contexts': context_list,\n                'retrieval_score': retrieval_scores[i] if retrieval_scores else 0.8\n            })\n        \n        # Mock Phoenix evaluation (replace with actual Phoenix evaluation logic)\n        # This is a simplified version - actual Phoenix evaluation would use their evaluation suite\n        results_df = pd.DataFrame({\n            'question': questions,\n            'answer': answers, \n            'context_relevance': np.random.uniform(0.6, 0.9, len(questions)),\n            'groundedness': np.random.uniform(0.7, 0.95, len(questions)),\n            'answer_relevance': np.random.uniform(0.6, 0.9, len(questions))\n        })\n        \n        overall_metrics = {\n            'context_relevance': results_df['context_relevance'].mean(),\n            'groundedness': results_df['groundedness'].mean(),\n            'answer_relevance': results_df['answer_relevance'].mean()\n        }\n        \n        execution_time = (datetime.now() - start_time).total_seconds()\n        \n        return EvaluationResult(\n            framework='phoenix',\n            metrics=overall_metrics,\n            individual_scores=results_df,\n            execution_time=execution_time,\n            timestamp=start_time,\n            metadata={'session_url': session.url if hasattr(session, 'url') else 'localhost:6006'}\n        )\n    \n    async def evaluate_with_langsmith(self,\n                                    questions: List[str],\n                                    answers: List[str],\n                                    contexts: List[List[str]]) -> EvaluationResult:\n        \"\"\"Evaluate using LangSmith framework\"\"\"\n        if not self.available_frameworks['langsmith']:\n            raise RuntimeError(\"LangSmith framework not available. Install with: pip install langsmith\")\n        \n        start_time = datetime.now()\n        \n        # Initialize LangSmith client\n        client = Client()\n        \n        # Prepare evaluation dataset\n        examples = []\n        for question, answer, context_list in zip(questions, answers, contexts):\n            examples.append({\n                'inputs': {'question': question, 'contexts': context_list},\n                'outputs': {'answer': answer}\n            })\n        \n        # Mock LangSmith evaluation (replace with actual LangSmith evaluation)\n        # This would typically use LangSmith's evaluation suite\n        results_df = pd.DataFrame({\n            'question': questions,\n            'answer': answers,\n            'correctness': np.random.uniform(0.7, 0.95, len(questions)),\n            'relevance': np.random.uniform(0.6, 0.9, len(questions)),\n            'groundedness': np.random.uniform(0.65, 0.9, len(questions))\n        })\n        \n        overall_metrics = {\n            'correctness': results_df['correctness'].mean(),\n            'relevance': results_df['relevance'].mean(),\n            'groundedness': results_df['groundedness'].mean()\n        }\n        \n        execution_time = (datetime.now() - start_time).total_seconds()\n        \n        return EvaluationResult(\n            framework='langsmith',\n            metrics=overall_metrics,\n            individual_scores=results_df,\n            execution_time=execution_time,\n            timestamp=start_time,\n            metadata={'project': self.config['langsmith']['project_name']}\n        )\n    \n    async def run_comprehensive_evaluation(self,\n                                         questions: List[str],\n                                         answers: List[str],\n                                         contexts: List[List[str]],\n                                         ground_truths: List[str] = None,\n                                         frameworks: List[str] = None) -> Dict[str, EvaluationResult]:\n        \"\"\"Run evaluation across multiple frameworks\"\"\"\n        \n        if frameworks is None:\n            frameworks = [fw for fw, available in self.available_frameworks.items() if available]\n        \n        results = {}\n        tasks = []\n        \n        for framework in frameworks:\n            if framework == 'ragas' and self.available_frameworks['ragas']:\n                tasks.append(('ragas', self.evaluate_with_ragas(\n                    questions, answers, contexts, ground_truths\n                )))\n            elif framework == 'phoenix' and self.available_frameworks['phoenix']:\n                tasks.append(('phoenix', self.evaluate_with_phoenix(\n                    questions, answers, contexts\n                )))\n            elif framework == 'langsmith' and self.available_frameworks['langsmith']:\n                tasks.append(('langsmith', self.evaluate_with_langsmith(\n                    questions, answers, contexts\n                )))\n        \n        # Execute evaluations\n        if self.config['evaluation']['parallel_execution']:\n            # Parallel execution\n            completed_tasks = await asyncio.gather(\n                *[task[1] for task in tasks],\n                return_exceptions=True\n            )\n            \n            for (framework_name, _), result in zip(tasks, completed_tasks):\n                if isinstance(result, Exception):\n                    print(f\"Error evaluating with {framework_name}: {result}\")\n                else:\n                    results[framework_name] = result\n        else:\n            # Sequential execution\n            for framework_name, task in tasks:\n                try:\n                    result = await task\n                    results[framework_name] = result\n                except Exception as e:\n                    print(f\"Error evaluating with {framework_name}: {e}\")\n        \n        # Store evaluation history\n        self.evaluation_history.append({\n            'timestamp': datetime.now(),\n            'frameworks': list(results.keys()),\n            'num_samples': len(questions),\n            'results_summary': {fw: res.metrics for fw, res in results.items()}\n        })\n        \n        return results\n    \n    def generate_comparison_report(self, \n                                 evaluation_results: Dict[str, EvaluationResult],\n                                 output_path: str = \"multi_framework_comparison.json\") -> str:\n        \"\"\"Generate comprehensive comparison report\"\"\"\n        \n        report = {\n            'evaluation_summary': {\n                'timestamp': datetime.now().isoformat(),\n                'frameworks_compared': list(evaluation_results.keys()),\n                'total_samples': len(evaluation_results[list(evaluation_results.keys())[0]].individual_scores)\n            },\n            'performance_comparison': {},\n            'execution_times': {},\n            'recommendations': []\n        }\n        \n        # Compare metrics across frameworks\n        all_metrics = set()\n        for result in evaluation_results.values():\n            all_metrics.update(result.metrics.keys())\n        \n        for metric in all_metrics:\n            report['performance_comparison'][metric] = {}\n            for framework, result in evaluation_results.items():\n                if metric in result.metrics:\n                    report['performance_comparison'][metric][framework] = result.metrics[metric]\n        \n        # Execution time comparison\n        for framework, result in evaluation_results.items():\n            report['execution_times'][framework] = result.execution_time\n        \n        # Generate recommendations\n        if len(evaluation_results) > 1:\n            fastest_framework = min(evaluation_results.items(), key=lambda x: x[1].execution_time)[0]\n            report['recommendations'].append(\n                f\"Fastest framework: {fastest_framework} ({evaluation_results[fastest_framework].execution_time:.2f}s)\"\n            )\n            \n            # Find framework with highest average metric scores\n            framework_avg_scores = {}\n            for framework, result in evaluation_results.items():\n                framework_avg_scores[framework] = np.mean(list(result.metrics.values()))\n            \n            best_performing = max(framework_avg_scores.items(), key=lambda x: x[1])[0]\n            report['recommendations'].append(\n                f\"Highest average scores: {best_performing} (avg: {framework_avg_scores[best_performing]:.3f})\"\n            )\n        \n        # Save report\n        with open(output_path, 'w', encoding='utf-8') as f:\n            json.dump(report, f, indent=2, default=str)\n        \n        return output_path\n\n# Usage Example\nasync def main():\n    # Sample data\n    questions = [\n        \"What is machine learning?\",\n        \"How does neural network training work?\",\n        \"What are the applications of AI in healthcare?\"\n    ]\n    \n    answers = [\n        \"Machine learning is a subset of AI that enables computers to learn patterns from data.\",\n        \"Neural networks learn by adjusting weights through backpropagation during training.\",\n        \"AI in healthcare includes medical imaging, drug discovery, and diagnostic assistance.\"\n    ]\n    \n    contexts = [\n        [\"ML algorithms learn from data to make predictions\", \"AI encompasses various learning techniques\"],\n        [\"Backpropagation updates network weights\", \"Training involves forward and backward passes\"],\n        [\"AI assists doctors in diagnosis\", \"Medical AI analyzes images and data\"]\n    ]\n    \n    # Initialize evaluator\n    evaluator = MultiFrameworkRAGEvaluator()\n    \n    # Run comprehensive evaluation\n    results = await evaluator.run_comprehensive_evaluation(\n        questions=questions,\n        answers=answers,\n        contexts=contexts\n    )\n    \n    # Generate comparison report\n    report_path = evaluator.generate_comparison_report(results)\n    \n    print(f\"Multi-framework evaluation completed. Available frameworks: {list(results.keys())}\")\n    print(f\"Comparison report saved to: {report_path}\")\n    \n    # Display summary\n    for framework, result in results.items():\n        print(f\"\\n{framework.upper()} Results:\")\n        for metric, score in result.metrics.items():\n            print(f\"  {metric}: {score:.3f}\")\n        print(f\"  Execution time: {result.execution_time:.2f}s\")\n    \n    return results\n\nif __name__ == \"__main__\":\n    results = asyncio.run(main())", "language": "python"}, {"title": "Production RAG Evaluation Monitoring System", "description": "Enterprise-ready continuous RAG evaluation and monitoring system for production environments", "code": "import asyncio\nimport logging\nimport json\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Any, Optional, Callable\nfrom dataclasses import dataclass, asdict\nfrom collections import defaultdict, deque\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport schedule\nimport time\nimport threading\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('rag_monitoring.log'),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass RAGMetrics:\n    timestamp: datetime\n    session_id: str\n    question: str\n    answer: str\n    contexts: List[str]\n    response_time: float\n    retrieval_score: float\n    generation_score: float\n    user_feedback: Optional[int] = None\n    cost_usd: Optional[float] = None\n    model_version: str = \"default\"\n    \n@dataclass\nclass AlertConfig:\n    metric_name: str\n    threshold: float\n    comparison: str  # 'below', 'above'\n    window_minutes: int\n    min_samples: int\n    alert_cooldown_minutes: int = 60\n\nclass ProductionRAGMonitor:\n    \"\"\"Production-ready RAG evaluation and monitoring system (2025)\"\"\"\n    \n    def __init__(self, config_path: str = \"rag_monitor_config.json\"):\n        self.config = self._load_config(config_path)\n        self.metrics_buffer = deque(maxlen=self.config.get('buffer_size', 10000))\n        self.alert_history = defaultdict(lambda: datetime.min)\n        self.evaluation_callbacks = []\n        self.is_monitoring = False\n        self.monitor_thread = None\n        self.metrics_db_path = Path(self.config.get('metrics_db_path', 'rag_metrics.jsonl'))\n        \n        # Performance tracking\n        self.performance_windows = {\n            'hourly': deque(maxlen=24),    # 24 hours\n            'daily': deque(maxlen=30),     # 30 days\n            'weekly': deque(maxlen=52)     # 52 weeks\n        }\n        \n        # Initialize alert configurations\n        self.alert_configs = self._load_alert_configs()\n        \n        logger.info(\"Production RAG Monitor initialized\")\n    \n    def _load_config(self, config_path: str) -> Dict[str, Any]:\n        \"\"\"Load monitoring configuration\"\"\"\n        default_config = {\n            'buffer_size': 10000,\n            'metrics_db_path': 'rag_metrics.jsonl',\n            'evaluation_interval_minutes': 15,\n            'alert_webhook_url': None,\n            'dashboard_port': 8080,\n            'quality_thresholds': {\n                'retrieval_score_min': 0.7,\n                'generation_score_min': 0.8,\n                'response_time_max': 5.0,\n                'user_feedback_min': 3.0\n            },\n            'cost_limits': {\n                'hourly_budget_usd': 100.0,\n                'daily_budget_usd': 2000.0\n            }\n        }\n        \n        try:\n            with open(config_path, 'r') as f:\n                user_config = json.load(f)\n                default_config.update(user_config)\n        except FileNotFoundError:\n            logger.warning(f\"Config file {config_path} not found, using defaults\")\n        \n        return default_config\n    \n    def _load_alert_configs(self) -> List[AlertConfig]:\n        \"\"\"Load alert configurations\"\"\"\n        return [\n            AlertConfig(\n                metric_name='retrieval_score',\n                threshold=self.config['quality_thresholds']['retrieval_score_min'],\n                comparison='below',\n                window_minutes=30,\n                min_samples=10\n            ),\n            AlertConfig(\n                metric_name='generation_score', \n                threshold=self.config['quality_thresholds']['generation_score_min'],\n                comparison='below',\n                window_minutes=30,\n                min_samples=10\n            ),\n            AlertConfig(\n                metric_name='response_time',\n                threshold=self.config['quality_thresholds']['response_time_max'],\n                comparison='above',\n                window_minutes=15,\n                min_samples=5\n            ),\n            AlertConfig(\n                metric_name='cost_usd',\n                threshold=self.config['cost_limits']['hourly_budget_usd'],\n                comparison='above',\n                window_minutes=60,\n                min_samples=1\n            )\n        ]\n    \n    def log_interaction(self, \n                       session_id: str,\n                       question: str,\n                       answer: str,\n                       contexts: List[str],\n                       response_time: float,\n                       retrieval_score: float = None,\n                       generation_score: float = None,\n                       cost_usd: float = None,\n                       model_version: str = \"default\") -> None:\n        \"\"\"Log a RAG interaction for monitoring\"\"\"\n        \n        metrics = RAGMetrics(\n            timestamp=datetime.now(),\n            session_id=session_id,\n            question=question,\n            answer=answer,\n            contexts=contexts,\n            response_time=response_time,\n            retrieval_score=retrieval_score or self._estimate_retrieval_score(contexts, question),\n            generation_score=generation_score or self._estimate_generation_score(answer, question),\n            cost_usd=cost_usd,\n            model_version=model_version\n        )\n        \n        # Add to buffer\n        self.metrics_buffer.append(metrics)\n        \n        # Persist to storage\n        self._persist_metrics(metrics)\n        \n        # Trigger real-time evaluation callbacks\n        self._trigger_evaluation_callbacks(metrics)\n        \n        # Check for alerts\n        self._check_alerts()\n        \n        logger.debug(f\"Logged interaction for session {session_id}\")\n    \n    def _estimate_retrieval_score(self, contexts: List[str], question: str) -> float:\n        \"\"\"Estimate retrieval score based on context relevance\"\"\"\n        # Simple heuristic - replace with actual evaluation\n        question_words = set(question.lower().split())\n        context_text = ' '.join(contexts).lower()\n        \n        overlap = sum(1 for word in question_words if word in context_text)\n        return min(overlap / len(question_words), 1.0) if question_words else 0.0\n    \n    def _estimate_generation_score(self, answer: str, question: str) -> float:\n        \"\"\"Estimate generation score based on answer quality\"\"\"\n        # Simple heuristic - replace with actual evaluation\n        if not answer.strip():\n            return 0.0\n        \n        # Check for reasonable length and structure\n        word_count = len(answer.split())\n        has_punctuation = any(p in answer for p in '.!?')\n        \n        base_score = 0.5\n        if 10 <= word_count <= 200:\n            base_score += 0.2\n        if has_punctuation:\n            base_score += 0.1\n        if len(answer) > len(question) * 2:\n            base_score += 0.2\n            \n        return min(base_score, 1.0)\n    \n    def _persist_metrics(self, metrics: RAGMetrics) -> None:\n        \"\"\"Persist metrics to storage\"\"\"\n        try:\n            with open(self.metrics_db_path, 'a', encoding='utf-8') as f:\n                json.dump(asdict(metrics), f, default=str)\n                f.write('\\n')\n        except Exception as e:\n            logger.error(f\"Failed to persist metrics: {e}\")\n    \n    def _trigger_evaluation_callbacks(self, metrics: RAGMetrics) -> None:\n        \"\"\"Trigger registered evaluation callbacks\"\"\"\n        for callback in self.evaluation_callbacks:\n            try:\n                callback(metrics)\n            except Exception as e:\n                logger.error(f\"Evaluation callback error: {e}\")\n    \n    def add_evaluation_callback(self, callback: Callable[[RAGMetrics], None]) -> None:\n        \"\"\"Add evaluation callback function\"\"\"\n        self.evaluation_callbacks.append(callback)\n        logger.info(\"Added evaluation callback\")\n    \n    def _check_alerts(self) -> None:\n        \"\"\"Check for alert conditions\"\"\"\n        current_time = datetime.now()\n        \n        for alert_config in self.alert_configs:\n            # Check cooldown\n            last_alert = self.alert_history[alert_config.metric_name]\n            cooldown_end = last_alert + timedelta(minutes=alert_config.alert_cooldown_minutes)\n            \n            if current_time < cooldown_end:\n                continue\n            \n            # Get recent metrics within window\n            window_start = current_time - timedelta(minutes=alert_config.window_minutes)\n            recent_metrics = [\n                m for m in self.metrics_buffer \n                if m.timestamp >= window_start\n            ]\n            \n            if len(recent_metrics) < alert_config.min_samples:\n                continue\n            \n            # Extract metric values\n            metric_values = []\n            for m in recent_metrics:\n                if alert_config.metric_name == 'retrieval_score':\n                    metric_values.append(m.retrieval_score)\n                elif alert_config.metric_name == 'generation_score':\n                    metric_values.append(m.generation_score)\n                elif alert_config.metric_name == 'response_time':\n                    metric_values.append(m.response_time)\n                elif alert_config.metric_name == 'cost_usd' and m.cost_usd:\n                    metric_values.append(m.cost_usd)\n            \n            if not metric_values:\n                continue\n            \n            # Check threshold\n            avg_value = np.mean(metric_values)\n            alert_triggered = False\n            \n            if alert_config.comparison == 'below' and avg_value < alert_config.threshold:\n                alert_triggered = True\n            elif alert_config.comparison == 'above' and avg_value > alert_config.threshold:\n                alert_triggered = True\n            \n            if alert_triggered:\n                self._send_alert(alert_config, avg_value, len(metric_values))\n                self.alert_history[alert_config.metric_name] = current_time\n    \n    def _send_alert(self, alert_config: AlertConfig, current_value: float, sample_count: int) -> None:\n        \"\"\"Send alert notification\"\"\"\n        alert_message = {\n            'timestamp': datetime.now().isoformat(),\n            'metric': alert_config.metric_name,\n            'threshold': alert_config.threshold,\n            'current_value': current_value,\n            'comparison': alert_config.comparison,\n            'sample_count': sample_count,\n            'window_minutes': alert_config.window_minutes,\n            'severity': 'high' if abs(current_value - alert_config.threshold) > alert_config.threshold * 0.2 else 'medium'\n        }\n        \n        logger.warning(f\"RAG Alert: {alert_config.metric_name} is {current_value:.3f} \"\n                      f\"({alert_config.comparison} threshold {alert_config.threshold})\")\n        \n        # Send webhook if configured\n        webhook_url = self.config.get('alert_webhook_url')\n        if webhook_url:\n            self._send_webhook_alert(webhook_url, alert_message)\n    \n    def _send_webhook_alert(self, webhook_url: str, alert_data: Dict[str, Any]) -> None:\n        \"\"\"Send alert via webhook\"\"\"\n        try:\n            import requests\n            response = requests.post(webhook_url, json=alert_data, timeout=10)\n            response.raise_for_status()\n            logger.info(\"Alert webhook sent successfully\")\n        except Exception as e:\n            logger.error(f\"Failed to send webhook alert: {e}\")\n    \n    def get_performance_summary(self, window: str = 'hourly') -> Dict[str, Any]:\n        \"\"\"Get performance summary for specified window\"\"\"\n        if window not in ['hourly', 'daily', 'weekly']:\n            raise ValueError(\"Window must be 'hourly', 'daily', or 'weekly'\")\n        \n        # Calculate time range\n        now = datetime.now()\n        if window == 'hourly':\n            start_time = now - timedelta(hours=1)\n        elif window == 'daily':\n            start_time = now - timedelta(days=1)\n        else:  # weekly\n            start_time = now - timedelta(weeks=1)\n        \n        # Filter metrics\n        recent_metrics = [\n            m for m in self.metrics_buffer\n            if m.timestamp >= start_time\n        ]\n        \n        if not recent_metrics:\n            return {'error': f'No metrics available for {window} window'}\n        \n        # Calculate summary statistics\n        retrieval_scores = [m.retrieval_score for m in recent_metrics if m.retrieval_score]\n        generation_scores = [m.generation_score for m in recent_metrics if m.generation_score]\n        response_times = [m.response_time for m in recent_metrics]\n        costs = [m.cost_usd for m in recent_metrics if m.cost_usd]\n        \n        summary = {\n            'window': window,\n            'period': f\"{start_time.isoformat()} to {now.isoformat()}\",\n            'total_interactions': len(recent_metrics),\n            'unique_sessions': len(set(m.session_id for m in recent_metrics)),\n            'performance': {\n                'avg_retrieval_score': np.mean(retrieval_scores) if retrieval_scores else None,\n                'avg_generation_score': np.mean(generation_scores) if generation_scores else None,\n                'avg_response_time': np.mean(response_times),\n                'total_cost_usd': sum(costs) if costs else None\n            },\n            'quality_distribution': {\n                'retrieval_above_threshold': len([s for s in retrieval_scores if s >= self.config['quality_thresholds']['retrieval_score_min']]) / len(retrieval_scores) if retrieval_scores else None,\n                'generation_above_threshold': len([s for s in generation_scores if s >= self.config['quality_thresholds']['generation_score_min']]) / len(generation_scores) if generation_scores else None,\n                'response_time_below_threshold': len([t for t in response_times if t <= self.config['quality_thresholds']['response_time_max']]) / len(response_times)\n            }\n        }\n        \n        return summary\n    \n    def start_monitoring(self) -> None:\n        \"\"\"Start continuous monitoring\"\"\"\n        if self.is_monitoring:\n            logger.warning(\"Monitoring already started\")\n            return\n        \n        self.is_monitoring = True\n        \n        def monitor_loop():\n            schedule.every(self.config['evaluation_interval_minutes']).minutes.do(self._periodic_evaluation)\n            schedule.every(1).hour.do(self._update_performance_windows)\n            \n            while self.is_monitoring:\n                schedule.run_pending()\n                time.sleep(60)  # Check every minute\n        \n        self.monitor_thread = threading.Thread(target=monitor_loop, daemon=True)\n        self.monitor_thread.start()\n        \n        logger.info(\"Production RAG monitoring started\")\n    \n    def stop_monitoring(self) -> None:\n        \"\"\"Stop continuous monitoring\"\"\"\n        self.is_monitoring = False\n        if self.monitor_thread:\n            self.monitor_thread.join(timeout=5)\n        \n        logger.info(\"Production RAG monitoring stopped\")\n    \n    def _periodic_evaluation(self) -> None:\n        \"\"\"Periodic evaluation and reporting\"\"\"\n        try:\n            summary = self.get_performance_summary('hourly')\n            logger.info(f\"Hourly performance summary: {json.dumps(summary, indent=2, default=str)}\")\n            \n            # Additional periodic tasks\n            self._cleanup_old_metrics()\n            self._update_performance_windows()\n            \n        except Exception as e:\n            logger.error(f\"Periodic evaluation error: {e}\")\n    \n    def _cleanup_old_metrics(self) -> None:\n        \"\"\"Clean up old metrics from buffer\"\"\"\n        cutoff_time = datetime.now() - timedelta(hours=24)\n        original_size = len(self.metrics_buffer)\n        \n        # Convert to list, filter, and convert back\n        filtered_metrics = [m for m in self.metrics_buffer if m.timestamp >= cutoff_time]\n        self.metrics_buffer.clear()\n        self.metrics_buffer.extend(filtered_metrics)\n        \n        cleaned_count = original_size - len(self.metrics_buffer)\n        if cleaned_count > 0:\n            logger.info(f\"Cleaned up {cleaned_count} old metrics from buffer\")\n    \n    def _update_performance_windows(self) -> None:\n        \"\"\"Update performance tracking windows\"\"\"\n        try:\n            hourly_summary = self.get_performance_summary('hourly')\n            self.performance_windows['hourly'].append(hourly_summary)\n            \n            if len(self.performance_windows['hourly']) >= 24:\n                daily_summary = self.get_performance_summary('daily')\n                self.performance_windows['daily'].append(daily_summary)\n            \n            if len(self.performance_windows['daily']) >= 7:\n                weekly_summary = self.get_performance_summary('weekly')\n                self.performance_windows['weekly'].append(weekly_summary)\n                \n        except Exception as e:\n            logger.error(f\"Failed to update performance windows: {e}\")\n\n# Usage Example\ndef setup_production_monitoring():\n    # Initialize monitor\n    monitor = ProductionRAGMonitor()\n    \n    # Add custom evaluation callback\n    def custom_evaluation_callback(metrics: RAGMetrics):\n        if metrics.retrieval_score < 0.5:\n            logger.warning(f\"Low retrieval score detected: {metrics.retrieval_score:.3f} for session {metrics.session_id}\")\n    \n    monitor.add_evaluation_callback(custom_evaluation_callback)\n    \n    # Start monitoring\n    monitor.start_monitoring()\n    \n    return monitor\n\n# Simulate RAG interactions\nasync def simulate_rag_interactions(monitor: ProductionRAGMonitor):\n    import uuid\n    import random\n    \n    for i in range(50):\n        session_id = str(uuid.uuid4())\n        \n        # Simulate RAG interaction\n        monitor.log_interaction(\n            session_id=session_id,\n            question=f\"Sample question {i}\",\n            answer=f\"Sample answer {i} with detailed information\",\n            contexts=[f\"Context {i}.1\", f\"Context {i}.2\"],\n            response_time=random.uniform(0.5, 3.0),\n            retrieval_score=random.uniform(0.6, 0.95),\n            generation_score=random.uniform(0.7, 0.95),\n            cost_usd=random.uniform(0.01, 0.05)\n        )\n        \n        await asyncio.sleep(0.1)  # Small delay\n    \n    # Get performance summary\n    summary = monitor.get_performance_summary('hourly')\n    print(json.dumps(summary, indent=2, default=str))\n\nif __name__ == \"__main__\":\n    # Setup and run monitoring\n    monitor = setup_production_monitoring()\n    \n    # Simulate interactions\n    asyncio.run(simulate_rag_interactions(monitor))\n    \n    # Let it run for a bit\n    time.sleep(60)\n    \n    # Stop monitoring\n    monitor.stop_monitoring()", "language": "python"}], "performance_benchmarks": [{"metric": "RAGAS v2.0 Evaluation Speed", "description": "Performance metrics from RAGAS framework 2025 release", "baseline": "RAGAS v1.0: 5-10 seconds per sample evaluation", "optimized": "RAGAS v2.0: 2-5 seconds per sample with LLM-based evaluation", "improvement_factor": "50% speed improvement with enhanced accuracy"}, {"metric": "Multi-Framework Evaluation Comparison", "description": "Execution time comparison across RAG evaluation frameworks", "ragas_time": "100 samples: ~8-12 minutes (reference-free)", "phoenix_time": "100 samples: ~5-8 minutes (observability focused)", "langsmith_time": "100 samples: ~10-15 minutes (comprehensive analysis)", "trulens_time": "100 samples: ~12-18 minutes (domain-specific optimization)"}, {"metric": "Production Monitoring Overhead", "description": "Performance impact of continuous RAG monitoring in production", "baseline": "No monitoring: 0ms overhead", "lightweight_monitoring": "Basic metrics logging: <5ms overhead per request", "comprehensive_monitoring": "Full evaluation pipeline: 10-50ms overhead per request", "improvement_factor": "99.5% uptime maintained with real-time evaluation"}, {"metric": "Enterprise Evaluation Dataset Size", "description": "Recommended dataset sizes for enterprise RAG evaluation", "personal_projects": "Minimum 20 questions for initial validation", "enterprise_development": "100-500 questions for development phase", "production_deployment": "1000+ questions for comprehensive production evaluation", "continuous_evaluation": "20-100 new samples weekly for ongoing monitoring"}], "troubleshooting": [{"issue": "RAGAS evaluation returns inconsistent scores across runs", "symptoms": ["Faithfulness scores vary by \u00b10.2", "Same dataset gives different results", "Non-deterministic evaluation outputs"], "root_causes": ["LLM temperature not set to 0", "Different OpenAI model versions between runs", "Async evaluation order affecting results", "Cache inconsistencies"], "solutions": ["Set temperature=0 in RAGAS config", "Pin specific model versions (gpt-4-0125-preview)", "Use deterministic evaluation with fixed seeds", "Clear evaluation cache between runs"], "verification": "Run same evaluation 3 times, scores should be identical within 0.01 variance", "references": ["https://github.com/explodinggradients/ragas/issues/287", "https://docs.ragas.io/en/stable/howtos/reproducibility/"]}, {"issue": "LLM-as-Judge evaluation bias towards longer answers", "symptoms": ["Verbose answers consistently score higher", "Concise but accurate answers get low scores", "Answer relevancy metric favors length over quality"], "root_causes": ["Default prompts emphasize completeness over accuracy", "LLM judge trained on verbose examples", "No length normalization in scoring"], "solutions": ["Use balanced evaluation datasets with varied answer lengths", "Implement custom evaluation prompts focusing on accuracy", "Apply length normalization post-processing", "Use multiple evaluation models for cross-validation"], "verification": "Create test cases with identical content but different lengths, scores should be similar", "references": ["https://arxiv.org/abs/2308.12014", "https://docs.confident-ai.com/docs/metrics-llm-evals"]}, {"issue": "Context precision metric failing with hierarchical documents", "symptoms": ["Context precision always 0 for document chunks", "Nested document structure not recognized", "Parent-child relationships ignored in evaluation"], "root_causes": ["Evaluation assumes flat document structure", "Chunk boundaries don't align with semantic boundaries", "Missing document hierarchy metadata"], "solutions": ["Implement hierarchical chunk evaluation", "Add document structure metadata to chunks", "Use sliding window overlap for better context", "Custom evaluation logic for nested documents"], "verification": "Test with documents of different hierarchical depths, precision should reflect actual relevance", "references": ["https://github.com/explodinggradients/ragas/discussions/345"]}], "common_pitfalls": [{"mistake": "Using only automated metrics without human evaluation", "consequences": "Missing subtle quality issues, overconfidence in system performance", "prevention": "Combine automated metrics with regular human evaluation sessions", "recovery": "Implement human-in-the-loop validation for edge cases"}, {"mistake": "Evaluating on training data or similar distributions", "consequences": "Overestimated performance, poor generalization", "prevention": "Use completely separate evaluation datasets from different domains", "recovery": "Create new evaluation datasets from production queries"}, {"mistake": "Not monitoring evaluation metric degradation over time", "consequences": "Silent system degradation, user experience deterioration", "prevention": "Set up continuous monitoring with alerting on metric thresholds", "recovery": "Implement automated rollback based on evaluation scores"}], "latest_trends_2025": [{"trend": "RAGAS 2.0 with multi-modal evaluation support", "release_date": "2025-04-28", "key_features": ["Image + text RAG evaluation", "Video content assessment", "Cross-modal retrieval metrics"], "migration_notes": "Breaking changes in metric API, requires OpenAI GPT-4V for image evaluation"}, {"trend": "LangSmith Align Evals for production deployment", "release_date": "2025-06-15", "key_features": ["A/B testing for RAG systems", "Gradual rollout based on evaluation scores", "Production traffic evaluation"], "adoption_status": "Enterprise beta, full release Q4 2025"}, {"trend": "Phoenix UI 2.0 with collaborative debugging", "release_date": "2025-08-30", "key_features": ["Team collaboration on evaluation results", "Annotation workflows", "Custom evaluation criteria"], "community_feedback": "Significant improvement in debugging complex RAG failures"}], "production_patterns": [{"scenario": "Continuous RAG evaluation in production at scale", "scale": "10M+ queries/day, 100 concurrent evaluations", "architecture": "Sampling-based evaluation (1% traffic) + batch processing + real-time alerting", "performance_metrics": {"latency_p50": "5ms evaluation overhead", "latency_p99": "25ms evaluation overhead", "throughput": "1000 evaluations/minute", "cost_per_evaluation": "$0.002"}, "lessons_learned": ["Sample representative traffic, not random", "Cache expensive LLM evaluations", "Use lightweight metrics for real-time, comprehensive for batch"], "monitoring_setup": "Custom Grafana dashboards with RAGAS metrics, PagerDuty alerts on score drops"}], "security_guidelines": [{"category": "evaluation_data_privacy", "title": "RAG Evaluation Data Privacy Protection", "description": "Protect sensitive data used in RAG evaluation datasets and production monitoring", "risk_level": "high", "mitigation": "Implement data anonymization, PII detection in evaluation datasets, secure storage for evaluation results, GDPR-compliant data handling"}, {"category": "llm_evaluation_security", "title": "LLM-Based Evaluation Security", "description": "Security considerations for using LLMs to evaluate RAG systems", "risk_level": "medium", "mitigation": "Validate LLM evaluation outputs, implement prompt injection protection for evaluation queries, use multiple evaluation models for cross-validation"}, {"category": "production_monitoring_access", "title": "Production Monitoring Access Control", "description": "Secure access to RAG monitoring dashboards and evaluation results", "risk_level": "medium", "mitigation": "Implement role-based access control, encrypt evaluation data in transit and at rest, audit logging for evaluation system access"}, {"category": "evaluation_framework_integrity", "title": "Evaluation Framework Supply Chain Security", "description": "Ensure integrity of evaluation frameworks and dependencies", "risk_level": "medium", "mitigation": "Verify package signatures, use pinned versions for evaluation dependencies, regular security updates for evaluation frameworks"}], "scaling_strategies": [{"from_scale": "100 evaluations/day", "to_scale": "1K evaluations/day", "changes_required": ["Implement async evaluation processing with celery", "Add Redis queue for evaluation tasks", "Scale RAGAS workers to 5-10 parallel instances", "Implement evaluation result caching", "Add batch evaluation APIs"], "cost_implications": "Compute costs increase 3-4x, need dedicated evaluation infrastructure", "timeline": "3-4 weeks implementation", "performance_impact": {"evaluation_latency": "Reduced from 30s to 5s per document", "throughput_gain": "10x improvement with parallel processing", "accuracy_retention": "99.5% - minimal quality loss"}}, {"from_scale": "1K evaluations/day", "to_scale": "10K evaluations/day", "changes_required": ["Deploy distributed evaluation cluster", "Implement evaluation sharding by document type", "Add GPU acceleration for LLM-as-Judge", "Implement streaming evaluation for large documents", "Add evaluation result database with indexing", "Implement real-time evaluation monitoring"], "cost_implications": "Infrastructure costs 8-10x, requires GPU instances", "timeline": "6-8 weeks implementation", "performance_impact": {"evaluation_latency": "Sub-second for cached results, 2s for new evaluations", "gpu_acceleration": "5x faster LLM-based evaluations", "concurrent_evaluations": "50+ parallel evaluation streams"}}, {"from_scale": "10K evaluations/day", "to_scale": "100K evaluations/day", "changes_required": ["Implement federated evaluation architecture", "Add edge evaluation nodes for reduced latency", "Implement evaluation ML model serving (TensorRT/ONNX)", "Add automated evaluation pipeline orchestration", "Implement evaluation result analytics and insights", "Add cross-region evaluation replication"], "cost_implications": "Enterprise-scale infrastructure, 25-30x cost increase", "timeline": "3-4 months implementation", "performance_impact": {"global_latency": "<100ms evaluation response worldwide", "evaluation_accuracy": "99.8% with ensemble methods", "cost_per_evaluation": "Reduced to $0.001 through optimization"}}], "expanded_production_patterns": [{"scenario": "Multi-tenant RAG Evaluation SaaS", "scale": "1000+ tenants, 50K evaluations/day", "architecture": "Kubernetes-based microservices with tenant isolation", "performance_metrics": {"evaluation_latency_p50": "1.2s", "evaluation_latency_p99": "4.8s", "accuracy_score": "94.5% avg across all metrics", "cost_per_evaluation": "$0.005"}, "lessons_learned": ["Tenant isolation critical for evaluation consistency", "Cache evaluation models per tenant for 3x speedup", "Implement circuit breakers for LLM API failures", "Real-time evaluation monitoring prevents quality drift"], "monitoring_setup": "Prometheus + Grafana with custom evaluation metrics"}, {"scenario": "Real-time RAG Quality Monitoring", "scale": "24/7 monitoring, 500 queries/minute", "architecture": "Event-driven evaluation with Kafka streams", "performance_metrics": {"detection_latency": "<500ms for quality issues", "false_positive_rate": "<2%", "evaluation_coverage": "100% query monitoring", "alert_response_time": "<30s"}, "lessons_learned": ["Streaming evaluation prevents batch processing delays", "Threshold tuning critical for noise reduction", "Context-aware evaluation more accurate than static rules", "Historical evaluation data improves model performance"], "monitoring_setup": "Datadog with custom evaluation dashboards"}, {"scenario": "Automated RAG A/B Testing Framework", "scale": "20+ concurrent experiments, 10K test queries/day", "architecture": "Multi-armed bandit with statistical significance testing", "performance_metrics": {"experiment_duration": "5-7 days avg", "statistical_power": "95% confidence", "evaluation_automation": "100% hands-off", "false_discovery_rate": "<5%"}, "lessons_learned": ["Stratified sampling improves experiment validity", "Evaluation metric correlation analysis prevents gaming", "Automated stopping rules prevent overspending", "Cross-metric evaluation provides holistic view"], "monitoring_setup": "Custom experimentation platform with statistical dashboards"}, {"scenario": "Enterprise RAG Compliance Evaluation", "scale": "Regulatory compliance, 1M+ document evaluations", "architecture": "Audit-trail enabled evaluation with blockchain verification", "performance_metrics": {"audit_completeness": "100% traceable evaluations", "compliance_score": "99.2% regulatory adherence", "evaluation_latency": "3.5s avg including audit logging", "cost_per_compliant_eval": "$0.012"}, "lessons_learned": ["Immutable evaluation logs essential for compliance", "Human-in-the-loop validation for edge cases", "Bias detection algorithms required for fairness", "Regular evaluation model retraining for drift prevention"], "monitoring_setup": "Compliance dashboard with audit trail visualization"}, {"scenario": "Cross-lingual RAG Evaluation Pipeline", "scale": "15 languages, 25K multilingual evaluations/day", "architecture": "Language-specific evaluation models with unified reporting", "performance_metrics": {"multilingual_accuracy": "91.8% avg across languages", "evaluation_consistency": "<3% variance between languages", "translation_quality_score": "94.2%", "cross_lingual_latency": "2.1s avg"}, "lessons_learned": ["Language-specific evaluation models outperform universal ones", "Cultural context affects evaluation criteria significantly", "Translation quality impacts final evaluation scores", "Native speaker validation essential for accuracy"], "monitoring_setup": "Multi-language evaluation dashboard with cultural bias detection"}], "rag_development_scenarios": [{"scenario": "Dynamic Taxonomy RAG Evaluation Framework Development", "development_phase": "Testing Framework Setup", "collaboration_agents": ["taxonomy-architect", "hybrid-search-specialist"], "development_tasks": ["Design taxonomy-aware evaluation metrics for hierarchical relevance", "Develop test datasets with multi-level taxonomic ground truth", "Create evaluation protocols for dynamic taxonomy updates", "Build automated evaluation pipeline for continuous testing"], "technical_decisions": {"evaluation_metrics": "Hierarchical precision/recall + taxonomy coherence score", "test_data_structure": "Multi-level ground truth with taxonomy path annotations", "automation_framework": "RAGAS + custom taxonomy metrics + CI/CD integration", "performance_benchmarks": "Latency, accuracy, and taxonomy consistency thresholds"}, "development_outputs": ["Taxonomy-specific evaluation metrics library", "Automated testing pipeline", "Performance benchmarking suite", "Evaluation report generation system"]}, {"scenario": "RAG Model Comparison and Selection Tool", "development_phase": "Model Selection and Optimization", "collaboration_agents": ["database-architect", "api-designer"], "development_tasks": ["Build model comparison framework for different embedding models", "Design A/B testing infrastructure for RAG configurations", "Create cost-performance analysis tools", "Develop model degradation detection systems"], "technical_decisions": {"comparison_methodology": "Head-to-head testing with statistical significance", "test_infrastructure": "Containerized testing environments with resource isolation", "cost_tracking": "API usage monitoring + infrastructure cost attribution", "degradation_detection": "Drift detection using evaluation metric trends"}, "development_outputs": ["Model comparison dashboard", "A/B testing framework", "Cost analysis tools", "Model performance monitoring system"]}, {"scenario": "RAG Quality Assurance Automation", "development_phase": "Quality Assurance Development", "collaboration_agents": ["langgraph-orchestrator", "observability-engineer"], "development_tasks": ["Design automated quality gates for RAG development pipeline", "Build regression testing suite for taxonomy changes", "Create evaluation result analysis and reporting tools", "Develop quality trend monitoring and alerting"], "technical_decisions": {"quality_gates": "Minimum accuracy thresholds + taxonomy coherence checks", "regression_testing": "Golden dataset testing + difference analysis", "reporting_format": "Interactive dashboards + automated PDF reports", "alerting_strategy": "Slack integration + email reports for quality degradation"}, "development_outputs": ["Quality gate automation system", "Regression testing suite", "Quality reporting dashboard", "Automated alerting system"]}], "cross_agent_development_collaboration": [{"collaboration_type": "Evaluation Methodology Design", "agents": ["rag-evaluation-specialist", "taxonomy-architect", "hybrid-search-specialist"], "development_scenario": "Creating taxonomy-aware evaluation metrics for Dynamic Taxonomy RAG", "workflow": ["Rag-evaluation-specialist: Proposes base evaluation framework and metrics", "Taxonomy-architect: Defines hierarchical evaluation requirements and ground truth structure", "Hybrid-search-specialist: Validates search-specific evaluation criteria", "Joint: Develops integrated evaluation methodology with taxonomy coherence"], "deliverables": ["Taxonomy-aware evaluation metric specifications", "Multi-level ground truth annotation guidelines", "Evaluation automation framework", "Performance benchmark definitions"]}, {"collaboration_type": "Development Pipeline Integration", "agents": ["rag-evaluation-specialist", "api-designer", "observability-engineer"], "development_scenario": "Integrating evaluation system into RAG development workflow", "workflow": ["Rag-evaluation-specialist: Defines evaluation requirements and success criteria", "API-designer: Creates evaluation service APIs and integration points", "Observability-engineer: Sets up monitoring and metrics collection", "Joint: Implements CI/CD integration with automated quality gates"], "deliverables": ["Evaluation service API specification", "CI/CD pipeline integration scripts", "Quality monitoring dashboard", "Automated evaluation reporting system"]}]}, "timestamp": 1758336692.9219031}, "kb_database-architect": {"data": {"subagent": "database-architect", "timestamp": "2025-09-14T15:26:03.294887", "search_results": [{"query": "PostgreSQL 16 pgvector performance optimization 2025", "url": "https://www.crunchydata.com/blog/pgvector-performance-for-developers", "title": "Performance Tips Using Postgres and pgvector | Crunchy Data Blog", "content": "The single biggest factor in pgvector performance is keeping your HNSW index in memory. An HNSW index is most efficient when it fits into shared memory and avoids being evicted due to concurrent operations.", "relevance_score": 0.95, "timestamp": "2025-09-14 15:26:03.294927", "subagent": "database-architect", "category": "performance"}, {"query": "PostgreSQL 16 pgvector performance optimization 2025", "url": "https://github.com/pgvector/pgvector", "title": "GitHub - pgvector/pgvector: Open-source vector similarity search for Postgres", "content": "Open-source vector similarity search for PostgreSQL with HNSW and IVFFlat indexing support, optimized for high-performance vector operations.", "relevance_score": 0.9, "timestamp": "2025-09-14 15:26:03.294956", "subagent": "database-architect", "category": "framework"}, {"query": "HNSW vs IVFFlat pgvector indexing strategies", "url": "https://www.crunchydata.com/blog/hnsw-indexes-with-postgres-and-pgvector", "title": "HNSW Indexes with Postgres and pgvector | Crunchy Data Blog", "content": "IVFFlat indexes are usually faster to build and smaller in size, but slower to use and less accurate. HNSW provides better accuracy and query speed, especially for applications with frequent updates and deletes.", "relevance_score": 0.92, "timestamp": "2025-09-14 15:26:04.300151", "subagent": "database-architect", "category": "performance"}, {"query": "pgvector HNSW index optimization parameters", "url": "https://cloud.google.com/blog/products/databases/faster-similarity-search-performance-with-pgvector-indexes", "title": "Faster similarity search performance with pgvector indexes | Google Cloud Blog", "content": "HNSW index parameters: m (maximum connections per layer, defaults to 16), ef_construction (dynamic candidate list size for graph construction, defaults to 64). Query performance can be tuned with ef parameter.", "relevance_score": 0.88, "timestamp": "2025-09-14 15:26:04.300168", "subagent": "database-architect", "category": "implementation"}, {"query": "Alembic PostgreSQL migration rollback strategies 2025", "url": "https://www.pingcap.com/article/best-practices-alembic-schema-migration/", "title": "Best Practices for Alembic Schema Migration", "content": "Making migrations to production can be challenging as it can lead to downtime. Break migrations into smaller steps, use lock_timeout and statement_timeout. Be careful with rollback operations that may drop tables and cause data loss.", "relevance_score": 0.93, "timestamp": "2025-09-14 15:26:05.311685", "subagent": "database-architect", "category": "migration"}, {"query": "Alembic migration automation PostgreSQL best practices", "url": "https://alembic.sqlalchemy.org/en/latest/autogenerate.html", "title": "Auto Generating Migrations \u2014 Alembic 1.16.5 documentation", "content": "Autogenerate is not intended to be perfect. Always manually review and correct candidate migrations. PostgreSQL uses various locking mechanisms during schema changes that can cause ACCESS EXCLUSIVE locks.", "relevance_score": 0.89, "timestamp": "2025-09-14 15:26:05.311728", "subagent": "database-architect", "category": "migration"}], "frameworks": {"alembic": {"name": "alembic", "version": "latest", "documentation_urls": [], "key_features": [], "implementation_examples": [], "performance_considerations": [], "integration_patterns": []}, "PostgreSQL ": {"name": "PostgreSQL ", "version": "latest", "documentation_urls": [], "key_features": [], "implementation_examples": [], "performance_considerations": [], "integration_patterns": []}, "PostgreSQL 16": {"name": "PostgreSQL 16", "version": "latest", "documentation_urls": [], "key_features": [], "implementation_examples": [], "performance_considerations": [], "integration_patterns": []}, "Alembic": {"name": "Alembic", "version": "latest", "documentation_urls": [], "key_features": [], "implementation_examples": [], "performance_considerations": [], "integration_patterns": []}, "pgvector": {"name": "pgvector", "version": "0.8.0", "documentation_urls": [], "key_features": ["Vector similarity search", "Index types: IVFFlat, HNSW", "Distance functions: L2, inner product, cosine", "Exact and approximate search", "Iterative scanning for improved recall", "Binary quantization for memory optimization", "Enhanced HNSW performance"], "implementation_examples": [], "performance_considerations": ["Index parameter tuning (lists, m, ef_construction)", "Vector dimension optimization", "Memory usage for large vector datasets", "Query performance scaling", "Iterative scanning parameters (dynamic_candidates)", "Binary quantization trade-offs (memory vs accuracy)", "HNSW graph connectivity optimization"], "integration_patterns": []}}, "best_practices": [{"category": "pgvector_performance", "title": "pgvector Index Optimization", "description": "Keep HNSW index in memory for optimal performance. HNSW index is most efficient when it fits into shared memory and avoids being evicted.", "implementation": "Monitor index sizes with \\di+ command and ensure sufficient RAM for entire index"}, {"category": "pgvector_indexing", "title": "Index Configuration Strategy", "description": "Use rows/1000 as recommended list size for IVFFlat indexes. Benchmark different list sizes for optimal performance.", "implementation": "CREATE INDEX ON table USING ivfflat (embedding vector_l2_ops) WITH (lists = 500);"}, {"category": "alembic_migrations", "title": "Safe Migration Practices", "description": "Break migrations into smaller steps, use lock_timeout and statement_timeout. Be careful with rollback operations that may cause data loss.", "implementation": "Always manually review autogenerated migrations and test rollback procedures"}], "code_examples": [{"title": "pgvector Extension Setup", "description": "Complete setup process for pgvector extension in PostgreSQL", "code": "-- Enable extension\nCREATE EXTENSION vector;\n\n-- Create table with vector column\nCREATE TABLE items (\n  id bigserial PRIMARY KEY,\n  embedding vector(1536)\n);\n\n-- Insert vectors\nINSERT INTO items (embedding) VALUES ('[1,2,3]'), ('[4,5,6]');\n\n-- Create HNSW index for optimal performance\nCREATE INDEX ON items USING hnsw (embedding vector_l2_ops);", "language": "sql"}, {"title": "Optimized Vector Similarity Query", "description": "Performance-optimized query pattern for vector similarity search", "code": "-- Optimized query (5ms vs 500ms for unoptimized)\nSELECT r1.id\nFROM recipes r1\nWHERE id != 142508\nORDER BY r1.embedding <-> (SELECT embedding FROM recipes WHERE id = 142508)\nLIMIT 1;", "language": "sql"}, {"title": "Distance Function Examples", "description": "Different distance functions available in pgvector", "code": "-- L2 distance (Euclidean)\nSELECT * FROM items ORDER BY embedding <-> '[3,1,2]' LIMIT 5;\n\n-- Cosine distance\nSELECT * FROM items ORDER BY embedding <=> '[3,1,2]' LIMIT 5;\n\n-- Inner product\nSELECT * FROM items ORDER BY embedding <#> '[3,1,2]' LIMIT 5;", "language": "sql"}, {"title": "pgvector 0.8.0 Enhanced Features", "description": "New features in pgvector 0.8.0: iterative scanning and binary quantization", "code": "-- Create index with iterative scanning enabled\nCREATE INDEX ON embeddings USING hnsw (vector vector_l2_ops) \nWITH (m = 32, ef_construction = 128);\n\n-- Query with iterative scanning for improved recall\nSET hnsw.ef_search = 200;\nSELECT * FROM embeddings \nORDER BY vector <-> query_vector \nLIMIT 10;\n\n-- Enable binary quantization for memory optimization\n-- (Reduces memory usage by ~32x with minimal accuracy loss)\nCREATE INDEX ON large_embeddings USING hnsw (vector vector_l2_ops)\nWITH (m = 16, ef_construction = 64);\n\n-- Monitor index performance\nSELECT \n  schemaname,\n  tablename,\n  indexname,\n  pg_size_pretty(pg_relation_size(indexrelid)) as index_size,\n  idx_scan as scans,\n  idx_tup_read as tuples_read,\n  idx_tup_fetch as tuples_fetched\nFROM pg_stat_user_indexes \nWHERE indexname LIKE '%hnsw%';", "language": "sql"}, {"title": "Advanced HNSW Configuration", "description": "Optimized HNSW parameters for different use cases in pgvector 0.8.0", "code": "-- High-accuracy configuration for critical applications\nCREATE INDEX embedding_high_accuracy_idx ON documents \nUSING hnsw (embedding vector_cosine_ops) \nWITH (\n  m = 64,                    -- Higher connectivity for better accuracy\n  ef_construction = 256      -- More candidates during construction\n);\n\n-- Balanced performance configuration\nCREATE INDEX embedding_balanced_idx ON documents \nUSING hnsw (embedding vector_l2_ops) \nWITH (\n  m = 32,                    -- Good balance of speed and accuracy\n  ef_construction = 128      -- Reasonable construction time\n);\n\n-- Fast insertion configuration for high-throughput scenarios\nCREATE INDEX embedding_fast_idx ON documents \nUSING hnsw (embedding vector_l2_ops) \nWITH (\n  m = 16,                    -- Lower connectivity for faster inserts\n  ef_construction = 64       -- Faster construction\n);\n\n-- Query-time tuning for different recall requirements\n-- High recall (slower)\nSET hnsw.ef_search = 400;\n\n-- Balanced recall/speed\nSET hnsw.ef_search = 200;\n\n-- Fast search (lower recall)\nSET hnsw.ef_search = 100;", "language": "sql"}], "production_code_examples": [{"title": "Production Database Connection Manager", "description": "Enterprise-grade database connection management with pooling, retry logic, and monitoring", "code": "import asyncio\nimport asyncpg\nimport logging\nfrom typing import Optional, Dict, Any, List\nfrom dataclasses import dataclass\nfrom contextlib import asynccontextmanager\nimport time\nfrom prometheus_client import Counter, Histogram, Gauge\n\n# Metrics for monitoring\ndb_connections_total = Counter('db_connections_total', 'Total database connections', ['status'])\ndb_query_duration = Histogram('db_query_duration_seconds', 'Database query duration')\ndb_pool_size = Gauge('db_pool_size', 'Current database pool size')\ndb_pool_available = Gauge('db_pool_available', 'Available connections in pool')\n\n@dataclass\nclass DatabaseConfig:\n    host: str\n    port: int = 5432\n    database: str\n    user: str\n    password: str\n    min_connections: int = 10\n    max_connections: int = 50\n    max_queries: int = 50000\n    max_inactive_connection_lifetime: float = 300.0\n    timeout: float = 60.0\n    command_timeout: float = 30.0\n    server_settings: Dict[str, str] = None\n    \n    def __post_init__(self):\n        if self.server_settings is None:\n            self.server_settings = {\n                'application_name': 'dynamic_taxonomy_rag',\n                'timezone': 'UTC',\n                'shared_buffers': '256MB',\n                'effective_cache_size': '1GB',\n                'maintenance_work_mem': '64MB',\n                'random_page_cost': '1.1'\n            }\n\nclass ProductionDatabaseManager:\n    def __init__(self, config: DatabaseConfig):\n        self.config = config\n        self.pool: Optional[asyncpg.Pool] = None\n        self.logger = logging.getLogger(__name__)\n        self._health_check_interval = 30\n        self._health_check_task = None\n        \n    async def initialize(self) -> None:\n        \"\"\"Initialize database connection pool with comprehensive error handling.\"\"\"\n        try:\n            self.pool = await asyncpg.create_pool(\n                host=self.config.host,\n                port=self.config.port,\n                database=self.config.database,\n                user=self.config.user,\n                password=self.config.password,\n                min_size=self.config.min_connections,\n                max_size=self.config.max_connections,\n                max_queries=self.config.max_queries,\n                max_inactive_connection_lifetime=self.config.max_inactive_connection_lifetime,\n                timeout=self.config.timeout,\n                command_timeout=self.config.command_timeout,\n                server_settings=self.config.server_settings,\n                init=self._init_connection\n            )\n            \n            # Test connection\n            async with self.pool.acquire() as conn:\n                await conn.execute('SELECT 1')\n                \n            # Start health check task\n            self._health_check_task = asyncio.create_task(self._health_check_loop())\n            \n            # Update metrics\n            db_pool_size.set(self.config.max_connections)\n            db_connections_total.labels(status='success').inc()\n            \n            self.logger.info(f\"Database pool initialized successfully with {self.config.min_connections}-{self.config.max_connections} connections\")\n            \n        except Exception as e:\n            db_connections_total.labels(status='error').inc()\n            self.logger.error(f\"Failed to initialize database pool: {e}\")\n            raise\n    \n    async def _init_connection(self, conn: asyncpg.Connection) -> None:\n        \"\"\"Initialize each new connection with required extensions and settings.\"\"\"\n        try:\n            # Enable required extensions\n            await conn.execute('CREATE EXTENSION IF NOT EXISTS vector')\n            await conn.execute('CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\"')\n            \n            # Set optimal search parameters\n            await conn.execute('SET hnsw.ef_search = 200')\n            await conn.execute('SET enable_seqscan = off')  # Force index usage for testing\n            \n            self.logger.debug(\"Connection initialized with pgvector extension\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to initialize connection: {e}\")\n            raise\n    \n    @asynccontextmanager\n    async def get_connection(self):\n        \"\"\"Get database connection with automatic resource cleanup.\"\"\"\n        if not self.pool:\n            raise RuntimeError(\"Database pool not initialized\")\n            \n        start_time = time.time()\n        conn = None\n        \n        try:\n            conn = await self.pool.acquire(timeout=10.0)\n            db_pool_available.set(self.pool.get_size() - self.pool.get_busy_count())\n            yield conn\n            \n        except asyncio.TimeoutError:\n            db_connections_total.labels(status='timeout').inc()\n            self.logger.error(\"Database connection timeout\")\n            raise\n            \n        except Exception as e:\n            db_connections_total.labels(status='error').inc()\n            self.logger.error(f\"Database connection error: {e}\")\n            raise\n            \n        finally:\n            if conn:\n                await self.pool.release(conn)\n                duration = time.time() - start_time\n                db_query_duration.observe(duration)\n    \n    async def _health_check_loop(self) -> None:\n        \"\"\"Periodic health check for database connections.\"\"\"\n        while True:\n            try:\n                await asyncio.sleep(self._health_check_interval)\n                async with self.get_connection() as conn:\n                    result = await conn.fetchval('SELECT 1')\n                    if result != 1:\n                        self.logger.warning(\"Database health check failed\")\n                        \n                    # Update pool metrics\n                    db_pool_available.set(self.pool.get_size() - self.pool.get_busy_count())\n                    \n            except Exception as e:\n                self.logger.error(f\"Health check failed: {e}\")\n    \n    async def execute_query(self, query: str, *args, timeout: Optional[float] = None) -> Any:\n        \"\"\"Execute query with comprehensive error handling and monitoring.\"\"\"\n        start_time = time.time()\n        \n        try:\n            async with self.get_connection() as conn:\n                result = await conn.fetch(query, *args, timeout=timeout or self.config.command_timeout)\n                \n                duration = time.time() - start_time\n                self.logger.debug(f\"Query executed in {duration:.3f}s: {query[:100]}...\")\n                \n                return result\n                \n        except asyncpg.PostgresError as e:\n            self.logger.error(f\"PostgreSQL error: {e.sqlstate} - {e.message}\")\n            raise\n        except Exception as e:\n            self.logger.error(f\"Query execution error: {e}\")\n            raise\n    \n    async def close(self) -> None:\n        \"\"\"Gracefully close database pool and cleanup resources.\"\"\"\n        if self._health_check_task:\n            self._health_check_task.cancel()\n            try:\n                await self._health_check_task\n            except asyncio.CancelledError:\n                pass\n                \n        if self.pool:\n            await self.pool.close()\n            self.logger.info(\"Database pool closed successfully\")", "language": "python"}, {"title": "Production Vector Search Service", "description": "Enterprise vector search service with caching, monitoring, and error recovery", "code": "import asyncio\nimport json\nimport hashlib\nfrom typing import List, Dict, Any, Optional, Tuple\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime, timedelta\nimport numpy as np\nfrom redis.asyncio import Redis\nfrom prometheus_client import Counter, Histogram, Gauge\n\n# Metrics\nvector_searches_total = Counter('vector_searches_total', 'Total vector searches', ['status', 'cache_hit'])\nvector_search_duration = Histogram('vector_search_duration_seconds', 'Vector search duration')\nvector_cache_size = Gauge('vector_cache_size', 'Number of cached vectors')\n\n@dataclass\nclass VectorSearchRequest:\n    query_embedding: List[float]\n    tenant_id: str\n    content_type: Optional[str] = None\n    similarity_threshold: float = 0.8\n    limit: int = 10\n    include_metadata: bool = True\n    \n    def __post_init__(self):\n        # Validate embedding dimension\n        if len(self.query_embedding) != 1536:\n            raise ValueError(f\"Expected 1536 dimensions, got {len(self.query_embedding)}\")\n        \n        # Normalize embedding for cosine similarity\n        norm = np.linalg.norm(self.query_embedding)\n        if norm > 0:\n            self.query_embedding = (np.array(self.query_embedding) / norm).tolist()\n    \n    def cache_key(self) -> str:\n        \"\"\"Generate cache key for this search request.\"\"\"\n        content = {\n            'embedding_hash': hashlib.md5(json.dumps(self.query_embedding).encode()).hexdigest()[:16],\n            'tenant_id': self.tenant_id,\n            'content_type': self.content_type,\n            'threshold': self.similarity_threshold,\n            'limit': self.limit\n        }\n        return f\"vector_search:{hashlib.md5(json.dumps(content, sort_keys=True).encode()).hexdigest()}\"\n\n@dataclass\nclass VectorSearchResult:\n    id: int\n    similarity_score: float\n    metadata: Dict[str, Any]\n    content_type: str\n    created_at: datetime\n    \nclass ProductionVectorSearchService:\n    def __init__(self, db_manager: ProductionDatabaseManager, redis_client: Redis):\n        self.db = db_manager\n        self.redis = redis_client\n        self.logger = logging.getLogger(__name__)\n        self.cache_ttl = 3600  # 1 hour\n        self.max_retry_attempts = 3\n        self.circuit_breaker_threshold = 5\n        self.circuit_breaker_failures = 0\n        self.circuit_breaker_last_failure = None\n        \n    async def search_similar_vectors(self, request: VectorSearchRequest) -> List[VectorSearchResult]:\n        \"\"\"Search for similar vectors with caching and error recovery.\"\"\"\n        start_time = time.time()\n        \n        try:\n            # Check circuit breaker\n            if self._is_circuit_open():\n                raise RuntimeError(\"Circuit breaker is open due to repeated failures\")\n            \n            # Try cache first\n            cached_result = await self._get_cached_result(request)\n            if cached_result:\n                vector_searches_total.labels(status='success', cache_hit='true').inc()\n                return cached_result\n            \n            # Execute database search with retry logic\n            results = await self._execute_search_with_retry(request)\n            \n            # Cache results\n            await self._cache_results(request, results)\n            \n            # Reset circuit breaker on success\n            self.circuit_breaker_failures = 0\n            \n            vector_searches_total.labels(status='success', cache_hit='false').inc()\n            return results\n            \n        except Exception as e:\n            self._handle_search_failure(e)\n            vector_searches_total.labels(status='error', cache_hit='false').inc()\n            raise\n            \n        finally:\n            duration = time.time() - start_time\n            vector_search_duration.observe(duration)\n    \n    async def _execute_search_with_retry(self, request: VectorSearchRequest) -> List[VectorSearchResult]:\n        \"\"\"Execute search with exponential backoff retry.\"\"\"\n        last_exception = None\n        \n        for attempt in range(self.max_retry_attempts):\n            try:\n                return await self._execute_database_search(request)\n                \n            except (asyncpg.PostgresError, asyncio.TimeoutError) as e:\n                last_exception = e\n                if attempt < self.max_retry_attempts - 1:\n                    wait_time = min(2 ** attempt, 10)  # Exponential backoff, max 10s\n                    self.logger.warning(f\"Search attempt {attempt + 1} failed, retrying in {wait_time}s: {e}\")\n                    await asyncio.sleep(wait_time)\n                else:\n                    self.logger.error(f\"All {self.max_retry_attempts} search attempts failed\")\n                    \n            except Exception as e:\n                # Don't retry for non-transient errors\n                self.logger.error(f\"Non-retryable search error: {e}\")\n                raise\n        \n        raise last_exception\n    \n    async def _execute_database_search(self, request: VectorSearchRequest) -> List[VectorSearchResult]:\n        \"\"\"Execute the actual database search query.\"\"\"\n        query = \"\"\"\n            SELECT \n                id,\n                1 - (embedding <=> $1::vector) as similarity_score,\n                metadata,\n                content_type,\n                created_at\n            FROM vector_documents\n            WHERE \n                tenant_id = $2::uuid\n                AND ($3::text IS NULL OR content_type = $3)\n                AND (1 - (embedding <=> $1::vector)) >= $4\n                AND security_level IN ('public', 'internal')\n            ORDER BY embedding <=> $1::vector\n            LIMIT $5\n        \"\"\"\n        \n        results = await self.db.execute_query(\n            query,\n            request.query_embedding,\n            request.tenant_id,\n            request.content_type,\n            request.similarity_threshold,\n            request.limit,\n            timeout=30.0\n        )\n        \n        return [\n            VectorSearchResult(\n                id=row['id'],\n                similarity_score=float(row['similarity_score']),\n                metadata=row['metadata'] if request.include_metadata else {},\n                content_type=row['content_type'],\n                created_at=row['created_at']\n            )\n            for row in results\n        ]\n    \n    async def _get_cached_result(self, request: VectorSearchRequest) -> Optional[List[VectorSearchResult]]:\n        \"\"\"Retrieve cached search results.\"\"\"\n        try:\n            cache_key = request.cache_key()\n            cached_data = await self.redis.get(cache_key)\n            \n            if cached_data:\n                data = json.loads(cached_data)\n                return [VectorSearchResult(**item) for item in data]\n                \n        except Exception as e:\n            self.logger.warning(f\"Cache retrieval failed: {e}\")\n            \n        return None\n    \n    async def _cache_results(self, request: VectorSearchRequest, results: List[VectorSearchResult]) -> None:\n        \"\"\"Cache search results with TTL.\"\"\"\n        try:\n            cache_key = request.cache_key()\n            cache_data = json.dumps([asdict(result) for result in results], default=str)\n            \n            await self.redis.setex(cache_key, self.cache_ttl, cache_data)\n            vector_cache_size.inc()\n            \n        except Exception as e:\n            self.logger.warning(f\"Cache storage failed: {e}\")\n    \n    def _is_circuit_open(self) -> bool:\n        \"\"\"Check if circuit breaker is open.\"\"\"\n        if self.circuit_breaker_failures < self.circuit_breaker_threshold:\n            return False\n            \n        if self.circuit_breaker_last_failure:\n            time_since_failure = datetime.now() - self.circuit_breaker_last_failure\n            if time_since_failure > timedelta(minutes=5):\n                # Reset circuit breaker after 5 minutes\n                self.circuit_breaker_failures = 0\n                return False\n                \n        return True\n    \n    def _handle_search_failure(self, exception: Exception) -> None:\n        \"\"\"Handle search failure for circuit breaker logic.\"\"\"\n        self.circuit_breaker_failures += 1\n        self.circuit_breaker_last_failure = datetime.now()\n        \n        self.logger.error(f\"Vector search failure #{self.circuit_breaker_failures}: {exception}\")\n        \n        if self.circuit_breaker_failures >= self.circuit_breaker_threshold:\n            self.logger.error(\"Circuit breaker opened due to repeated failures\")", "language": "python"}], "deployment_configurations": [{"environment": "production", "title": "PostgreSQL Production Configuration", "description": "Optimized PostgreSQL settings for production vector workloads", "config": {"postgresql_conf": {"shared_buffers": "8GB", "effective_cache_size": "24GB", "maintenance_work_mem": "2GB", "checkpoint_completion_target": "0.9", "wal_buffers": "16MB", "default_statistics_target": "100", "random_page_cost": "1.1", "effective_io_concurrency": "200", "work_mem": "32MB", "min_wal_size": "1GB", "max_wal_size": "4GB", "max_worker_processes": "16", "max_parallel_workers_per_gather": "4", "max_parallel_workers": "16", "max_parallel_maintenance_workers": "4"}, "vector_specific": {"hnsw.ef_search": "200", "shared_preload_libraries": "'pg_stat_statements,auto_explain,vector'", "log_min_duration_statement": "1000", "auto_explain.log_min_duration": "5000", "auto_explain.log_analyze": "true"}}}, {"environment": "kubernetes", "title": "Kubernetes Deployment with High Availability", "description": "Production-ready Kubernetes deployment for vector database", "config": {"deployment.yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres-vector\n  namespace: rag-system\nspec:\n  serviceName: postgres-vector\n  replicas: 3\n  selector:\n    matchLabels:\n      app: postgres-vector\n  template:\n    metadata:\n      labels:\n        app: postgres-vector\n    spec:\n      containers:\n      - name: postgres\n        image: pgvector/pgvector:0.8.0-pg16\n        env:\n        - name: POSTGRES_DB\n          value: vector_rag\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: username\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: password\n        - name: PGDATA\n          value: /var/lib/postgresql/data/pgdata\n        ports:\n        - containerPort: 5432\n        volumeMounts:\n        - name: postgres-storage\n          mountPath: /var/lib/postgresql/data\n        - name: postgres-config\n          mountPath: /etc/postgresql/postgresql.conf\n          subPath: postgresql.conf\n        resources:\n          requests:\n            memory: \"16Gi\"\n            cpu: \"4\"\n          limits:\n            memory: \"32Gi\"\n            cpu: \"8\"\n        livenessProbe:\n          exec:\n            command:\n            - /bin/bash\n            - -c\n            - pg_isready -U $POSTGRES_USER -d $POSTGRES_DB\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          exec:\n            command:\n            - /bin/bash\n            - -c\n            - pg_isready -U $POSTGRES_USER -d $POSTGRES_DB\n          initialDelaySeconds: 5\n          periodSeconds: 5\n      volumes:\n      - name: postgres-config\n        configMap:\n          name: postgres-config\n  volumeClaimTemplates:\n  - metadata:\n      name: postgres-storage\n    spec:\n      accessModes: [\"ReadWriteOnce\"]\n      resources:\n        requests:\n          storage: 1Ti\n      storageClassName: fast-ssd", "service.yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: postgres-vector-service\n  namespace: rag-system\nspec:\n  selector:\n    app: postgres-vector\n  ports:\n  - name: postgres\n    port: 5432\n    targetPort: 5432\n  type: ClusterIP", "configmap.yaml": "apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: postgres-config\n  namespace: rag-system\ndata:\n  postgresql.conf: |\n    shared_buffers = 8GB\n    effective_cache_size = 24GB\n    maintenance_work_mem = 2GB\n    checkpoint_completion_target = 0.9\n    wal_buffers = 16MB\n    default_statistics_target = 100\n    random_page_cost = 1.1\n    effective_io_concurrency = 200\n    work_mem = 32MB\n    min_wal_size = 1GB\n    max_wal_size = 4GB\n    shared_preload_libraries = 'pg_stat_statements,auto_explain,vector'\n    hnsw.ef_search = 200"}}, {"environment": "docker_compose", "title": "Docker Compose Development Setup", "description": "Complete development environment with monitoring and observability", "config": {"docker-compose.yml": "version: '3.8'\n\nservices:\n  postgres:\n    image: pgvector/pgvector:0.8.0-pg16\n    environment:\n      POSTGRES_DB: vector_rag_dev\n      POSTGRES_USER: postgres\n      POSTGRES_PASSWORD: postgres\n      PGDATA: /var/lib/postgresql/data/pgdata\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n      - ./postgresql.conf:/etc/postgresql/postgresql.conf\n      - ./init-scripts:/docker-entrypoint-initdb.d\n    command: postgres -c config_file=/etc/postgresql/postgresql.conf\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U postgres -d vector_rag_dev\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n    deploy:\n      resources:\n        limits:\n          memory: 4G\n          cpus: '2'\n\n  redis:\n    image: redis:7-alpine\n    ports:\n      - \"6379:6379\"\n    command: redis-server --maxmemory 1gb --maxmemory-policy allkeys-lru\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n      interval: 5s\n      timeout: 3s\n      retries: 5\n\n  prometheus:\n    image: prom/prometheus:latest\n    ports:\n      - \"9090:9090\"\n    volumes:\n      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n      - prometheus_data:/prometheus\n    command:\n      - '--config.file=/etc/prometheus/prometheus.yml'\n      - '--storage.tsdb.path=/prometheus'\n      - '--web.console.libraries=/etc/prometheus/console_libraries'\n      - '--web.console.templates=/etc/prometheus/consoles'\n      - '--storage.tsdb.retention.time=200h'\n      - '--web.enable-lifecycle'\n\n  grafana:\n    image: grafana/grafana:latest\n    ports:\n      - \"3000:3000\"\n    environment:\n      GF_SECURITY_ADMIN_PASSWORD: admin\n    volumes:\n      - grafana_data:/var/lib/grafana\n      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards\n      - ./grafana/datasources:/etc/grafana/provisioning/datasources\n\nvolumes:\n  postgres_data:\n  prometheus_data:\n  grafana_data:\n\nnetworks:\n  default:\n    driver: bridge"}}], "performance_benchmarks": [{"metric": "Query Performance Improvement", "baseline": "500ms (unoptimized query)", "optimized": "5ms (optimized subquery pattern)", "improvement_factor": "100x faster"}, {"metric": "Index Configuration", "recommendation": "lists = rows/1000 for IVFFlat indexes", "example": "500 lists performed best in benchmark testing", "context": "For datasets up to 1M rows"}], "troubleshooting": [{"issue": "pgvector HNSW index not being used in queries", "symptoms": ["Query takes >1000ms", "EXPLAIN shows Seq Scan instead of Index Scan", "pg_stat_user_indexes shows 0 idx_scan"], "root_causes": ["Index created with wrong operator class", "Query uses different distance function than index", "Outdated table statistics", "Index too small to be considered by planner"], "solutions": ["Recreate index with correct ops: CREATE INDEX USING hnsw (embedding vector_l2_ops)", "Match query distance operator with index: <-> for L2, <=> for cosine", "Run ANALYZE table_name to update statistics", "Increase ef parameter: SET hnsw.ef_search = 200 for specific session"], "verification": "EXPLAIN (ANALYZE, BUFFERS) should show 'Index Scan using hnsw_idx' and execution time <50ms for 1M vectors", "references": ["https://github.com/pgvector/pgvector/issues/389", "https://www.crunchydata.com/blog/pgvector-performance-debugging"]}, {"issue": "pgvector performance degradation with large datasets (100M+ vectors)", "symptoms": ["Query latency increases exponentially after 10M vectors", "High memory usage causing OOM errors", "Index creation takes >24 hours"], "root_causes": ["HNSW index doesn't fit in shared_buffers", "Default m parameter too high for large datasets", "No proper vacuuming strategy"], "solutions": ["Increase shared_buffers to 40% of RAM", "Use lower m parameter: CREATE INDEX USING hnsw (embedding vector_l2_ops) WITH (m = 8)", "Implement regular VACUUM and REINDEX schedule", "Consider partitioning by time or category"], "verification": "Monitor pg_stat_bgwriter for buffer allocation, query time should be <100ms p99", "references": ["https://github.com/pgvector/pgvector/discussions/456", "PostgreSQL 17 release notes"]}, {"issue": "Alembic migration deadlocks in production", "symptoms": ["Migration hangs indefinitely", "ERROR: deadlock detected", "Other transactions timeout waiting for locks"], "root_causes": ["ACCESS EXCLUSIVE lock conflicts with application queries", "Multiple migrations running simultaneously", "Long-running transactions prevent lock acquisition"], "solutions": ["Set lock_timeout = '30s' in migration", "Use CONCURRENTLY for index creation", "Schedule migrations during low-traffic windows", "Break large migrations into smaller chunks"], "verification": "Check pg_locks view during migration, monitor application error rates", "references": ["https://alembic.sqlalchemy.org/en/latest/cookbook.html#postgresql-concurrency"]}], "common_pitfalls": [{"mistake": "Creating HNSW index on empty table then bulk loading data", "consequences": "Index becomes unbalanced, poor query performance", "prevention": "Load data first, then create index, or use REINDEX after bulk load", "recovery": "DROP INDEX and recreate after data load completion"}, {"mistake": "Using default pgvector parameters for production workloads", "consequences": "Suboptimal performance, wasted memory", "prevention": "Benchmark different m, ef_construction parameters for your specific dataset", "recovery": "Recreate index with optimized parameters based on load testing"}, {"mistake": "Not monitoring vector index memory usage", "consequences": "Index eviction from memory, severe performance degradation", "prevention": "Set up monitoring for shared_buffers usage and index sizes", "recovery": "Increase shared_buffers or implement index partitioning strategy"}], "latest_trends_2025": [{"trend": "pgvector 0.8.0 with enhanced iterative scanning and binary quantization", "release_date": "2025-08-15", "key_features": ["30% faster index build times", "Reduced memory usage for large indexes", "Better parallel query support"], "migration_notes": "Requires PostgreSQL 16+ for optimal performance"}, {"trend": "PostgreSQL 17 vector performance improvements", "release_date": "2025-09-14", "key_features": ["Native vector partitioning support", "Improved planner statistics for vectors", "Better memory management for large indexes"], "adoption_status": "Beta testing in production environments"}, {"trend": "Hybrid vector-relational database patterns", "description": "Combining traditional relational data with vector embeddings in single queries", "use_cases": ["Multi-modal RAG systems", "Contextual similarity search", "Hierarchical document classification"], "implementation_example": "JOIN tables with vector similarity conditions"}], "production_patterns": [{"scenario": "Multi-tenant vector database with 1B+ vectors", "scale": "1000+ tenants, 1M vectors per tenant, 10k QPS", "architecture": "Row-level security + tenant-based partitioning + connection pooling", "performance_metrics": {"latency_p50": "15ms", "latency_p99": "85ms", "throughput": "12,000 QPS", "cost_per_request": "$0.0002"}, "lessons_learned": ["Partition by tenant_id for isolation", "Use prepared statements for common queries", "Monitor index fragmentation weekly"], "monitoring_setup": "Prometheus + custom pgvector metrics, alerts on p99 > 100ms"}, {"scenario": "Zero-downtime pgvector migration to new index algorithm", "scale": "100M vectors, 24/7 production traffic, 5K QPS during migration", "architecture": "Blue-green deployment with synchronized vector indexes", "performance_metrics": {"migration_time": "4 hours total downtime", "data_consistency": "100% vector integrity validation", "rollback_time": "<10 minutes", "traffic_loss": "0% with proper load balancer configuration"}, "lessons_learned": ["Test migration on full-scale staging first", "Use WAL-based replication for consistency", "Implement gradual traffic shifting"], "monitoring_setup": "Real-time index build progress, vector count validation, performance regression alerts"}, {"scenario": "Disaster recovery for distributed vector database", "scale": "3 geographic regions, 500M vectors, RTO 15 minutes", "architecture": "Cross-region replication + automated failover + vector index synchronization", "performance_metrics": {"rpo_target": "5 minutes data loss maximum", "rto_actual": "12 minutes average failover time", "consistency_lag": "<30 seconds between regions", "failover_success_rate": "99.8% automated recovery"}, "lessons_learned": ["Vector indexes don't replicate automatically", "Implement vector-aware backup validation", "Test failover monthly with production data"], "monitoring_setup": "Cross-region lag monitoring, vector index health checks, automated DR testing"}, {"scenario": "Real-time vector ingestion with high write throughput", "scale": "50K vectors/second ingestion, concurrent reads at 8K QPS", "architecture": "Write-optimized partitioning + asynchronous index updates + read replicas", "performance_metrics": {"write_latency_p95": "25ms", "read_latency_p95": "40ms during writes", "index_lag": "<2 seconds for new vectors", "resource_utilization": "75% CPU, 85% memory"}, "lessons_learned": ["Batch vector insertions for better performance", "Separate write and read workloads", "Use HOT updates for metadata changes"], "monitoring_setup": "Write queue depth, index update lag, read/write isolation metrics"}, {"scenario": "Cost-optimized vector storage with tiered access patterns", "scale": "10B vectors, 80% cold data, 20% hot data access", "architecture": "Hot-cold data tiering + compressed cold storage + intelligent caching", "performance_metrics": {"hot_data_latency": "20ms p95", "cold_data_latency": "200ms p95", "storage_cost_reduction": "65% with tiering", "cache_hit_rate": "85% for hot data"}, "lessons_learned": ["Implement vector access pattern analytics", "Use vector compression for cold storage", "Pre-warm cache based on usage patterns"], "monitoring_setup": "Storage tier utilization, cache hit rates, cost per query analytics"}], "scaling_strategies": [{"from_scale": "1K vectors/day", "to_scale": "10K vectors/day", "changes_required": ["Enable connection pooling with pgBouncer", "Tune shared_buffers to 25% of RAM", "Add read replicas for query load distribution", "Implement vector index maintenance scheduling"], "cost_implications": "Infrastructure costs increase 2-3x, operational complexity +30%", "timeline": "2-3 weeks implementation", "performance_impact": {"query_latency": "Remains <50ms p95 with proper indexing", "throughput_capacity": "10x increase with minimal latency degradation", "storage_growth": "Linear growth ~100MB per 10K 1536-dim vectors"}}, {"from_scale": "10K vectors/day", "to_scale": "100K vectors/day", "changes_required": ["Partition tables by time or hash for parallel processing", "Implement vector compression strategies", "Add dedicated indexing workers", "Scale to multi-node setup with Postgres-XL or Citus"], "cost_implications": "Infrastructure costs 5-8x, requires distributed architecture", "timeline": "6-8 weeks major architecture change", "performance_impact": {"query_latency": "Target <100ms p95 with partitioning", "throughput_capacity": "100x increase requires horizontal scaling", "storage_growth": "1GB+ per 100K vectors, compression saves 30-50%"}}, {"from_scale": "100K vectors/day", "to_scale": "1M vectors/day", "changes_required": ["Full distributed vector database (Weaviate, Qdrant, or Pinecone)", "Implement vector quantization and approximation", "Add vector caching layer with Redis/Memcached", "Separate read/write workloads with event streaming"], "cost_implications": "Infrastructure costs 10-15x, specialized vector DB licensing", "timeline": "3-4 months complete platform migration", "performance_impact": {"query_latency": "Target <200ms p95 with approximation algorithms", "throughput_capacity": "1000x increase with distributed processing", "storage_growth": "10GB+ per 1M vectors, requires storage tiering"}}], "security_guidelines": [{"category": "migration_safety", "title": "Alembic Migration Security", "description": "PostgreSQL uses ACCESS EXCLUSIVE locks during schema changes. Always review autogenerated migrations for potential locking issues.", "risk_level": "high", "mitigation": "Use lock_timeout and statement_timeout, test migrations on staging environment first"}, {"category": "vector_data_protection", "title": "Vector Embedding Security", "description": "Vector embeddings can leak information about original data. Implement proper access controls and consider differential privacy for sensitive vectors.", "risk_level": "medium", "mitigation": "Use row-level security, encrypt vectors at rest, implement audit logging for vector queries"}], "rag_development_scenarios": [{"scenario": "Vector Database Architecture Design for Dynamic Taxonomy RAG", "development_phase": "Architecture Planning", "collaboration_agents": ["hybrid-search-specialist", "taxonomy-architect"], "development_tasks": ["Design hierarchical vector storage schema for taxonomic relationships", "Plan pgvector indexing strategy for multi-level semantic search", "Architect metadata schema for dynamic taxonomy updates", "Design vector dimension optimization for taxonomy embeddings"], "technical_decisions": {"vector_dimensions": "1536 for OpenAI embeddings vs 768 for sentence-transformers", "index_strategy": "HNSW for similarity search + GIN for metadata filtering", "partition_strategy": "Partition by taxonomy level and domain for query optimization", "update_approach": "Delta updates with versioned taxonomy snapshots"}, "development_outputs": ["Vector database schema DDL scripts", "Indexing configuration for pgvector", "Migration scripts for taxonomy evolution", "Performance benchmarking setup"]}, {"scenario": "RAG Evaluation Database Design", "development_phase": "Testing Infrastructure", "collaboration_agents": ["rag-evaluation-specialist", "observability-engineer"], "development_tasks": ["Design evaluation metrics storage schema", "Plan test dataset organization and versioning", "Architect performance benchmarking database", "Design A/B testing result storage"], "technical_decisions": {"metrics_storage": "Time-series database for performance metrics + relational for test cases", "test_data_versioning": "Git-like versioning for evaluation datasets", "benchmark_schema": "Separate schemas for different evaluation types (accuracy, latency, cost)", "result_aggregation": "Pre-computed aggregations for dashboard performance"}, "development_outputs": ["Evaluation database schema", "Test data management scripts", "Benchmarking automation tools", "Evaluation result APIs"]}, {"scenario": "Document Processing Pipeline Database Integration", "development_phase": "Data Pipeline Development", "collaboration_agents": ["document-ingestion-specialist", "classification-pipeline-expert"], "development_tasks": ["Design document metadata and processing status tracking", "Plan chunking strategy storage and retrieval optimization", "Architect document versioning and update detection", "Design classification result storage and indexing"], "technical_decisions": {"document_storage": "Hybrid approach: blob storage for files + metadata in PostgreSQL", "chunk_management": "Document-chunk relationship tracking with position indexing", "processing_status": "State machine tracking with audit trail", "classification_schema": "Flexible JSON schema for multi-label classification results"}, "development_outputs": ["Document processing database schema", "Chunk management system", "Processing workflow tracking", "Classification result storage system"]}], "cross_agent_development_collaboration": [{"collaboration_type": "Architecture Co-design", "agents": ["database-architect", "hybrid-search-specialist", "taxonomy-architect"], "development_scenario": "Designing unified storage for hierarchical taxonomy with vector embeddings", "workflow": ["Database-architect: Proposes storage schema and indexing strategy", "Taxonomy-architect: Validates hierarchical relationship representation", "Hybrid-search-specialist: Reviews search performance implications", "Joint: Iterative refinement of schema based on search and hierarchy requirements"], "deliverables": ["Unified database schema supporting both vector search and taxonomy navigation", "Performance optimization strategy for multi-dimensional queries", "Migration plan for taxonomy evolution without search disruption"]}, {"collaboration_type": "Performance Optimization", "agents": ["database-architect", "observability-engineer", "api-designer"], "development_scenario": "Optimizing RAG system database performance for production deployment", "workflow": ["Database-architect: Identifies performance bottlenecks and optimization opportunities", "Observability-engineer: Sets up monitoring and provides performance insights", "API-designer: Reviews query patterns and suggests API-level optimizations", "Joint: Implements and validates performance improvements"], "deliverables": ["Database performance optimization plan", "Monitoring setup for database performance tracking", "API query optimization guidelines", "Load testing and benchmarking results"]}]}, "timestamp": 1758336923.252168}, "kb_hybrid-search-specialist": {"data": {"subagent": "hybrid-search-specialist", "timestamp": "2025-09-14T15:26:14.415125", "search_results": [{"query": "BM25 vector similarity hybrid search RAG 2025", "url": "https://superlinked.com/vectorhub/articles/optimizing-rag-with-hybrid-search-reranking", "title": "Optimizing RAG with Hybrid Search & Reranking | VectorHub by Superlinked", "content": "Hybrid search bridges the gap between BM25's precision with vector search's contextual understanding, delivering faster, more accurate, and semantically aware results. For 2025 roadmaps, the pragmatic recipe is clear: two routes + smart fusion + optional reranker.", "relevance_score": 0.95, "timestamp": "2025-09-14 15:26:14.415233", "subagent": "hybrid-search-specialist", "category": "performance"}, {"query": "BM25 vector similarity hybrid search RAG 2025", "url": "https://weaviate.io/blog/hybrid-search-explained", "title": "Hybrid Search Explained | Weaviate", "content": "Hybrid search is a keyword-sensitive semantic search approach that combines vector search and keyword search algorithms to take advantage of their respective strengths while mitigating limitations. Weaviate supports rankedFusion and relativeScoreFusion algorithms.", "relevance_score": 0.92, "timestamp": "2025-09-14 15:26:14.415285", "subagent": "hybrid-search-specialist", "category": "framework"}, {"query": "FAISS Chromadb Qdrant vector database comparison 2025", "url": "https://liquidmetal.ai/casesAndBlogs/vector-comparison/", "title": "Vector Database Comparison: Pinecone vs Weaviate vs Qdrant vs FAISS vs Milvus vs Chroma (2025)", "content": "Choose FAISS for raw performance and research, ChromaDB for simplicity and quick development, and Qdrant for production-ready real-time applications with complex filtering needs. Dataset size recommendations: <10k vectors: ChromaDB or FAISS, 200k-10M vectors: FAISS (HNSW or IVF).", "relevance_score": 0.9, "timestamp": "2025-09-14 15:26:15.420647", "subagent": "hybrid-search-specialist", "category": "comparison"}, {"query": "FAISS Chromadb Qdrant vector database comparison 2025", "url": "https://subhojyoti99.medium.com/faiss-vs-chroma-vs-qdrant-vs-aws-6d3108ac5370", "title": "FAISS vs. Chroma vs. Qdrant vs. AWS | by Subhojyoti Singha", "content": "FAISS excels in scenarios requiring extreme search speed or handling high-dimensional vectors. ChromaDB is lightweight, easy-to-use option for smaller projects. Qdrant is ideal for real-time applications that need immediate, high-availability vector search with complex filtering capabilities.", "relevance_score": 0.88, "timestamp": "2025-09-14 15:26:15.420668", "subagent": "hybrid-search-specialist", "category": "comparison"}, {"query": "Hybrid search fusion methods RRF reranking 2025", "url": "https://medium.com/etoai/hybrid-search-combining-bm25-and-semantic-search-for-better-results-with-lan-1358038fe7e6", "title": "Hybrid Search: Combining BM25 and Semantic Search with Langchain", "content": "Fusion methods include RRF (Reciprocal Rank Fusion) or weighted scoring for multiple recall strategies. Model-based rerank includes cross encoder models like bge-reranker-v2-m3. BM25 is the initial filter, leveraging keyword-based precision to reduce the dataset to manageable subset.", "relevance_score": 0.85, "timestamp": "2025-09-14 15:26:16.431646", "subagent": "hybrid-search-specialist", "category": "implementation"}, {"query": "Hybrid search fusion methods RRF reranking 2025", "url": "https://www.chitika.com/hybrid-retrieval-rag/", "title": "Implementing Hybrid Retrieval (BM25 + FAISS) in RAG", "content": "Binary Quantization dramatically shrinks vector embeddings by converting 32-bit floats to compact 1-bit representations. Fine-tuning k1 and b parameters is essential; increasing k1 can emphasize term frequency, while adjusting b accounts for document length variability.", "relevance_score": 0.82, "timestamp": "2025-09-14 15:26:16.431717", "subagent": "hybrid-search-specialist", "category": "implementation"}], "frameworks": {"weaviate": {"name": "Weaviate", "version": "1.21+", "key_features": ["Native hybrid search with rankedFusion and relativeScoreFusion", "BM25/BM25F sparse vector search", "Dense vector search with multiple models", "GraphQL and REST API support", "Multi-tenancy and RBAC"], "installation": "docker run -p 8080:8080 semitechnologies/weaviate:latest"}, "langchain": {"name": "LangChain", "version": "1.0+", "key_features": ["EnsembleRetriever for hybrid search", "BM25Retriever for keyword search", "VectorstoreRetriever for semantic search", "Document compression and reranking", "RAG pipeline integration", "LangGraph workflow orchestration", "Enhanced streaming and async support", "Improved memory management", "Better error handling and retries"], "installation": "pip install langchain>=1.0.0 langchain-community langchain-core rank_bm25"}, "chromadb": {"name": "ChromaDB", "version": "0.4+", "key_features": ["Vector database with metadata filtering", "Embedding functions integration", "Collection-based organization", "Persistent and in-memory modes", "Python and JavaScript clients"], "installation": "pip install chromadb"}, "elasticsearch": {"name": "Elasticsearch", "version": "8.x", "key_features": ["Hybrid search with sparse and dense vectors", "BM25 scoring with boosting", "kNN vector search", "Reciprocal rank fusion", "Real-time indexing and search"], "installation": "pip install elasticsearch"}}, "best_practices": [{"category": "fusion_algorithms", "title": "Optimal Search Fusion Strategy", "description": "Use Reciprocal Rank Fusion (RRF) to combine BM25 keyword and vector similarity scores effectively", "implementation": "RRF score = \u03a3(1/(rank_i + k)) where k=60 is commonly used constant"}, {"category": "parameter_tuning", "title": "Alpha Parameter Optimization", "description": "Tune alpha parameter to balance keyword vs semantic search: \u03b1=0 (pure keyword), \u03b1=0.5 (balanced), \u03b1=1 (pure vector)", "implementation": "Start with \u03b1=0.7 for most RAG applications, then optimize based on evaluation metrics"}, {"category": "reranking", "title": "Two-Stage Retrieval with Reranking", "description": "Apply cross-encoder reranking after initial hybrid retrieval for improved accuracy", "implementation": "Use models like bge-reranker-v2-m3 or cross-encoder/ms-marco-MiniLM-L-6-v2"}, {"category": "performance_optimization", "title": "Efficient Hybrid Index Management", "description": "Maintain separate sparse (BM25) and dense (vector) indices for optimal query performance", "implementation": "Use inverted indices for keywords and HNSW/IVF indices for vectors"}, {"category": "evaluation", "title": "Comprehensive Evaluation Metrics", "description": "Evaluate hybrid search using both keyword-focused and semantic-focused metrics", "implementation": "Track MRR, NDCG, Hit@K for keyword queries and semantic similarity scores for contextual queries"}], "code_examples": [{"title": "LangChain Hybrid Search with EnsembleRetriever", "description": "Combine BM25 and vector search using LangChain ensemble retriever", "code": "from langchain.retrievers import BM25Retriever, EnsembleRetriever\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.schema import Document\nfrom rank_bm25 import BM25Okapi\nimport numpy as np\nfrom typing import List, Dict, Any\n\nclass HybridSearchRetriever:\n    def __init__(self, documents: List[Document], embeddings_model, alpha: float = 0.7):\n        self.documents = documents\n        self.embeddings = embeddings_model\n        self.alpha = alpha\n        \n        # Initialize BM25 retriever\n        self.bm25_retriever = BM25Retriever.from_documents(documents)\n        self.bm25_retriever.k = 10  # Retrieve top 10 for fusion\n        \n        # Initialize vector retriever\n        self.vectorstore = Chroma.from_documents(documents, embeddings_model)\n        self.vector_retriever = self.vectorstore.as_retriever(search_kwargs={\"k\": 10})\n        \n        # Create ensemble retriever\n        self.ensemble_retriever = EnsembleRetriever(\n            retrievers=[self.bm25_retriever, self.vector_retriever],\n            weights=[1-alpha, alpha]  # BM25 weight, Vector weight\n        )\n    \n    def search(self, query: str, k: int = 5) -> List[Document]:\n        \"\"\"Perform hybrid search and return top k documents\"\"\"\n        return self.ensemble_retriever.get_relevant_documents(query)[:k]\n    \n    def search_with_scores(self, query: str, k: int = 5) -> List[tuple]:\n        \"\"\"Perform hybrid search with relevance scores\"\"\"\n        # Get BM25 results\n        bm25_docs = self.bm25_retriever.get_relevant_documents(query)\n        bm25_scores = self._get_bm25_scores(query, bm25_docs)\n        \n        # Get vector search results\n        vector_results = self.vectorstore.similarity_search_with_score(query, k=10)\n        \n        # Apply Reciprocal Rank Fusion\n        fused_results = self._reciprocal_rank_fusion(\n            bm25_docs, bm25_scores, vector_results, k=k\n        )\n        \n        return fused_results\n    \n    def _get_bm25_scores(self, query: str, docs: List[Document]) -> List[float]:\n        \"\"\"Calculate BM25 scores for documents\"\"\"\n        # Tokenize query and documents\n        tokenized_docs = [doc.page_content.lower().split() for doc in self.documents]\n        bm25 = BM25Okapi(tokenized_docs)\n        query_tokens = query.lower().split()\n        \n        # Get scores for retrieved documents\n        doc_indices = [self.documents.index(doc) for doc in docs]\n        scores = [bm25.get_scores(query_tokens)[i] for i in doc_indices]\n        return scores\n    \n    def _reciprocal_rank_fusion(self, bm25_docs: List[Document], bm25_scores: List[float],\n                               vector_results: List[tuple], k: int = 60) -> List[tuple]:\n        \"\"\"Apply Reciprocal Rank Fusion to combine results\"\"\"\n        # Create unified scoring\n        doc_scores = {}\n        \n        # Add BM25 scores\n        for rank, (doc, score) in enumerate(zip(bm25_docs, bm25_scores)):\n            doc_key = doc.page_content[:100]  # Use first 100 chars as key\n            doc_scores[doc_key] = doc_scores.get(doc_key, 0) + (1 / (rank + k))\n        \n        # Add vector scores\n        for rank, (doc, score) in enumerate(vector_results):\n            doc_key = doc.page_content[:100]\n            # Convert distance to similarity score\n            similarity_score = 1 / (1 + score) if score > 0 else 1.0\n            doc_scores[doc_key] = doc_scores.get(doc_key, 0) + (similarity_score / (rank + k))\n        \n        # Sort by combined score\n        sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n        \n        # Return top k results\n        result_docs = []\n        for doc_key, score in sorted_docs[:k]:\n            # Find original document\n            matching_doc = next(doc for doc in self.documents \n                               if doc.page_content.startswith(doc_key[:50]))\n            result_docs.append((matching_doc, score))\n        \n        return result_docs", "language": "python"}, {"title": "Weaviate Hybrid Search Implementation", "description": "Native hybrid search using Weaviate with configurable fusion algorithms", "code": "import weaviate\nfrom typing import List, Dict, Optional\nimport json\n\nclass WeaviateHybridSearch:\n    def __init__(self, weaviate_url: str = \"http://localhost:8080\", \n                 api_key: Optional[str] = None):\n        # Initialize Weaviate client\n        if api_key:\n            auth_config = weaviate.AuthApiKey(api_key=api_key)\n            self.client = weaviate.Client(url=weaviate_url, auth_client_secret=auth_config)\n        else:\n            self.client = weaviate.Client(url=weaviate_url)\n        \n        self.class_name = \"Document\"\n        self._create_schema()\n    \n    def _create_schema(self):\n        \"\"\"Create Weaviate schema for documents\"\"\"\n        schema = {\n            \"classes\": [{\n                \"class\": self.class_name,\n                \"description\": \"A document for hybrid search\",\n                \"properties\": [\n                    {\n                        \"name\": \"content\",\n                        \"dataType\": [\"text\"],\n                        \"description\": \"The content of the document\",\n                    },\n                    {\n                        \"name\": \"title\",\n                        \"dataType\": [\"string\"],\n                        \"description\": \"The title of the document\",\n                    },\n                    {\n                        \"name\": \"category\",\n                        \"dataType\": [\"string\"],\n                        \"description\": \"The category of the document\",\n                    }\n                ],\n                \"vectorizer\": \"text2vec-openai\"  # or your preferred vectorizer\n            }]\n        }\n        \n        # Create schema if it doesn't exist\n        if not self.client.schema.exists(self.class_name):\n            self.client.schema.create(schema)\n    \n    def add_documents(self, documents: List[Dict[str, str]]):\n        \"\"\"Add documents to Weaviate\"\"\"\n        with self.client.batch as batch:\n            batch.batch_size = 100\n            for doc in documents:\n                batch.add_data_object(\n                    data_object=doc,\n                    class_name=self.class_name\n                )\n    \n    def hybrid_search(self, query: str, alpha: float = 0.75, limit: int = 5,\n                     fusion_type: str = \"rankedFusion\", where_filter: Optional[Dict] = None) -> List[Dict]:\n        \"\"\"Perform hybrid search with configurable parameters\"\"\"\n        # Build the query\n        query_builder = (self.client.query\n                        .get(self.class_name, [\"content\", \"title\", \"category\"])\n                        .with_hybrid(query=query, alpha=alpha, fusion_type=fusion_type)\n                        .with_limit(limit)\n                        .with_additional([\"score\", \"id\"]))\n        \n        # Add where filter if provided\n        if where_filter:\n            query_builder = query_builder.with_where(where_filter)\n        \n        # Execute query\n        result = query_builder.do()\n        \n        return result[\"data\"][\"Get\"][self.class_name]\n    \n    def keyword_search(self, query: str, limit: int = 5) -> List[Dict]:\n        \"\"\"Pure BM25 keyword search (alpha=0)\"\"\"\n        return self.hybrid_search(query, alpha=0.0, limit=limit)\n    \n    def vector_search(self, query: str, limit: int = 5) -> List[Dict]:\n        \"\"\"Pure vector search (alpha=1)\"\"\"\n        return self.hybrid_search(query, alpha=1.0, limit=limit)\n    \n    def compare_search_methods(self, query: str, limit: int = 5) -> Dict[str, List[Dict]]:\n        \"\"\"Compare different search methods for the same query\"\"\"\n        return {\n            \"keyword_only\": self.keyword_search(query, limit),\n            \"vector_only\": self.vector_search(query, limit),\n            \"hybrid_balanced\": self.hybrid_search(query, alpha=0.5, limit=limit),\n            \"hybrid_vector_focused\": self.hybrid_search(query, alpha=0.75, limit=limit)\n        }\n    \n    def explain_search_results(self, results: List[Dict]) -> None:\n        \"\"\"Print detailed explanation of search results\"\"\"\n        for i, result in enumerate(results, 1):\n            print(f\"\\nResult {i}:\")\n            print(f\"Title: {result.get('title', 'N/A')}\")\n            print(f\"Category: {result.get('category', 'N/A')}\")\n            print(f\"Score: {result['_additional']['score']:.4f}\")\n            print(f\"Content: {result['content'][:200]}...\") \n\n# Usage example\ndef demonstrate_weaviate_hybrid_search():\n    \"\"\"Example usage of Weaviate hybrid search\"\"\"\n    searcher = WeaviateHybridSearch()\n    \n    # Add sample documents\n    documents = [\n        {\"title\": \"Python Programming\", \"content\": \"Learn Python programming with examples\", \"category\": \"technology\"},\n        {\"title\": \"Machine Learning Basics\", \"content\": \"Introduction to ML algorithms and concepts\", \"category\": \"AI\"},\n        {\"title\": \"Data Science Guide\", \"content\": \"Complete guide to data science and analytics\", \"category\": \"analytics\"}\n    ]\n    \n    searcher.add_documents(documents)\n    \n    # Compare different search approaches\n    query = \"Python machine learning\"\n    comparison_results = searcher.compare_search_methods(query)\n    \n    for method, results in comparison_results.items():\n        print(f\"\\n=== {method.upper()} ===\")\n        searcher.explain_search_results(results)\n    \n    return searcher", "language": "python"}, {"title": "LangChain v1.0 Enhanced Hybrid Search", "description": "Modern hybrid search implementation using LangChain v1.0 features with LangGraph workflows", "code": "from langchain_core.retrievers import BaseRetriever\nfrom langchain_core.documents import Document\nfrom langchain_core.callbacks import CallbackManagerForRetrieverRun\nfrom langchain_community.retrievers import BM25Retriever\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langgraph import StateGraph, END\nfrom langgraph.graph import MessageGraph\nfrom typing import List, Dict, Any, Optional, TypedDict\nimport asyncio\nfrom dataclasses import dataclass\nimport logging\nfrom datetime import datetime\n\n# LangChain v1.0 compatible type definitions\nclass SearchState(TypedDict):\n    query: str\n    bm25_results: List[Document]\n    vector_results: List[Document]\n    fused_results: List[Document]\n    final_results: List[Document]\n    metadata: Dict[str, Any]\n\n@dataclass\nclass HybridSearchConfig:\n    bm25_weight: float = 0.3\n    vector_weight: float = 0.7\n    rrf_k: int = 60\n    max_results: int = 10\n    enable_async: bool = True\n    timeout_seconds: int = 30\n\nclass LangChainV1HybridRetriever(BaseRetriever):\n    \"\"\"Enhanced hybrid retriever using LangChain v1.0 features.\"\"\"\n    \n    def __init__(self, documents: List[Document], \n                 embeddings: OpenAIEmbeddings,\n                 config: HybridSearchConfig = None):\n        super().__init__()\n        self.documents = documents\n        self.embeddings = embeddings\n        self.config = config or HybridSearchConfig()\n        self.logger = logging.getLogger(__name__)\n        \n        # Initialize retrievers with v1.0 patterns\n        self._setup_retrievers()\n        self._setup_workflow()\n    \n    def _setup_retrievers(self) -> None:\n        \"\"\"Setup BM25 and vector retrievers with improved error handling.\"\"\"\n        try:\n            # BM25 retriever with enhanced configuration\n            self.bm25_retriever = BM25Retriever.from_documents(\n                self.documents,\n                k=self.config.max_results * 2  # Get more for fusion\n            )\n            \n            # Vector store with improved settings\n            self.vectorstore = Chroma.from_documents(\n                documents=self.documents,\n                embedding=self.embeddings,\n                collection_metadata={\"hnsw:space\": \"cosine\"}  # v1.0 optimization\n            )\n            \n            self.vector_retriever = self.vectorstore.as_retriever(\n                search_type=\"similarity_score_threshold\",\n                search_kwargs={\n                    \"k\": self.config.max_results * 2,\n                    \"score_threshold\": 0.7\n                }\n            )\n            \n            self.logger.info(\"Retrievers initialized successfully\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to setup retrievers: {e}\")\n            raise\n    \n    def _setup_workflow(self) -> None:\n        \"\"\"Setup LangGraph workflow for hybrid search orchestration.\"\"\"\n        try:\n            # Create workflow graph\n            workflow = StateGraph(SearchState)\n            \n            # Add nodes\n            workflow.add_node(\"bm25_search\", self._bm25_search_node)\n            workflow.add_node(\"vector_search\", self._vector_search_node)\n            workflow.add_node(\"fusion\", self._fusion_node)\n            workflow.add_node(\"rerank\", self._rerank_node)\n            \n            # Define edges\n            workflow.set_entry_point(\"bm25_search\")\n            workflow.add_edge(\"bm25_search\", \"vector_search\")\n            workflow.add_edge(\"vector_search\", \"fusion\")\n            workflow.add_edge(\"fusion\", \"rerank\")\n            workflow.add_edge(\"rerank\", END)\n            \n            self.workflow = workflow.compile()\n            \n            self.logger.info(\"LangGraph workflow configured\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to setup workflow: {e}\")\n            raise\n    \n    async def _bm25_search_node(self, state: SearchState) -> SearchState:\n        \"\"\"BM25 search node with async support.\"\"\"\n        try:\n            if self.config.enable_async:\n                # Use async retrieval in v1.0\n                bm25_results = await self.bm25_retriever.ainvoke(\n                    state[\"query\"],\n                    config={\"timeout\": self.config.timeout_seconds}\n                )\n            else:\n                bm25_results = self.bm25_retriever.invoke(state[\"query\"])\n            \n            state[\"bm25_results\"] = bm25_results\n            state[\"metadata\"][\"bm25_count\"] = len(bm25_results)\n            \n            return state\n            \n        except Exception as e:\n            self.logger.error(f\"BM25 search failed: {e}\")\n            state[\"bm25_results\"] = []\n            return state\n    \n    async def _vector_search_node(self, state: SearchState) -> SearchState:\n        \"\"\"Vector search node with enhanced similarity scoring.\"\"\"\n        try:\n            if self.config.enable_async:\n                vector_results = await self.vector_retriever.ainvoke(\n                    state[\"query\"],\n                    config={\"timeout\": self.config.timeout_seconds}\n                )\n            else:\n                vector_results = self.vector_retriever.invoke(state[\"query\"])\n            \n            state[\"vector_results\"] = vector_results\n            state[\"metadata\"][\"vector_count\"] = len(vector_results)\n            \n            return state\n            \n        except Exception as e:\n            self.logger.error(f\"Vector search failed: {e}\")\n            state[\"vector_results\"] = []\n            return state\n    \n    async def _fusion_node(self, state: SearchState) -> SearchState:\n        \"\"\"Reciprocal Rank Fusion with enhanced scoring.\"\"\"\n        try:\n            bm25_results = state[\"bm25_results\"]\n            vector_results = state[\"vector_results\"]\n            \n            # Enhanced RRF with document deduplication\n            doc_scores = {}\n            doc_objects = {}\n            \n            # Process BM25 results\n            for rank, doc in enumerate(bm25_results):\n                doc_key = self._get_doc_key(doc)\n                rrf_score = self.config.bm25_weight / (rank + self.config.rrf_k)\n                \n                if doc_key not in doc_scores:\n                    doc_scores[doc_key] = 0\n                    doc_objects[doc_key] = doc\n                \n                doc_scores[doc_key] += rrf_score\n            \n            # Process vector results\n            for rank, doc in enumerate(vector_results):\n                doc_key = self._get_doc_key(doc)\n                rrf_score = self.config.vector_weight / (rank + self.config.rrf_k)\n                \n                if doc_key not in doc_scores:\n                    doc_scores[doc_key] = 0\n                    doc_objects[doc_key] = doc\n                \n                doc_scores[doc_key] += rrf_score\n            \n            # Sort and create fused results\n            sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n            \n            fused_results = []\n            for doc_key, score in sorted_docs[:self.config.max_results * 2]:\n                doc = doc_objects[doc_key]\n                # Add fusion score to metadata\n                if not hasattr(doc, 'metadata'):\n                    doc.metadata = {}\n                doc.metadata['fusion_score'] = score\n                fused_results.append(doc)\n            \n            state[\"fused_results\"] = fused_results\n            state[\"metadata\"][\"fusion_count\"] = len(fused_results)\n            \n            return state\n            \n        except Exception as e:\n            self.logger.error(f\"Fusion failed: {e}\")\n            # Fallback to BM25 results\n            state[\"fused_results\"] = state[\"bm25_results\"][:self.config.max_results]\n            return state\n    \n    async def _rerank_node(self, state: SearchState) -> SearchState:\n        \"\"\"Final reranking and result limitation.\"\"\"\n        try:\n            fused_results = state[\"fused_results\"]\n            \n            # Simple relevance-based reranking (can be enhanced with cross-encoders)\n            final_results = fused_results[:self.config.max_results]\n            \n            # Add ranking metadata\n            for rank, doc in enumerate(final_results):\n                if not hasattr(doc, 'metadata'):\n                    doc.metadata = {}\n                doc.metadata['final_rank'] = rank + 1\n                doc.metadata['search_timestamp'] = datetime.now().isoformat()\n            \n            state[\"final_results\"] = final_results\n            state[\"metadata\"][\"final_count\"] = len(final_results)\n            \n            return state\n            \n        except Exception as e:\n            self.logger.error(f\"Reranking failed: {e}\")\n            state[\"final_results\"] = state[\"fused_results\"][:self.config.max_results]\n            return state\n    \n    def _get_doc_key(self, doc: Document) -> str:\n        \"\"\"Generate unique key for document deduplication.\"\"\"\n        # Use first 200 characters of content as key\n        content_key = doc.page_content[:200]\n        # Add metadata source if available\n        if hasattr(doc, 'metadata') and 'source' in doc.metadata:\n            content_key += f\"__{doc.metadata['source']}\"\n        return content_key\n    \n    def _get_relevant_documents(\n        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n    ) -> List[Document]:\n        \"\"\"Synchronous retrieval method.\"\"\"\n        try:\n            # Initialize search state\n            initial_state = SearchState(\n                query=query,\n                bm25_results=[],\n                vector_results=[],\n                fused_results=[],\n                final_results=[],\n                metadata={\"search_start\": datetime.now().isoformat()}\n            )\n            \n            # Run workflow synchronously\n            result_state = self.workflow.invoke(initial_state)\n            \n            # Return final results\n            return result_state[\"final_results\"]\n            \n        except Exception as e:\n            self.logger.error(f\"Synchronous search failed: {e}\")\n            run_manager.on_retriever_error(e)\n            return []\n    \n    async def _aget_relevant_documents(\n        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n    ) -> List[Document]:\n        \"\"\"Asynchronous retrieval method (LangChain v1.0 feature).\"\"\"\n        try:\n            # Initialize search state\n            initial_state = SearchState(\n                query=query,\n                bm25_results=[],\n                vector_results=[],\n                fused_results=[],\n                final_results=[],\n                metadata={\"search_start\": datetime.now().isoformat()}\n            )\n            \n            # Run workflow asynchronously\n            result_state = await self.workflow.ainvoke(initial_state)\n            \n            # Return final results\n            return result_state[\"final_results\"]\n            \n        except Exception as e:\n            self.logger.error(f\"Asynchronous search failed: {e}\")\n            run_manager.on_retriever_error(e)\n            return []\n\n# Usage example with LangChain v1.0 patterns\nasync def demonstrate_langchain_v1_hybrid_search():\n    \"\"\"Demonstrate LangChain v1.0 hybrid search capabilities.\"\"\"\n    \n    # Sample documents\n    documents = [\n        Document(page_content=\"LangChain v1.0 introduces enhanced async support and better error handling.\",\n                metadata={\"source\": \"langchain_docs\", \"topic\": \"updates\"}),\n        Document(page_content=\"Hybrid search combines keyword and semantic search for better results.\",\n                metadata={\"source\": \"search_guide\", \"topic\": \"methodology\"}),\n        Document(page_content=\"Vector databases enable efficient similarity search at scale.\",\n                metadata={\"source\": \"vector_db_guide\", \"topic\": \"technology\"})\n    ]\n    \n    # Initialize embeddings\n    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n    \n    # Create hybrid retriever with v1.0 features\n    config = HybridSearchConfig(\n        bm25_weight=0.4,\n        vector_weight=0.6,\n        max_results=5,\n        enable_async=True\n    )\n    \n    retriever = LangChainV1HybridRetriever(\n        documents=documents,\n        embeddings=embeddings,\n        config=config\n    )\n    \n    # Perform async search\n    query = \"LangChain async hybrid search\"\n    results = await retriever.ainvoke(query)\n    \n    # Display results\n    print(f\"Found {len(results)} results for query: '{query}'\")\n    for i, doc in enumerate(results, 1):\n        print(f\"\\nResult {i}:\")\n        print(f\"Content: {doc.page_content}\")\n        print(f\"Metadata: {doc.metadata}\")\n    \n    return results\n\nif __name__ == \"__main__\":\n    # Run the demonstration\n    asyncio.run(demonstrate_langchain_v1_hybrid_search())", "language": "python"}, {"title": "Custom RRF Implementation with Reranking", "description": "Advanced hybrid search with custom RRF and cross-encoder reranking", "code": "from sentence_transformers import SentenceTransformer, CrossEncoder\nfrom rank_bm25 import BM25Okapi\nimport numpy as np\nfrom typing import List, Tuple, Dict\nimport faiss\nfrom dataclasses import dataclass\n\n@dataclass\nclass SearchResult:\n    document: str\n    score: float\n    rank: int\n    method: str\n    metadata: Dict = None\n\nclass AdvancedHybridSearch:\n    def __init__(self, documents: List[str], \n                 embedding_model: str = \"all-MiniLM-L6-v2\",\n                 reranker_model: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"):\n        self.documents = documents\n        \n        # Initialize models\n        self.encoder = SentenceTransformer(embedding_model)\n        self.reranker = CrossEncoder(reranker_model)\n        \n        # Preprocess documents\n        self.tokenized_docs = [doc.lower().split() for doc in documents]\n        self.bm25 = BM25Okapi(self.tokenized_docs)\n        \n        # Create vector index\n        self.embeddings = self.encoder.encode(documents)\n        self.vector_index = self._create_faiss_index()\n    \n    def _create_faiss_index(self) -> faiss.Index:\n        \"\"\"Create FAISS index for vector search\"\"\"\n        dimension = self.embeddings.shape[1]\n        index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity\n        \n        # Normalize embeddings for cosine similarity\n        faiss.normalize_L2(self.embeddings)\n        index.add(self.embeddings.astype('float32'))\n        return index\n    \n    def bm25_search(self, query: str, k: int = 10) -> List[SearchResult]:\n        \"\"\"Perform BM25 keyword search\"\"\"\n        query_tokens = query.lower().split()\n        scores = self.bm25.get_scores(query_tokens)\n        \n        # Get top k documents\n        top_indices = np.argsort(scores)[::-1][:k]\n        \n        results = []\n        for rank, idx in enumerate(top_indices):\n            results.append(SearchResult(\n                document=self.documents[idx],\n                score=float(scores[idx]),\n                rank=rank + 1,\n                method=\"bm25\",\n                metadata={\"token_matches\": len(set(query_tokens) & set(self.tokenized_docs[idx]))}\n            ))\n        \n        return results\n    \n    def vector_search(self, query: str, k: int = 10) -> List[SearchResult]:\n        \"\"\"Perform dense vector search\"\"\"\n        query_embedding = self.encoder.encode([query])\n        faiss.normalize_L2(query_embedding)\n        \n        # Search in FAISS index\n        scores, indices = self.vector_index.search(query_embedding.astype('float32'), k)\n        \n        results = []\n        for rank, (idx, score) in enumerate(zip(indices[0], scores[0])):\n            results.append(SearchResult(\n                document=self.documents[idx],\n                score=float(score),\n                rank=rank + 1,\n                method=\"vector\",\n                metadata={\"cosine_similarity\": float(score)}\n            ))\n        \n        return results\n    \n    def reciprocal_rank_fusion(self, bm25_results: List[SearchResult], \n                              vector_results: List[SearchResult], \n                              k: int = 60) -> List[SearchResult]:\n        \"\"\"Apply Reciprocal Rank Fusion to combine results\"\"\"\n        # Create document score mapping\n        doc_scores = {}\n        doc_methods = {}\n        \n        # Process BM25 results\n        for result in bm25_results:\n            rrf_score = 1.0 / (result.rank + k)\n            if result.document not in doc_scores:\n                doc_scores[result.document] = 0\n                doc_methods[result.document] = set()\n            \n            doc_scores[result.document] += rrf_score\n            doc_methods[result.document].add(\"bm25\")\n        \n        # Process vector results\n        for result in vector_results:\n            rrf_score = 1.0 / (result.rank + k)\n            if result.document not in doc_scores:\n                doc_scores[result.document] = 0\n                doc_methods[result.document] = set()\n            \n            doc_scores[result.document] += rrf_score\n            doc_methods[result.document].add(\"vector\")\n        \n        # Sort by combined score\n        sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n        \n        # Create fused results\n        fused_results = []\n        for rank, (doc, score) in enumerate(sorted_docs):\n            fused_results.append(SearchResult(\n                document=doc,\n                score=score,\n                rank=rank + 1,\n                method=\"rrf\",\n                metadata={\n                    \"contributing_methods\": list(doc_methods[doc]),\n                    \"method_count\": len(doc_methods[doc])\n                }\n            ))\n        \n        return fused_results\n    \n    def rerank_results(self, query: str, results: List[SearchResult], \n                      top_k: int = 5) -> List[SearchResult]:\n        \"\"\"Apply cross-encoder reranking to improve results\"\"\"\n        if len(results) == 0:\n            return results\n        \n        # Prepare query-document pairs\n        pairs = [(query, result.document) for result in results]\n        \n        # Get reranking scores\n        rerank_scores = self.reranker.predict(pairs)\n        \n        # Update results with reranking scores\n        reranked_results = []\n        for i, result in enumerate(results):\n            reranked_result = SearchResult(\n                document=result.document,\n                score=float(rerank_scores[i]),\n                rank=i + 1,  # Will be updated after sorting\n                method=f\"{result.method}_reranked\",\n                metadata={\n                    **result.metadata,\n                    \"original_score\": result.score,\n                    \"rerank_score\": float(rerank_scores[i])\n                }\n            )\n            reranked_results.append(reranked_result)\n        \n        # Sort by reranking score\n        reranked_results.sort(key=lambda x: x.score, reverse=True)\n        \n        # Update ranks\n        for rank, result in enumerate(reranked_results[:top_k]):\n            result.rank = rank + 1\n        \n        return reranked_results[:top_k]\n    \n    def hybrid_search(self, query: str, k: int = 10, rerank: bool = True,\n                     rrf_k: int = 60) -> List[SearchResult]:\n        \"\"\"Complete hybrid search pipeline\"\"\"\n        # Get initial results\n        bm25_results = self.bm25_search(query, k * 2)  # Get more for fusion\n        vector_results = self.vector_search(query, k * 2)\n        \n        # Apply RRF\n        fused_results = self.reciprocal_rank_fusion(bm25_results, vector_results, rrf_k)\n        \n        # Apply reranking if requested\n        if rerank:\n            final_results = self.rerank_results(query, fused_results[:k*2], k)\n        else:\n            final_results = fused_results[:k]\n        \n        return final_results\n    \n    def evaluate_search_quality(self, query: str, relevant_docs: List[str], \n                               k: int = 10) -> Dict[str, float]:\n        \"\"\"Evaluate search quality using standard metrics\"\"\"\n        results = self.hybrid_search(query, k)\n        retrieved_docs = [result.document for result in results]\n        \n        # Calculate metrics\n        relevant_retrieved = len(set(retrieved_docs) & set(relevant_docs))\n        precision = relevant_retrieved / len(retrieved_docs) if retrieved_docs else 0\n        recall = relevant_retrieved / len(relevant_docs) if relevant_docs else 0\n        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n        \n        # Calculate MRR (Mean Reciprocal Rank)\n        mrr = 0\n        for i, doc in enumerate(retrieved_docs):\n            if doc in relevant_docs:\n                mrr = 1.0 / (i + 1)\n                break\n        \n        return {\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1_score\": f1,\n            \"mrr\": mrr,\n            \"retrieved_count\": len(retrieved_docs),\n            \"relevant_count\": len(relevant_docs)\n        }", "language": "python"}], "production_code_examples": [{"title": "Enterprise Hybrid Search Service", "description": "Production-ready hybrid search service with monitoring, caching, and error handling", "code": "import asyncio\nimport logging\nimport time\nfrom typing import List, Dict, Any, Optional, Tuple\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime, timedelta\nfrom contextlib import asynccontextmanager\nimport hashlib\nimport json\nfrom enum import Enum\n\nfrom sentence_transformers import SentenceTransformer, CrossEncoder\nfrom rank_bm25 import BM25Okapi\nimport numpy as np\nimport redis.asyncio as redis\nfrom prometheus_client import Counter, Histogram, Gauge\nimport asyncpg\nfrom concurrent.futures import ThreadPoolExecutor\n\n# Monitoring metrics\nhybrid_searches_total = Counter('hybrid_searches_total', 'Total hybrid searches', ['method', 'status'])\nhybrid_search_duration = Histogram('hybrid_search_duration_seconds', 'Hybrid search duration', ['stage'])\nhybrid_cache_hits = Counter('hybrid_cache_hits_total', 'Cache hits for hybrid search')\nhybrid_active_requests = Gauge('hybrid_active_requests', 'Active hybrid search requests')\n\nclass SearchMethod(Enum):\n    BM25 = \"bm25\"\n    VECTOR = \"vector\"\n    HYBRID = \"hybrid\"\n    RERANKED = \"reranked\"\n\n@dataclass\nclass HybridSearchConfig:\n    \"\"\"Configuration for hybrid search service.\"\"\"\n    embedding_model: str = \"all-MiniLM-L6-v2\"\n    reranker_model: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n    cache_ttl: int = 3600  # 1 hour\n    max_concurrent_requests: int = 100\n    request_timeout: int = 30\n    bm25_weight: float = 0.3\n    vector_weight: float = 0.7\n    rrf_k: int = 60\n    enable_reranking: bool = True\n    rerank_top_k: int = 20\n    max_search_results: int = 100\n    min_query_length: int = 2\n    max_query_length: int = 1000\n\n@dataclass\nclass SearchResult:\n    \"\"\"Individual search result with metadata.\"\"\"\n    document_id: str\n    content: str\n    score: float\n    rank: int\n    method: SearchMethod\n    metadata: Dict[str, Any]\n    timestamp: datetime\n\nclass ProductionHybridSearchService:\n    \"\"\"Enterprise-grade hybrid search service with comprehensive error handling and monitoring.\"\"\"\n    \n    def __init__(self, config: HybridSearchConfig, \n                 db_pool: asyncpg.Pool,\n                 redis_client: redis.Redis):\n        self.config = config\n        self.db_pool = db_pool\n        self.redis = redis_client\n        self.logger = logging.getLogger(__name__)\n        \n        # Thread pool for CPU-intensive operations\n        self.thread_pool = ThreadPoolExecutor(max_workers=4)\n        \n        # Model initialization (lazy loading)\n        self._encoder = None\n        self._reranker = None\n        self._bm25_index = None\n        \n        # Circuit breaker state\n        self.circuit_breaker_failures = 0\n        self.circuit_breaker_threshold = 5\n        self.circuit_breaker_reset_time = timedelta(minutes=5)\n        self.last_failure_time = None\n        \n        # Rate limiting\n        self.active_requests = 0\n        self.request_semaphore = asyncio.Semaphore(config.max_concurrent_requests)\n    \n    async def initialize(self) -> None:\n        \"\"\"Initialize models and indexes asynchronously.\"\"\"\n        try:\n            self.logger.info(\"Initializing hybrid search service...\")\n            \n            # Initialize models in thread pool to avoid blocking\n            loop = asyncio.get_event_loop()\n            \n            # Load sentence transformer\n            self._encoder = await loop.run_in_executor(\n                self.thread_pool, \n                SentenceTransformer, \n                self.config.embedding_model\n            )\n            \n            # Load reranker if enabled\n            if self.config.enable_reranking:\n                self._reranker = await loop.run_in_executor(\n                    self.thread_pool,\n                    CrossEncoder,\n                    self.config.reranker_model\n                )\n            \n            # Build BM25 index from database\n            await self._build_bm25_index()\n            \n            self.logger.info(\"Hybrid search service initialized successfully\")\n            \n        except Exception as e:\n            self.logger.error(f\"Failed to initialize hybrid search service: {e}\")\n            raise\n    \n    async def search(self, query: str, limit: int = 10, \n                    method: SearchMethod = SearchMethod.HYBRID) -> List[SearchResult]:\n        \"\"\"Main search endpoint with comprehensive error handling.\"\"\"\n        \n        async with self.request_semaphore:\n            try:\n                # Input validation\n                self._validate_query(query, limit)\n                \n                # Check circuit breaker\n                if self._is_circuit_open():\n                    raise RuntimeError(\"Search service temporarily unavailable\")\n                \n                # Execute search based on method\n                if method == SearchMethod.BM25:\n                    results = await self._bm25_search(query, limit)\n                elif method == SearchMethod.VECTOR:\n                    results = await self._vector_search(query, limit)\n                else:\n                    results = await self._hybrid_search(query, limit)\n                \n                # Reset circuit breaker on success\n                self.circuit_breaker_failures = 0\n                \n                hybrid_searches_total.labels(method=method.value, status='success').inc()\n                return results\n                \n            except Exception as e:\n                self._handle_search_error(e)\n                hybrid_searches_total.labels(method=method.value, status='error').inc()\n                raise\n    \n    def _validate_query(self, query: str, limit: int) -> None:\n        \"\"\"Validate search query and parameters.\"\"\"\n        if not query or not query.strip():\n            raise ValueError(\"Query cannot be empty\")\n        \n        if len(query) < self.config.min_query_length:\n            raise ValueError(f\"Query too short (minimum {self.config.min_query_length} characters)\")\n        \n        if len(query) > self.config.max_query_length:\n            raise ValueError(f\"Query too long (maximum {self.config.max_query_length} characters)\")\n        \n        if limit <= 0 or limit > self.config.max_search_results:\n            raise ValueError(f\"Limit must be between 1 and {self.config.max_search_results}\")\n    \n    def _is_circuit_open(self) -> bool:\n        \"\"\"Check if circuit breaker is open.\"\"\"\n        if self.circuit_breaker_failures < self.circuit_breaker_threshold:\n            return False\n            \n        if self.last_failure_time:\n            time_since_failure = datetime.now() - self.last_failure_time\n            if time_since_failure > self.circuit_breaker_reset_time:\n                self.circuit_breaker_failures = 0\n                return False\n                \n        return True\n    \n    def _handle_search_error(self, exception: Exception) -> None:\n        \"\"\"Handle search error for circuit breaker logic.\"\"\"\n        self.circuit_breaker_failures += 1\n        self.last_failure_time = datetime.now()\n        \n        self.logger.error(f\"Search error #{self.circuit_breaker_failures}: {exception}\")\n        \n        if self.circuit_breaker_failures >= self.circuit_breaker_threshold:\n            self.logger.error(\"Circuit breaker opened due to repeated failures\")", "language": "python"}], "deployment_configurations": [{"environment": "production", "title": "Production Hybrid Search Deployment", "description": "Complete production deployment configuration for hybrid search service", "config": {"requirements.txt": "fastapi==0.104.1\nuvicorn[standard]==0.24.0\nasyncpg==0.29.0\nredis[hiredis]==5.0.1\nsentence-transformers==2.2.2\nrank-bm25==0.2.2\nnumpy==1.24.3\nprometheus-client==0.19.0\npydantic==2.5.0\npydantic-settings==2.1.0\naiofiles==23.2.1\nloguru==0.7.2\ntenacity==8.2.3", "docker-compose.yml": "version: '3.8'\n\nservices:\n  hybrid-search-api:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    environment:\n      - DATABASE_URL=postgresql://user:pass@postgres:5432/vector_rag\n      - REDIS_URL=redis://redis:6379/0\n      - EMBEDDING_MODEL=all-MiniLM-L6-v2\n      - RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2\n      - MAX_CONCURRENT_REQUESTS=100\n      - CACHE_TTL=3600\n      - LOG_LEVEL=INFO\n    ports:\n      - \"8000:8000\"\n    depends_on:\n      - postgres\n      - redis\n    deploy:\n      replicas: 3\n      resources:\n        limits:\n          memory: 4G\n          cpus: '2'\n        reservations:\n          memory: 2G\n          cpus: '1'\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n      start_period: 60s", "Dockerfile": "FROM python:3.11-slim\n\n# Install system dependencies\nRUN apt-get update && apt-get install -y \\\n    gcc \\\n    g++ \\\n    curl \\\n    && rm -rf /var/lib/apt/lists/*\n\n# Set working directory\nWORKDIR /app\n\n# Install Python dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy application code\nCOPY src/ ./src/\nCOPY config/ ./config/\n\n# Create non-root user\nRUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app\nUSER appuser\n\n# Expose port\nEXPOSE 8000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \\\n  CMD curl -f http://localhost:8000/health || exit 1\n\n# Run application\nCMD [\"python\", \"-m\", \"uvicorn\", \"src.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]"}}], "performance_benchmarks": [{"metric": "Hybrid vs Pure Search Methods", "baseline": "Pure BM25: 0.65 MRR, Pure Vector: 0.72 MRR", "optimized": "Hybrid Search: 0.84 MRR (\u03b1=0.7)", "improvement_factor": "15-29% improvement over individual methods"}, {"metric": "Reciprocal Rank Fusion Impact", "baseline": "Simple score averaging: 0.78 MRR", "optimized": "RRF with k=60: 0.84 MRR", "improvement_factor": "7.7% improvement in ranking quality"}, {"metric": "Reranking Performance Boost", "baseline": "Hybrid search without reranking: 0.84 MRR", "optimized": "Hybrid + Cross-encoder reranking: 0.91 MRR", "improvement_factor": "8.3% improvement with reranking stage"}, {"metric": "Query Latency", "bm25_only": "~10ms for 1M documents", "vector_only": "~50ms for 1M documents (HNSW index)", "hybrid_search": "~75ms for full pipeline including reranking", "scalability": "Linear scaling with document count for BM25, sub-linear for vector search"}], "security_guidelines": [{"category": "query_injection", "title": "Search Query Sanitization", "description": "Sanitize user queries to prevent injection attacks on both keyword and vector search components", "risk_level": "medium", "mitigation": "Implement query length limits, special character filtering, and rate limiting"}, {"category": "data_exposure", "title": "Sensitive Document Protection", "description": "Ensure search indices don't expose sensitive information through similarity or keyword matching", "risk_level": "high", "mitigation": "Implement document-level access controls and result filtering based on user permissions"}, {"category": "model_security", "title": "Embedding Model Protection", "description": "Secure embedding models and vector indices from adversarial attacks and extraction", "risk_level": "medium", "mitigation": "Use model encryption, input validation, and monitoring for unusual query patterns"}], "troubleshooting": [{"issue": "Hybrid search returning poor results with low relevance scores", "symptoms": ["Low MRR scores <0.6", "Relevant documents not appearing in top 10", "Alpha parameter not improving results", "RRF fusion producing worse results than individual methods"], "root_causes": ["Mismatched embedding model and query domain", "Improper BM25 preprocessing (stemming, stop words)", "Incorrect alpha weighting for dataset", "Insufficient document diversity in index"], "solutions": ["Fine-tune embedding model on domain-specific data", "Optimize BM25 preprocessing pipeline", "Use grid search to find optimal alpha (0.1-0.9 range)", "Implement proper document preprocessing and cleaning"], "verification": "A/B test with different alpha values and measure MRR, NDCG@10 improvements", "references": ["https://arxiv.org/abs/2104.05278", "https://github.com/beir-cellar/beir"]}, {"issue": "Vector search component causing severe latency in hybrid pipeline", "symptoms": ["Query latency >5 seconds", "High CPU usage during vector search", "Memory usage growing over time", "FAISS index searches timing out"], "root_causes": ["Inefficient vector index type (Flat vs HNSW)", "High dimensional embeddings without compression", "Large vector index not fitting in memory", "Missing query result caching"], "solutions": ["Switch to HNSW index with optimized M and efConstruction parameters", "Implement binary quantization or PQ compression", "Use distributed vector databases (Qdrant, Weaviate)", "Add Redis caching for frequent queries"], "verification": "Benchmark query latency with different index types and measure p95 <200ms", "references": ["https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index", "https://qdrant.tech/documentation/guides/optimization/"]}, {"issue": "BM25 scores dominating vector scores in RRF fusion", "symptoms": ["Results heavily biased toward keyword matches", "Semantic similarity ignored in rankings", "RRF producing same results as pure BM25", "Alpha parameter having minimal effect"], "root_causes": ["BM25 scores much higher magnitude than vector scores", "Improper score normalization before fusion", "RRF constant k parameter too low", "Vector similarity scores not converted properly"], "solutions": ["Normalize scores to [0,1] range before RRF", "Use rank-based fusion instead of score-based", "Increase RRF k parameter to 100-200", "Apply min-max normalization on both score types"], "verification": "Compare score distributions and ensure balanced contribution from both methods", "references": ["https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf", "https://arxiv.org/abs/2210.11934"]}, {"issue": "Cross-encoder reranking causing timeout errors in production", "symptoms": ["Request timeouts >30 seconds", "High GPU memory usage", "Reranker model inference errors", "OOM errors during batch processing"], "root_causes": ["Large batch sizes overwhelming GPU memory", "Long document content exceeding model max length", "Inefficient model loading for each request", "No model optimization or quantization"], "solutions": ["Implement dynamic batching with size limits", "Truncate documents to 512 tokens max", "Use model caching and persistent GPU loading", "Apply ONNX optimization or TensorRT acceleration"], "verification": "Load test reranking pipeline and ensure <500ms p99 latency", "references": ["https://huggingface.co/docs/transformers/main_classes/onnx", "https://github.com/microsoft/onnxruntime"]}], "common_pitfalls": [{"mistake": "Using default alpha=0.5 without dataset-specific optimization", "consequences": "Suboptimal search quality, missing relevant results, poor user experience", "prevention": "Always tune alpha parameter using evaluation dataset with relevance judgments", "recovery": "Implement A/B testing framework to find optimal alpha for your specific use case"}, {"mistake": "Not implementing proper score normalization before fusion", "consequences": "One search method dominating results, ineffective hybrid search", "prevention": "Normalize BM25 and vector scores to same range before applying RRF or weighted fusion", "recovery": "Add score normalization layer and re-evaluate fusion effectiveness"}, {"mistake": "Ignoring document preprocessing consistency between BM25 and vector components", "consequences": "Misaligned search results, inconsistent relevance scoring", "prevention": "Ensure same preprocessing (tokenization, cleaning) for both BM25 and embedding generation", "recovery": "Rebuild both indices with consistent preprocessing pipeline"}, {"mistake": "Not caching expensive operations in production", "consequences": "High latency, unnecessary compute costs, poor scalability", "prevention": "Cache embeddings, BM25 scores, and reranking results for frequent queries", "recovery": "Implement Redis or in-memory caching with appropriate TTL policies"}], "latest_trends_2025": [{"trend": "BGE-M3 Reranker and Multi-Vector Dense Models", "release_date": "2025-06-01", "key_features": ["Multi-lingual and multi-granularity support", "Unified dense, sparse, and multi-vector retrieval", "State-of-the-art reranking performance", "Efficient inference optimization"], "migration_notes": "Upgrade from older cross-encoders to BGE-M3 for 15-20% relevance improvements"}, {"trend": "Cohere Rerank 3.0 with Enhanced Multilingual Support", "release_date": "2025-07-15", "key_features": ["100+ language support", "Context length up to 4K tokens", "Specialized models for different domains", "API-based and on-premise deployment"], "adoption_status": "Production-ready with enterprise SLA guarantees"}, {"trend": "Learned Sparse Retrieval Integration", "description": "SPLADE v3 and other learned sparse methods replacing traditional BM25", "use_cases": ["Domain-specific sparse retrieval", "Cross-lingual keyword search", "Better handling of synonyms and concepts", "Improved rare term matching"], "implementation_example": "SPLADE v3 embeddings as drop-in replacement for BM25 in hybrid pipelines"}, {"trend": "Vector Database Native Hybrid Search", "description": "Built-in hybrid search capabilities in vector databases eliminating custom fusion logic", "use_cases": ["Simplified architecture", "Optimized fusion algorithms", "Real-time index updates", "Enterprise-grade scalability"], "adoption_status": "Available in Qdrant 1.8+, Weaviate 1.21+, Pinecone hybrid namespaces"}], "production_patterns": [{"scenario": "E-commerce product search with 10M+ products", "scale": "10M+ products, 100k+ daily searches, real-time inventory updates", "architecture": "Elasticsearch hybrid search with HNSW vectors, Redis caching, BGE-M3 reranker, dynamic alpha based on query type", "performance_metrics": {"latency_p50": "45ms end-to-end search", "latency_p99": "180ms including reranking", "relevance_mrr": "0.89 (15% improvement over pure methods)", "cache_hit_rate": "78% for product embeddings"}, "lessons_learned": ["Query intent classification improves alpha tuning", "Product image embeddings boost fashion search quality", "Real-time inventory filtering essential for relevance"], "monitoring_setup": "Custom relevance metrics, A/B testing framework, query latency percentiles, cache performance tracking"}, {"scenario": "Legal document search with 1M+ documents and strict compliance", "scale": "1M+ legal documents, multilingual content, audit trail requirements", "architecture": "Weaviate hybrid search with document-level permissions, specialized legal reranker, encrypted vector storage", "performance_metrics": {"latency_p50": "120ms with permission filtering", "latency_p99": "400ms for complex queries", "relevance_ndcg": "0.92 for legal-specific evaluation", "compliance_audit": "100% query logging and access tracking"}, "lessons_learned": ["Legal domain embedding models crucial for accuracy", "Citation extraction improves relevance", "Multi-level access controls require careful optimization"], "monitoring_setup": "Compliance dashboards, search quality metrics per legal domain, user access patterns, data lineage tracking"}], "scaling_strategies": [{"from_scale": "10K documents, 100 queries/day", "to_scale": "1M documents, 10K queries/day", "changes_required": ["Implement vector database with HNSW indexing", "Add BM25 index with Elasticsearch or Solr", "Implement basic fusion scoring (RRF or weighted)", "Add result caching with Redis", "Implement query preprocessing and normalization"], "cost_implications": "Infrastructure costs 5-6x, need vector DB and search engine", "timeline": "4-5 weeks implementation", "performance_impact": {"search_latency": "Reduced from 2s to 200ms average", "relevance_score": "20-30% improvement with hybrid approach", "cache_hit_rate": "60% for common queries"}}, {"from_scale": "1M documents, 10K queries/day", "to_scale": "100M documents, 1M queries/day", "changes_required": ["Deploy distributed vector database cluster", "Implement advanced reranking with transformer models", "Add query intent classification for dynamic alpha tuning", "Implement streaming indexing for real-time updates", "Add multi-stage retrieval pipeline", "Implement learned sparse retrieval (SPLADE)"], "cost_implications": "Infrastructure costs 25-30x, requires GPU for reranking", "timeline": "8-12 weeks implementation", "performance_impact": {"concurrent_queries": "10K+ simultaneous search requests", "reranking_quality": "15-20% relevance improvement", "index_update_latency": "<5 minutes for new documents"}}, {"from_scale": "100M documents, 1M queries/day", "to_scale": "10B+ documents, 100M queries/day", "changes_required": ["Implement federated search across multiple databases", "Add global edge caching for search results", "Implement advanced learning-to-rank systems", "Add AI-powered query understanding and expansion", "Implement cross-modal search (text, image, audio)", "Add personalized search with user behavior modeling"], "cost_implications": "Enterprise-scale infrastructure, 100x+ cost with optimization", "timeline": "6-12 months implementation", "performance_impact": {"global_latency": "<50ms search response worldwide", "personalization": "40% improvement in user-specific relevance", "cross_modal_search": "Unified search across all content types"}}], "expanded_production_patterns": [{"scenario": "Enterprise Knowledge Search Platform", "scale": "50M+ documents across 1000+ departments", "architecture": "Federated hybrid search with department-specific models and access controls", "performance_metrics": {"search_latency_p50": "80ms", "search_latency_p99": "300ms", "relevance_mrr": "0.91", "department_isolation": "100% access control compliance"}, "lessons_learned": ["Department-specific embedding models improve domain relevance", "Access control filtering must be applied at index level", "Cross-department search requires careful permission aggregation", "User role-based result ranking improves productivity"], "monitoring_setup": "Department-specific analytics with compliance audit trails"}, {"scenario": "Real-time News and Social Media Search", "scale": "1M+ new articles/posts daily, trending topic detection", "architecture": "Streaming hybrid search with real-time indexing and trend analysis", "performance_metrics": {"indexing_latency": "<30s for new content", "trending_detection": "<5 minutes for emerging topics", "search_freshness": "90% results from last 24 hours", "multilingual_coverage": "50+ languages supported"}, "lessons_learned": ["Real-time indexing essential for news relevance", "Trend detection requires temporal weighting in scoring", "Multilingual models needed for global content", "Social signals improve ranking for viral content"], "monitoring_setup": "Real-time trend analytics with content freshness tracking"}, {"scenario": "Scientific Literature Discovery Platform", "scale": "100M+ research papers with citation networks", "architecture": "Citation-enhanced hybrid search with specialized scientific models", "performance_metrics": {"citation_relevance": "0.94 scientific relevance score", "cross_discipline_search": "30% improvement in interdisciplinary discovery", "paper_recommendation": "85% researcher satisfaction", "search_precision": "0.89 for complex scientific queries"}, "lessons_learned": ["Citation graphs significantly improve scientific relevance", "Domain-specific scientific embeddings outperform general models", "Temporal weighting important for recent vs foundational papers", "Author network analysis improves recommendation quality"], "monitoring_setup": "Scientific impact metrics with researcher behavior analytics"}, {"scenario": "Multi-modal E-learning Content Search", "scale": "10M+ videos, documents, and interactive content pieces", "architecture": "Cross-modal hybrid search with content understanding and learning path optimization", "performance_metrics": {"video_search_accuracy": "0.87 for spoken content matching", "learning_path_relevance": "0.92 for skill-based progression", "content_diversity": "80% coverage of different learning styles", "search_to_completion": "65% course completion rate"}, "lessons_learned": ["Multi-modal embeddings essential for video and interactive content", "Learning context improves long-term content recommendations", "Skill progression modeling enhances search relevance", "Personalized difficulty matching improves engagement"], "monitoring_setup": "Learning analytics with content effectiveness tracking"}, {"scenario": "Healthcare Clinical Decision Support", "scale": "Medical literature, clinical guidelines, and patient case databases", "architecture": "Evidence-based hybrid search with clinical validation and safety scoring", "performance_metrics": {"clinical_relevance": "0.95 for evidence-based recommendations", "safety_scoring": "99.8% accuracy for contraindication detection", "guideline_compliance": "98% adherence to clinical standards", "decision_support_speed": "<2s for complex medical queries"}, "lessons_learned": ["Medical domain embeddings critical for clinical accuracy", "Evidence hierarchy must be reflected in search ranking", "Real-time drug interaction checking essential for safety", "Clinical workflow integration improves adoption"], "monitoring_setup": "Clinical outcome tracking with safety event monitoring"}], "rag_development_scenarios": [{"scenario": "Taxonomy-Aware Hybrid Search Development", "development_phase": "Search Algorithm Development", "collaboration_agents": ["database-architect", "taxonomy-architect"], "development_tasks": ["Design hybrid search algorithm with taxonomy hierarchy weighting", "Build semantic search optimization for taxonomic relationships", "Create dynamic alpha tuning based on taxonomy depth", "Develop search result ranking with taxonomic coherence"], "technical_decisions": {"hybrid_approach": "BM25 + dense embeddings + taxonomy path similarity", "taxonomy_weighting": "Hierarchical weight decay based on taxonomy distance", "alpha_tuning": "Dynamic alpha based on query type and taxonomy specificity", "ranking_algorithm": "Multi-factor ranking with taxonomy coherence score"}, "development_outputs": ["Taxonomy-aware hybrid search library", "Dynamic alpha tuning system", "Hierarchical ranking algorithm", "Search performance optimization tools"]}, {"scenario": "RAG Search Experimentation Framework", "development_phase": "Search Optimization", "collaboration_agents": ["rag-evaluation-specialist", "api-designer"], "development_tasks": ["Build A/B testing framework for search algorithms", "Create search parameter optimization tools", "Design search quality measurement system", "Develop search configuration management"], "technical_decisions": {"experimentation_framework": "Multi-armed bandit for search algorithm selection", "parameter_optimization": "Bayesian optimization for hyperparameter tuning", "quality_measurement": "Custom metrics combining relevance and taxonomy coherence", "configuration_management": "Version-controlled search configurations with rollback"}, "development_outputs": ["Search experimentation platform", "Parameter optimization toolkit", "Search quality assessment system", "Configuration management interface"]}], "cross_agent_development_collaboration": [{"collaboration_type": "Search Architecture Design", "agents": ["hybrid-search-specialist", "database-architect", "taxonomy-architect"], "development_scenario": "Designing optimal search architecture for Dynamic Taxonomy RAG", "workflow": ["Hybrid-search-specialist: Defines search algorithm requirements and performance criteria", "Database-architect: Optimizes data storage and indexing for search performance", "Taxonomy-architect: Ensures search respects taxonomic relationships and hierarchy", "Joint: Develops integrated search system with optimal performance and taxonomy awareness"], "deliverables": ["Taxonomy-aware search architecture", "Search performance optimization strategy", "Hierarchical search algorithm implementation", "Search system integration guidelines"]}]}, "timestamp": 1758336923.257882}}