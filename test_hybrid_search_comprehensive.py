#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ í¬ê´„ì  í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
BM25 + Vector Similarity + Cross-Encoder Reranking í†µí•© í…ŒìŠ¤íŠ¸

í…ŒìŠ¤íŠ¸ í•­ëª©:
1. BM25 ìŠ¤ì½”ì–´ë§ í…ŒìŠ¤íŠ¸ (í•œê¸€ í…ìŠ¤íŠ¸ í¬í•¨)
2. Vector ì„ë² ë”© ë° ìœ ì‚¬ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸
3. Cross-Encoder ë¦¬ë­í‚¹ í…ŒìŠ¤íŠ¸
4. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í†µí•© í…ŒìŠ¤íŠ¸
5. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ê²€ì¦
6. UTF-8 ì¸ì½”ë”© í˜¸í™˜ì„± í™•ì¸
"""

import asyncio
import time
import logging
import json
import traceback
from typing import List, Dict, Any, Tuple
from datetime import datetime
import os
import sys
import uuid

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from apps.api.database import (
    DatabaseManager, BM25Scorer, EmbeddingService, CrossEncoderReranker,
    SearchDAO, SearchMetrics, db_manager, init_database
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('hybrid_search_test.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class HybridSearchTester:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ í¬ê´„ì  í…ŒìŠ¤í„°"""

    def __init__(self):
        self.db_manager = db_manager
        self.test_metrics = SearchMetrics()
        self.test_results = {
            "bm25_tests": {},
            "vector_tests": {},
            "cross_encoder_tests": {},
            "hybrid_tests": {},
            "performance_metrics": {},
            "encoding_tests": {},
            "integration_tests": {}
        }

        # í…ŒìŠ¤íŠ¸ ë°ì´í„° - í•œê¸€ê³¼ ì˜ì–´ í˜¼ì¬
        self.test_queries = [
            "RAG ì‹œìŠ¤í…œ ì„¤ëª…",
            "machine learning algorithms",
            "ë²¡í„° ê²€ìƒ‰ ë°©ë²•",
            "What is retrieval augmented generation?",
            "ì¸ê³µì§€ëŠ¥ ë¶„ë¥˜ ê¸°ë²•",
            "neural network architecture",
            "ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”",
            "embedding model comparison"
        ]

        self.test_documents = [
            {
                "title": "RAG ì‹œìŠ¤í…œ ê°œìš”",
                "text": "RAG(Retrieval-Augmented Generation)ëŠ” ê²€ìƒ‰ ê¸°ë°˜ ìƒì„± ëª¨ë¸ì…ë‹ˆë‹¤. ì´ ì‹œìŠ¤í…œì€ ì™¸ë¶€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤. ë²¡í„° ì„ë² ë”©ê³¼ ìœ ì‚¬ë„ ê³„ì‚°ì„ í†µí•´ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ìŠµë‹ˆë‹¤.",
                "category": ["AI", "RAG"]
            },
            {
                "title": "Machine Learning Classification",
                "text": "Machine learning classification algorithms include Support Vector Machines (SVM), Random Forest, and Neural Networks. These algorithms learn patterns from training data to classify new instances. Feature engineering and model selection are crucial for performance.",
                "category": ["AI", "ML"]
            },
            {
                "title": "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í™œìš©",
                "text": "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” ê³ ì°¨ì› ë²¡í„° ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥í•˜ê³  ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. pgvectorì™€ ê°™ì€ í™•ì¥ì„ í†µí•´ PostgreSQLì—ì„œë„ ë²¡í„° ì—°ì‚°ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì½”ì‚¬ì¸ ìœ ì‚¬ë„, ìœ í´ë¦¬ë“œ ê±°ë¦¬ ë“± ë‹¤ì–‘í•œ ê±°ë¦¬ ì¸¡ì • ë°©ë²•ì„ ì§€ì›í•©ë‹ˆë‹¤.",
                "category": ["AI", "Database"]
            },
            {
                "title": "Cross-Encoder Reranking",
                "text": "Cross-encoder models use BERT-like architectures to score query-document pairs directly. Unlike bi-encoders that create separate embeddings, cross-encoders process both inputs together for more accurate relevance scoring. This approach is commonly used for reranking search results.",
                "category": ["AI", "NLP"]
            },
            {
                "title": "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ",
                "text": "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì€ í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰(BM25)ê³¼ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰(ë²¡í„° ìœ ì‚¬ë„)ì„ ê²°í•©í•©ë‹ˆë‹¤. ê° ë°©ë²•ì˜ ì¥ì ì„ í™œìš©í•˜ì—¬ ë” ì •í™•í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°€ì¤‘ì¹˜ ì¡°ì •ê³¼ ì¬ë­í‚¹ ê³¼ì •ì„ í†µí•´ ìµœì¢… ê²°ê³¼ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤.",
                "category": ["AI", "Search"]
            }
        ]

    async def setup_test_data(self) -> bool:
        """í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ì„¤ì •"""
        try:
            logger.info("í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¤ì • ì‹œì‘")

            # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
            if not await init_database():
                logger.error("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨")
                return False

            async with self.db_manager.async_session() as session:
                # í…ŒìŠ¤íŠ¸ ë¬¸ì„œë“¤ì„ ì‹¤ì œ DBì— ì‚½ì…
                from sqlalchemy import text

                for i, doc_data in enumerate(self.test_documents):
                    # SQLite í˜¸í™˜ ë¬¸ì„œ ì‚½ì… (doc_idë¥¼ ëª…ì‹œì ìœ¼ë¡œ ìƒì„±)
                    new_doc_id = str(uuid.uuid4())

                    doc_insert = text("""
                        INSERT OR IGNORE INTO documents (doc_id, title, content_type, file_size)
                        VALUES (:doc_id, :title, 'text/plain', :file_size)
                    """)
                    await session.execute(doc_insert, {
                        "doc_id": new_doc_id,
                        "title": doc_data["title"],
                        "file_size": len(doc_data["text"])
                    })

                    # ì‹¤ì œë¡œ ì‚½ì…ëœ doc_id í™•ì¸
                    doc_check = await session.execute(
                        text("SELECT doc_id FROM documents WHERE title = :title"),
                        {"title": doc_data["title"]}
                    )
                    doc_id = doc_check.scalar()

                    try:
                        if doc_id:
                            # SQLite í˜¸í™˜ ì²­í¬ ì‚½ì…
                            new_chunk_id = str(uuid.uuid4())
                            chunk_insert = text("""
                                INSERT OR IGNORE INTO chunks (chunk_id, doc_id, text, span, chunk_index)
                                VALUES (:chunk_id, :doc_id, :text, :span, :chunk_index)
                            """)
                            await session.execute(chunk_insert, {
                                "chunk_id": new_chunk_id,
                                "doc_id": doc_id,
                                "text": doc_data["text"],
                                "span": "1,1000",  # SQLiteìš© span í˜•ì‹
                                "chunk_index": i
                            })

                            # ì²­í¬ê°€ ìƒì„±ë˜ì—ˆìœ¼ë©´ ì„ë² ë”© ìƒì„±
                            chunk_check = await session.execute(
                                text("SELECT chunk_id FROM chunks WHERE doc_id = :doc_id AND chunk_index = :chunk_index"),
                                {"doc_id": doc_id, "chunk_index": i}
                            )
                            chunk_id = chunk_check.scalar()

                            if chunk_id:
                                # ì„ë² ë”© ìƒì„± ë° ì‚½ì…
                                embedding = await EmbeddingService.generate_embedding(doc_data["text"])
                                if embedding:
                                    new_embedding_id = str(uuid.uuid4())
                                    embedding_insert = text("""
                                        INSERT OR IGNORE INTO embeddings (embedding_id, chunk_id, vec, model_name)
                                        VALUES (:embedding_id, :chunk_id, :vec, 'text-embedding-ada-002')
                                    """)
                                    await session.execute(embedding_insert, {
                                        "embedding_id": new_embedding_id,
                                        "chunk_id": chunk_id,
                                        "vec": embedding
                                    })

                            # ë¶„ë¥˜ ì •ë³´ ì‚½ì…
                            if doc_data.get("category"):
                                new_mapping_id = str(uuid.uuid4())
                                taxonomy_insert = text("""
                                    INSERT OR IGNORE INTO doc_taxonomy (mapping_id, doc_id, path, confidence, source)
                                    VALUES (:mapping_id, :doc_id, :path, :confidence, 'test')
                                """)
                                await session.execute(taxonomy_insert, {
                                    "mapping_id": new_mapping_id,
                                    "doc_id": doc_id,
                                    "path": json.dumps(doc_data["category"]),  # SQLiteìš© JSON ë¬¸ìì—´
                                    "confidence": 1.0
                                })

                    except Exception as e:
                        logger.warning(f"ë¬¸ì„œ {doc_data['title']} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue

                await session.commit()

            logger.info("í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¤ì • ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¤ì • ì‹¤íŒ¨: {e}")
            logger.error(traceback.format_exc())
            return False

    async def test_bm25_scoring(self) -> Dict[str, Any]:
        """BM25 ìŠ¤ì½”ì–´ë§ í…ŒìŠ¤íŠ¸"""
        logger.info("BM25 ìŠ¤ì½”ì–´ë§ í…ŒìŠ¤íŠ¸ ì‹œì‘")

        results = {
            "success": False,
            "test_cases": [],
            "performance": {},
            "encoding_test": {}
        }

        try:
            # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 1: ê¸°ë³¸ BM25 ìŠ¤ì½”ì–´ ê³„ì‚°
            start_time = time.time()

            # í•œê¸€ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
            korean_text = "RAG ì‹œìŠ¤í…œì€ ê²€ìƒ‰ ê¸°ë°˜ ìƒì„± ëª¨ë¸ì…ë‹ˆë‹¤"
            korean_tokens = BM25Scorer.preprocess_text(korean_text)

            english_text = "Machine learning algorithms include SVM and Random Forest"
            english_tokens = BM25Scorer.preprocess_text(english_text)

            # ì½”í¼ìŠ¤ í†µê³„ (ë”ë¯¸ ë°ì´í„°)
            corpus_stats = {
                "avg_doc_length": 50,
                "total_docs": 100,
                "term_doc_freq": {
                    "rag": 5,
                    "ì‹œìŠ¤í…œ": 10,
                    "machine": 8,
                    "learning": 12,
                    "ê²€ìƒ‰": 7,
                    "ê¸°ë°˜": 15
                }
            }

            # BM25 ìŠ¤ì½”ì–´ ê³„ì‚° í…ŒìŠ¤íŠ¸
            test_cases = [
                {
                    "query": "RAG ì‹œìŠ¤í…œ",
                    "doc_text": korean_text,
                    "expected_score_range": (0.0, 10.0)
                },
                {
                    "query": "machine learning",
                    "doc_text": english_text,
                    "expected_score_range": (0.0, 10.0)
                },
                {
                    "query": "ê²€ìƒ‰ ê¸°ë°˜",
                    "doc_text": korean_text,
                    "expected_score_range": (0.0, 10.0)
                }
            ]

            for i, test_case in enumerate(test_cases):
                query_tokens = BM25Scorer.preprocess_text(test_case["query"])
                doc_tokens = BM25Scorer.preprocess_text(test_case["doc_text"])

                score = BM25Scorer.calculate_bm25_score(
                    query_tokens, doc_tokens, corpus_stats
                )

                min_score, max_score = test_case["expected_score_range"]
                test_success = min_score <= score <= max_score

                results["test_cases"].append({
                    "case_id": i + 1,
                    "query": test_case["query"],
                    "query_tokens": query_tokens,
                    "doc_tokens": doc_tokens[:10],  # ì²˜ìŒ 10ê°œë§Œ ë¡œê¹…
                    "bm25_score": score,
                    "expected_range": test_case["expected_score_range"],
                    "success": test_success
                })

            # ì„±ëŠ¥ ë©”íŠ¸ë¦­
            end_time = time.time()
            results["performance"] = {
                "total_time": end_time - start_time,
                "avg_time_per_case": (end_time - start_time) / len(test_cases),
                "throughput": len(test_cases) / (end_time - start_time)
            }

            # UTF-8 ì¸ì½”ë”© í…ŒìŠ¤íŠ¸
            unicode_texts = [
                "í•œê¸€ í…ìŠ¤íŠ¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸",
                "English text processing test",
                "æ··åˆæ–‡å­—å‡¦ç†ãƒ†ã‚¹ãƒˆ",  # ì¼ë³¸ì–´
                "Ğ¢ĞµÑÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ‚ĞµĞºÑÑ‚Ğ°"  # ëŸ¬ì‹œì•„ì–´
            ]

            encoding_results = []
            for text in unicode_texts:
                try:
                    tokens = BM25Scorer.preprocess_text(text)
                    encoding_results.append({
                        "text": text,
                        "tokens": tokens,
                        "success": True,
                        "token_count": len(tokens)
                    })
                except Exception as e:
                    encoding_results.append({
                        "text": text,
                        "tokens": [],
                        "success": False,
                        "error": str(e)
                    })

            results["encoding_test"] = {
                "test_texts": encoding_results,
                "success_rate": sum(1 for r in encoding_results if r["success"]) / len(encoding_results)
            }

            # ì „ì²´ ì„±ê³µ ì—¬ë¶€
            successful_cases = sum(1 for case in results["test_cases"] if case["success"])
            results["success"] = (successful_cases == len(results["test_cases"]) and
                                results["encoding_test"]["success_rate"] == 1.0)

            self.test_results["bm25_tests"] = results

        except Exception as e:
            logger.error(f"BM25 í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            results["error"] = str(e)
            results["traceback"] = traceback.format_exc()

        logger.info(f"BM25 í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ì„±ê³µ: {results['success']}")
        return results

    async def test_vector_embedding_similarity(self) -> Dict[str, Any]:
        """ë²¡í„° ì„ë² ë”© ë° ìœ ì‚¬ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸"""
        logger.info("ë²¡í„° ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì‹œì‘")

        results = {
            "success": False,
            "embedding_tests": [],
            "similarity_tests": [],
            "performance": {},
            "encoding_test": {}
        }

        try:
            start_time = time.time()

            # ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸
            test_texts = [
                "RAG ì‹œìŠ¤í…œì€ ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•©ë‹ˆë‹¤",
                "Machine learning enables pattern recognition",
                "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤",
                "Neural networks learn complex representations"
            ]

            embeddings = []
            for i, text in enumerate(test_texts):
                embedding_start = time.time()
                embedding = await EmbeddingService.generate_embedding(text)
                embedding_time = time.time() - embedding_start

                # ì„ë² ë”© ê²€ì¦
                is_valid = (
                    isinstance(embedding, list) and
                    len(embedding) > 0 and
                    all(isinstance(x, (int, float)) for x in embedding)
                )

                test_result = {
                    "text": text,
                    "embedding_length": len(embedding),
                    "generation_time": embedding_time,
                    "is_valid": is_valid,
                    "sample_values": embedding[:5] if embedding else []
                }

                results["embedding_tests"].append(test_result)
                if is_valid:
                    embeddings.append(embedding)

            # ìœ ì‚¬ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸
            if len(embeddings) >= 2:
                import numpy as np
                from sklearn.metrics.pairwise import cosine_similarity

                similarity_tests = [
                    # ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ ìŒ
                    (0, 2, "RAG-ë²¡í„°DB ìœ ì‚¬ì„± (í•œê¸€-í•œê¸€)"),
                    (1, 3, "ML-NN ìœ ì‚¬ì„± (ì˜ì–´-ì˜ì–´)"),
                    # ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ìŒ
                    (0, 1, "RAG-ML ì°¨ì´ (í•œê¸€-ì˜ì–´)"),
                    (2, 3, "ë²¡í„°DB-NN ì°¨ì´ (í•œê¸€-ì˜ì–´)")
                ]

                for idx1, idx2, description in similarity_tests:
                    if idx1 < len(embeddings) and idx2 < len(embeddings):
                        emb1 = np.array(embeddings[idx1]).reshape(1, -1)
                        emb2 = np.array(embeddings[idx2]).reshape(1, -1)

                        cosine_sim = cosine_similarity(emb1, emb2)[0][0]
                        euclidean_dist = np.linalg.norm(emb1 - emb2)

                        results["similarity_tests"].append({
                            "description": description,
                            "text1": test_texts[idx1],
                            "text2": test_texts[idx2],
                            "cosine_similarity": float(cosine_sim),
                            "euclidean_distance": float(euclidean_dist),
                            "similarity_valid": -1.0 <= cosine_sim <= 1.0
                        })

            # ì„±ëŠ¥ ë©”íŠ¸ë¦­
            end_time = time.time()
            results["performance"] = {
                "total_time": end_time - start_time,
                "avg_embedding_time": sum(t["generation_time"] for t in results["embedding_tests"]) / len(results["embedding_tests"]),
                "embeddings_per_second": len(test_texts) / (end_time - start_time)
            }

            # UTF-8 ì¸ì½”ë”© í…ŒìŠ¤íŠ¸
            unicode_texts = [
                "í•œêµ­ì–´ ì„ë² ë”© í…ŒìŠ¤íŠ¸ ğŸ‡°ğŸ‡·",
                "English embedding test ğŸ‡ºğŸ‡¸",
                "æ—¥æœ¬èªåŸ‹ã‚è¾¼ã¿ãƒ†ã‚¹ãƒˆ ğŸ‡¯ğŸ‡µ",
                "Ğ ÑƒÑÑĞºĞ¸Ğ¹ Ñ‚ĞµÑÑ‚ Ğ²ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ğŸ‡·ğŸ‡º"
            ]

            encoding_results = []
            for text in unicode_texts:
                try:
                    embedding = await EmbeddingService.generate_embedding(text)
                    encoding_results.append({
                        "text": text,
                        "success": len(embedding) > 0,
                        "embedding_length": len(embedding)
                    })
                except Exception as e:
                    encoding_results.append({
                        "text": text,
                        "success": False,
                        "error": str(e)
                    })

            results["encoding_test"] = {
                "test_results": encoding_results,
                "success_rate": sum(1 for r in encoding_results if r["success"]) / len(encoding_results)
            }

            # ì „ì²´ ì„±ê³µ ì—¬ë¶€
            valid_embeddings = sum(1 for t in results["embedding_tests"] if t["is_valid"])
            valid_similarities = sum(1 for t in results["similarity_tests"] if t["similarity_valid"])

            results["success"] = (
                valid_embeddings == len(results["embedding_tests"]) and
                valid_similarities == len(results["similarity_tests"]) and
                results["encoding_test"]["success_rate"] >= 0.75
            )

            self.test_results["vector_tests"] = results

        except Exception as e:
            logger.error(f"ë²¡í„° ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            results["error"] = str(e)
            results["traceback"] = traceback.format_exc()

        logger.info(f"ë²¡í„° ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ì„±ê³µ: {results['success']}")
        return results

    async def test_cross_encoder_reranking(self) -> Dict[str, Any]:
        """Cross-Encoder ë¦¬ë­í‚¹ í…ŒìŠ¤íŠ¸"""
        logger.info("Cross-Encoder ë¦¬ë­í‚¹ í…ŒìŠ¤íŠ¸ ì‹œì‘")

        results = {
            "success": False,
            "reranking_tests": [],
            "performance": {},
            "quality_metrics": {}
        }

        try:
            start_time = time.time()

            # ê°€ìƒì˜ ê²€ìƒ‰ ê²°ê³¼ ìƒì„±
            test_query = "RAG ì‹œìŠ¤í…œ ì„¤ëª…"

            mock_search_results = [
                {
                    "chunk_id": str(uuid.uuid4()),
                    "text": "RAG ì‹œìŠ¤í…œì€ ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•œ AI ëª¨ë¸ì…ë‹ˆë‹¤.",
                    "score": 0.6,
                    "metadata": {"bm25_score": 0.8, "vector_score": 0.4, "source": "hybrid"}
                },
                {
                    "chunk_id": str(uuid.uuid4()),
                    "text": "ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì€ ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤.",
                    "score": 0.3,
                    "metadata": {"bm25_score": 0.2, "vector_score": 0.4, "source": "hybrid"}
                },
                {
                    "chunk_id": str(uuid.uuid4()),
                    "text": "ê²€ìƒ‰ ì¦ê°• ìƒì„±(RAG)ì€ ì™¸ë¶€ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ë” ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.",
                    "score": 0.7,
                    "metadata": {"bm25_score": 0.9, "vector_score": 0.5, "source": "hybrid"}
                },
                {
                    "chunk_id": str(uuid.uuid4()),
                    "text": "ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”ëŠ” ì¿¼ë¦¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.",
                    "score": 0.2,
                    "metadata": {"bm25_score": 0.1, "vector_score": 0.3, "source": "hybrid"}
                },
                {
                    "chunk_id": str(uuid.uuid4()),
                    "text": "RAG ì•„í‚¤í…ì²˜ëŠ” retrieverì™€ generator ì»´í¬ë„ŒíŠ¸ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.",
                    "score": 0.65,
                    "metadata": {"bm25_score": 0.7, "vector_score": 0.6, "source": "hybrid"}
                }
            ]

            # ì›ë³¸ ìˆœì„œ ê¸°ë¡
            original_order = [(r["chunk_id"], r["score"]) for r in mock_search_results]

            # ë¦¬ë­í‚¹ ìˆ˜í–‰
            reranked_results = CrossEncoderReranker.rerank_results(
                test_query, mock_search_results.copy(), top_k=3
            )

            # ê²°ê³¼ ë¶„ì„
            reranked_order = [(r["chunk_id"], r["score"]) for r in reranked_results]

            # ìˆœì„œ ë³€í™” í™•ì¸
            order_changed = original_order != reranked_order

            # ê´€ë ¨ì„± ê°œì„  í™•ì¸ (RAG ê´€ë ¨ ë¬¸ì„œê°€ ìƒìœ„ì— ì˜¤ëŠ”ì§€)
            rag_relevant_indices = []
            for i, result in enumerate(reranked_results):
                if "RAG" in result["text"] or "ê²€ìƒ‰" in result["text"] or "ìƒì„±" in result["text"]:
                    rag_relevant_indices.append(i)

            relevance_improved = len(rag_relevant_indices) > 0 and max(rag_relevant_indices) <= 1

            results["reranking_tests"].append({
                "query": test_query,
                "original_count": len(mock_search_results),
                "reranked_count": len(reranked_results),
                "top_k": 3,
                "order_changed": order_changed,
                "relevance_improved": relevance_improved,
                "original_scores": [r[1] for r in original_order][:3],
                "reranked_scores": [r[1] for r in reranked_order],
                "score_improvement": any(
                    r_score > o_score
                    for (_, r_score), (_, o_score)
                    in zip(reranked_order[:3], original_order[:3])
                )
            })

            # ë‹¤ì–‘í•œ top_k ê°’ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
            for top_k in [1, 2, 5]:
                if top_k <= len(mock_search_results):
                    sub_reranked = CrossEncoderReranker.rerank_results(
                        test_query, mock_search_results.copy(), top_k=top_k
                    )

                    results["reranking_tests"].append({
                        "query": test_query,
                        "top_k": top_k,
                        "result_count": len(sub_reranked),
                        "top_k_respected": len(sub_reranked) == top_k,
                        "scores_descending": all(
                            sub_reranked[i]["score"] >= sub_reranked[i+1]["score"]
                            for i in range(len(sub_reranked)-1)
                        ) if len(sub_reranked) > 1 else True
                    })

            # ì„±ëŠ¥ ë©”íŠ¸ë¦­
            end_time = time.time()
            results["performance"] = {
                "total_time": end_time - start_time,
                "reranking_latency": (end_time - start_time) / len(results["reranking_tests"]),
                "throughput": len(results["reranking_tests"]) / (end_time - start_time)
            }

            # í’ˆì§ˆ ë©”íŠ¸ë¦­
            successful_tests = sum(1 for test in results["reranking_tests"]
                                 if test.get("relevance_improved", False) or
                                    test.get("top_k_respected", False))

            results["quality_metrics"] = {
                "success_rate": successful_tests / len(results["reranking_tests"]),
                "relevance_improvement_rate": sum(1 for test in results["reranking_tests"]
                                                if test.get("relevance_improved", False)) /
                                           len([t for t in results["reranking_tests"] if "relevance_improved" in t]),
                "order_change_rate": sum(1 for test in results["reranking_tests"]
                                       if test.get("order_changed", False)) /
                                   len([t for t in results["reranking_tests"] if "order_changed" in t])
            }

            results["success"] = results["quality_metrics"]["success_rate"] >= 0.7
            self.test_results["cross_encoder_tests"] = results

        except Exception as e:
            logger.error(f"Cross-Encoder í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            results["error"] = str(e)
            results["traceback"] = traceback.format_exc()

        logger.info(f"Cross-Encoder í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ì„±ê³µ: {results['success']}")
        return results

    async def test_hybrid_search_integration(self) -> Dict[str, Any]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í†µí•© í…ŒìŠ¤íŠ¸"""
        logger.info("í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")

        results = {
            "success": False,
            "search_tests": [],
            "performance": {},
            "quality_metrics": {}
        }

        try:
            # ì—¬ëŸ¬ ì¿¼ë¦¬ë¡œ í†µí•© ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            for query in self.test_queries:
                search_start = time.time()

                # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰
                search_results = await SearchDAO.hybrid_search(
                    query=query,
                    filters=None,
                    topk=5,
                    bm25_topk=10,
                    vector_topk=10,
                    rerank_candidates=20
                )

                search_time = time.time() - search_start

                # ê²°ê³¼ ë¶„ì„
                test_result = {
                    "query": query,
                    "search_time": search_time,
                    "result_count": len(search_results),
                    "has_results": len(search_results) > 0,
                    "results_details": []
                }

                # ê° ê²°ê³¼ ê²€ì¦
                for i, result in enumerate(search_results[:3]):  # ìƒìœ„ 3ê°œë§Œ ë¶„ì„
                    detail = {
                        "rank": i + 1,
                        "has_chunk_id": "chunk_id" in result,
                        "has_text": "text" in result and len(result.get("text", "")) > 0,
                        "has_score": "score" in result and isinstance(result.get("score"), (int, float)),
                        "has_metadata": "metadata" in result,
                        "text_preview": result.get("text", "")[:100] + "..." if result.get("text") else "",
                        "score": result.get("score", 0)
                    }

                    # ë©”íƒ€ë°ì´í„° ê²€ì¦
                    metadata = result.get("metadata", {})
                    detail["metadata_complete"] = all(
                        key in metadata for key in ["bm25_score", "vector_score", "source"]
                    )

                    test_result["results_details"].append(detail)

                # í’ˆì§ˆ ì§€í‘œ
                test_result["quality_score"] = sum(
                    1 for detail in test_result["results_details"]
                    if detail["has_text"] and detail["has_score"] and detail["metadata_complete"]
                ) / max(1, len(test_result["results_details"]))

                # ì„±ëŠ¥ ì§€í‘œ
                test_result["performance_acceptable"] = search_time < 5.0  # 5ì´ˆ ì´ë‚´

                results["search_tests"].append(test_result)

                # ë©”íŠ¸ë¦­ ê¸°ë¡
                self.test_metrics.record_search(
                    "hybrid", search_time, error=len(search_results) == 0
                )

            # í•„í„°ë§ í…ŒìŠ¤íŠ¸
            filtered_search_start = time.time()
            filtered_results = await SearchDAO.hybrid_search(
                query="RAG ì‹œìŠ¤í…œ",
                filters={"canonical_in": [["AI", "RAG"]]},
                topk=3
            )
            filtered_search_time = time.time() - filtered_search_start

            results["search_tests"].append({
                "query": "RAG ì‹œìŠ¤í…œ (í•„í„°ë§)",
                "search_time": filtered_search_time,
                "result_count": len(filtered_results),
                "filter_applied": True,
                "performance_acceptable": filtered_search_time < 5.0
            })

            # ì „ì²´ ì„±ëŠ¥ ë©”íŠ¸ë¦­
            search_times = [test["search_time"] for test in results["search_tests"]]
            results["performance"] = {
                "avg_search_time": sum(search_times) / len(search_times),
                "max_search_time": max(search_times),
                "min_search_time": min(search_times),
                "p95_search_time": sorted(search_times)[int(len(search_times) * 0.95)] if len(search_times) > 1 else max(search_times),
                "searches_per_second": len(search_times) / sum(search_times),
                "performance_target_met": all(t < 5.0 for t in search_times)
            }

            # í’ˆì§ˆ ë©”íŠ¸ë¦­
            quality_scores = [test.get("quality_score", 0) for test in results["search_tests"] if "quality_score" in test]
            successful_searches = sum(1 for test in results["search_tests"] if test["has_results"])

            results["quality_metrics"] = {
                "avg_quality_score": sum(quality_scores) / len(quality_scores) if quality_scores else 0,
                "search_success_rate": successful_searches / len(results["search_tests"]),
                "quality_target_met": sum(quality_scores) / len(quality_scores) >= 0.7 if quality_scores else False
            }

            # ì „ì²´ ì„±ê³µ ì—¬ë¶€
            results["success"] = (
                results["quality_metrics"]["search_success_rate"] >= 0.8 and
                results["performance"]["performance_target_met"] and
                results["quality_metrics"]["quality_target_met"]
            )

            self.test_results["hybrid_tests"] = results

        except Exception as e:
            logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            results["error"] = str(e)
            results["traceback"] = traceback.format_exc()

        logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ì„±ê³µ: {results['success']}")
        return results

    async def test_performance_metrics_collection(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ê²€ì¦ í…ŒìŠ¤íŠ¸"""
        logger.info("ì„±ëŠ¥ ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸ ì‹œì‘")

        results = {
            "success": False,
            "metrics_tests": [],
            "performance": {}
        }

        try:
            # ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
            self.test_metrics.reset()

            # ë‹¤ì–‘í•œ ê²€ìƒ‰ ìˆ˜í–‰í•˜ì—¬ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            search_scenarios = [
                ("BM25 í…ŒìŠ¤íŠ¸", "bm25"),
                ("Vector í…ŒìŠ¤íŠ¸", "vector"),
                ("Hybrid í…ŒìŠ¤íŠ¸", "hybrid")
            ]

            for description, search_type in search_scenarios:
                for i in range(5):  # ê° íƒ€ì…ë³„ 5íšŒ ì‹¤í–‰
                    start_time = time.time()

                    # ì‹¤ì œ ê²€ìƒ‰ ìˆ˜í–‰ (ê°„ë‹¨í•œ ë²„ì „)
                    await asyncio.sleep(0.1 + i * 0.02)  # ì‹œë®¬ë ˆì´ì…˜ëœ ê²€ìƒ‰ ì‹œê°„

                    latency = time.time() - start_time
                    error = i == 4  # ë§ˆì§€ë§‰ ê²€ìƒ‰ì€ ì˜¤ë¥˜ë¡œ ì‹œë®¬ë ˆì´ì…˜

                    self.test_metrics.record_search(search_type, latency, error)

            # ë©”íŠ¸ë¦­ ì¡°íšŒ ë° ê²€ì¦
            collected_metrics = self.test_metrics.get_metrics()

            # ë©”íŠ¸ë¦­ ê²€ì¦
            metrics_test = {
                "has_avg_latency": "avg_latency" in collected_metrics,
                "has_p95_latency": "p95_latency" in collected_metrics,
                "has_total_searches": "total_searches" in collected_metrics,
                "has_search_counts": "search_counts" in collected_metrics,
                "has_error_rate": "error_rate" in collected_metrics,
                "total_searches_correct": collected_metrics.get("total_searches", 0) == 15,
                "error_rate_reasonable": 0 <= collected_metrics.get("error_rate", 0) <= 1,
                "latency_positive": collected_metrics.get("avg_latency", 0) > 0
            }

            metrics_test["all_metrics_present"] = all([
                metrics_test["has_avg_latency"],
                metrics_test["has_p95_latency"],
                metrics_test["has_total_searches"],
                metrics_test["has_search_counts"],
                metrics_test["has_error_rate"]
            ])

            metrics_test["data_consistency"] = all([
                metrics_test["total_searches_correct"],
                metrics_test["error_rate_reasonable"],
                metrics_test["latency_positive"]
            ])

            results["metrics_tests"].append({
                "description": "ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ê¸°ë³¸ í…ŒìŠ¤íŠ¸",
                "collected_metrics": collected_metrics,
                "validation": metrics_test,
                "success": metrics_test["all_metrics_present"] and metrics_test["data_consistency"]
            })

            # ì„±ëŠ¥ ë¶„ì„
            results["performance"] = {
                "metric_collection_latency": "ì‹¤ì‹œê°„",
                "metric_accuracy": "ì •í™•",
                "memory_usage": "ê²½ëŸ‰"
            }

            # ì „ì²´ ì„±ê³µ ì—¬ë¶€
            results["success"] = all(test["success"] for test in results["metrics_tests"])

            self.test_results["performance_metrics"] = results

        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            results["error"] = str(e)
            results["traceback"] = traceback.format_exc()

        logger.info(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ì„±ê³µ: {results['success']}")
        return results

    async def test_utf8_encoding_compatibility(self) -> Dict[str, Any]:
        """UTF-8 ì¸ì½”ë”© í˜¸í™˜ì„± í™•ì¸ í…ŒìŠ¤íŠ¸"""
        logger.info("UTF-8 ì¸ì½”ë”© í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹œì‘")

        results = {
            "success": False,
            "encoding_tests": [],
            "character_tests": {}
        }

        try:
            # ë‹¤ì–‘í•œ ì–¸ì–´ì™€ íŠ¹ìˆ˜ë¬¸ì í…ŒìŠ¤íŠ¸
            unicode_test_cases = [
                {
                    "name": "í•œê¸€",
                    "text": "ì•ˆë…•í•˜ì„¸ìš”! RAG ì‹œìŠ¤í…œì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤. í•œêµ­ì–´ ì²˜ë¦¬ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ë‚˜ìš”?",
                    "language": "ko"
                },
                {
                    "name": "ì˜ì–´",
                    "text": "Hello! This is a test for the RAG system. Does English processing work correctly?",
                    "language": "en"
                },
                {
                    "name": "ì¼ë³¸ì–´",
                    "text": "ã“ã‚“ã«ã¡ã¯ï¼RAGã‚·ã‚¹ãƒ†ãƒ ã‚’ãƒ†ã‚¹ãƒˆã—ã¦ã„ã¾ã™ã€‚æ—¥æœ¬èªå‡¦ç†ã¯æ­£å¸¸ã«å‹•ä½œã—ã¾ã™ã‹ï¼Ÿ",
                    "language": "ja"
                },
                {
                    "name": "ëŸ¬ì‹œì•„ì–´",
                    "text": "ĞŸÑ€Ğ¸Ğ²ĞµÑ‚! Ğ­Ñ‚Ğ¾ Ñ‚ĞµÑÑ‚ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ RAG. ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ»Ğ¸ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ€ÑƒÑÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°?",
                    "language": "ru"
                },
                {
                    "name": "ì´ëª¨ì§€ ë° íŠ¹ìˆ˜ë¬¸ì",
                    "text": "ğŸš€ AI ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ğŸ¤– Special chars: @#$%^&*()_+-=[]{}|;':\",./<>?",
                    "language": "mixed"
                },
                {
                    "name": "ìˆ˜í•™ ê¸°í˜¸",
                    "text": "ìˆ˜í•™ ê¸°í˜¸: âˆ‘âˆ«âˆ†âˆ‡âˆÂ±â‰¤â‰¥â‰ â‰ˆâˆâˆˆâˆ‰âˆªâˆ©âŠ‚âŠƒâŠ†âŠ‡âˆ§âˆ¨Â¬âˆ€âˆƒ",
                    "language": "math"
                }
            ]

            for test_case in unicode_test_cases:
                encoding_result = {
                    "name": test_case["name"],
                    "original_text": test_case["text"],
                    "language": test_case["language"],
                    "tests": {}
                }

                try:
                    # 1. BM25 í† í°í™” í…ŒìŠ¤íŠ¸
                    tokens = BM25Scorer.preprocess_text(test_case["text"])
                    encoding_result["tests"]["bm25_tokenization"] = {
                        "success": True,
                        "token_count": len(tokens),
                        "sample_tokens": tokens[:5]
                    }
                except Exception as e:
                    encoding_result["tests"]["bm25_tokenization"] = {
                        "success": False,
                        "error": str(e)
                    }

                try:
                    # 2. ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸
                    embedding = await EmbeddingService.generate_embedding(test_case["text"])
                    encoding_result["tests"]["embedding_generation"] = {
                        "success": len(embedding) > 0,
                        "embedding_length": len(embedding),
                        "sample_values": embedding[:3] if embedding else []
                    }
                except Exception as e:
                    encoding_result["tests"]["embedding_generation"] = {
                        "success": False,
                        "error": str(e)
                    }

                try:
                    # 3. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥/ì¡°íšŒ í…ŒìŠ¤íŠ¸ (ì‹œë®¬ë ˆì´ì…˜)
                    # ì‹¤ì œë¡œëŠ” DBì— ì €ì¥í•˜ì§€ ì•Šê³  ë¬¸ìì—´ ì²˜ë¦¬ë§Œ í™•ì¸
                    encoded_text = test_case["text"].encode('utf-8').decode('utf-8')
                    encoding_result["tests"]["database_compatibility"] = {
                        "success": encoded_text == test_case["text"],
                        "round_trip_success": True
                    }
                except Exception as e:
                    encoding_result["tests"]["database_compatibility"] = {
                        "success": False,
                        "error": str(e)
                    }

                # ì „ì²´ í…ŒìŠ¤íŠ¸ ì„±ê³µ ì—¬ë¶€
                encoding_result["overall_success"] = all(
                    test.get("success", False)
                    for test in encoding_result["tests"].values()
                )

                results["encoding_tests"].append(encoding_result)

            # íŠ¹ìˆ˜ ë¬¸ì ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
            special_chars = {
                "quotes": "\"'""''",
                "dashes": "â€”â€“-",
                "spaces": "\u00A0\u2000\u2001\u2002\u2003",
                "control": "\n\r\t",
                "combining": "Ã©Ì‚Ã±ÌƒÃ¼Ìˆ",
                "rtl": "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ×¢×‘×¨×™×ª"
            }

            char_test_results = {}
            for category, chars in special_chars.items():
                try:
                    test_text = f"Test with {category}: {chars}"
                    tokens = BM25Scorer.preprocess_text(test_text)
                    char_test_results[category] = {
                        "success": True,
                        "processed": len(tokens) > 0
                    }
                except Exception as e:
                    char_test_results[category] = {
                        "success": False,
                        "error": str(e)
                    }

            results["character_tests"] = char_test_results

            # ì „ì²´ ì„±ê³µ ì—¬ë¶€
            successful_encodings = sum(1 for test in results["encoding_tests"] if test["overall_success"])
            successful_chars = sum(1 for test in char_test_results.values() if test["success"])

            results["success"] = (
                successful_encodings >= len(results["encoding_tests"]) * 0.8 and
                successful_chars >= len(char_test_results) * 0.7
            )

            self.test_results["encoding_tests"] = results

        except Exception as e:
            logger.error(f"UTF-8 ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            results["error"] = str(e)
            results["traceback"] = traceback.format_exc()

        logger.info(f"UTF-8 ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ì„±ê³µ: {results['success']}")
        return results

    async def run_all_tests(self) -> Dict[str, Any]:
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("=== í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ í¬ê´„ì  í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")

        # ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹œì‘ ì‹œê°„
        overall_start = time.time()

        # 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¤ì •
        setup_success = await self.setup_test_data()
        if not setup_success:
            logger.warning("í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¤ì • ì‹¤íŒ¨, ê¸°ë³¸ ë°ì´í„°ë¡œ ì§„í–‰")

        # 2. ê°œë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_functions = [
            ("BM25 ìŠ¤ì½”ì–´ë§", self.test_bm25_scoring),
            ("ë²¡í„° ì„ë² ë”©", self.test_vector_embedding_similarity),
            ("Cross-Encoder ë¦¬ë­í‚¹", self.test_cross_encoder_reranking),
            ("í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í†µí•©", self.test_hybrid_search_integration),
            ("ì„±ëŠ¥ ë©”íŠ¸ë¦­", self.test_performance_metrics_collection),
            ("UTF-8 ì¸ì½”ë”©", self.test_utf8_encoding_compatibility)
        ]

        for test_name, test_func in test_functions:
            logger.info(f"{test_name} í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
            try:
                await test_func()
            except Exception as e:
                logger.error(f"{test_name} í…ŒìŠ¤íŠ¸ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
                logger.error(traceback.format_exc())

        # ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹œê°„
        total_time = time.time() - overall_start

        # 3. í†µí•© ê²°ê³¼ ìƒì„±
        overall_results = {
            "test_summary": {
                "total_time": total_time,
                "setup_success": setup_success,
                "tests_completed": len(test_functions),
                "timestamp": datetime.utcnow().isoformat()
            },
            "individual_results": self.test_results,
            "final_metrics": self.test_metrics.get_metrics(),
            "success_summary": {},
            "recommendations": []
        }

        # 4. ì„±ê³µë¥  ê³„ì‚°
        success_counts = {}
        for test_category, test_result in self.test_results.items():
            success_counts[test_category] = test_result.get("success", False)

        overall_success = sum(success_counts.values()) / len(success_counts) if success_counts else 0

        overall_results["success_summary"] = {
            "overall_success_rate": overall_success,
            "individual_successes": success_counts,
            "critical_failures": [
                category for category, success in success_counts.items()
                if not success and category in ["hybrid_tests", "vector_tests"]
            ]
        }

        # 5. ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = []

        if not success_counts.get("bm25_tests", False):
            recommendations.append("BM25 ìŠ¤ì½”ì–´ë§ ê°œì„ : í† í°í™” ë¡œì§ê³¼ ì ìˆ˜ ê³„ì‚° ì•Œê³ ë¦¬ì¦˜ì„ ê²€í† í•˜ì„¸ìš”.")

        if not success_counts.get("vector_tests", False):
            recommendations.append("ë²¡í„° ì„ë² ë”© ê°œì„ : OpenAI API í‚¤ ì„¤ì •ê³¼ ì„ë² ë”© ìƒì„± ê³¼ì •ì„ í™•ì¸í•˜ì„¸ìš”.")

        if not success_counts.get("cross_encoder_tests", False):
            recommendations.append("Cross-Encoder ê°œì„ : ë¦¬ë­í‚¹ ì•Œê³ ë¦¬ì¦˜ì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°ì„ ê°œì„ í•˜ì„¸ìš”.")

        if not success_counts.get("hybrid_tests", False):
            recommendations.append("í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê°œì„ : BM25ì™€ ë²¡í„° ê²€ìƒ‰ì˜ ê°€ì¤‘ì¹˜ ì¡°ì •ê³¼ ê²°ê³¼ í•©ì„± ë¡œì§ì„ ìµœì í™”í•˜ì„¸ìš”.")

        if overall_results["final_metrics"].get("error_rate", 0) > 0.1:
            recommendations.append("ì˜¤ë¥˜ìœ¨ ê°ì†Œ: ì‹œìŠ¤í…œ ì•ˆì •ì„±ì„ ìœ„í•´ ì˜¤ë¥˜ ì²˜ë¦¬ì™€ í´ë°± ë©”ì»¤ë‹ˆì¦˜ì„ ê°•í™”í•˜ì„¸ìš”.")

        avg_latency = overall_results["final_metrics"].get("avg_latency", 0)
        if avg_latency > 2.0:
            recommendations.append(f"ì„±ëŠ¥ ìµœì í™”: í‰ê·  ì§€ì—°ì‹œê°„({avg_latency:.2f}s)ì„ 2ì´ˆ ì´í•˜ë¡œ ê°œì„ í•˜ì„¸ìš”.")

        if not success_counts.get("encoding_tests", False):
            recommendations.append("ì¸ì½”ë”© ê°œì„ : ë‹¤êµ­ì–´ ë° íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬ ë¡œì§ì„ ê°•í™”í•˜ì„¸ìš”.")

        overall_results["recommendations"] = recommendations

        # 6. ê²°ê³¼ ë¡œê¹…
        logger.info("=== í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ===")
        logger.info(f"ì „ì²´ ì„±ê³µë¥ : {overall_success:.1%}")
        logger.info(f"ì „ì²´ ì‹¤í–‰ ì‹œê°„: {total_time:.2f}ì´ˆ")
        logger.info(f"ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸: {[k for k, v in success_counts.items() if not v]}")

        if recommendations:
            logger.info("ê¶Œì¥ì‚¬í•­:")
            for rec in recommendations:
                logger.info(f"  - {rec}")

        return overall_results

    def save_results_to_file(self, results: Dict[str, Any], filename: str = "hybrid_search_test_results.json"):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ íŒŒì¼ì— ì €ì¥"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2, default=str)
            logger.info(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ í¬ê´„ì  í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)

    tester = HybridSearchTester()

    try:
        # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        results = await tester.run_all_tests()

        # ê²°ê³¼ ì €ì¥
        tester.save_results_to_file(results)

        # ìš”ì•½ ì¶œë ¥
        print("\n" + "=" * 60)
        print("í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"ì „ì²´ ì„±ê³µë¥ : {results['success_summary']['overall_success_rate']:.1%}")
        print(f"ì´ ì‹¤í–‰ì‹œê°„: {results['test_summary']['total_time']:.2f}ì´ˆ")

        if results['recommendations']:
            print("\nê¶Œì¥ì‚¬í•­:")
            for rec in results['recommendations']:
                print(f"  - {rec}")

        print(f"\nìƒì„¸ ê²°ê³¼: hybrid_search_test_results.json")
        print("=" * 60)

        return results['success_summary']['overall_success_rate'] >= 0.7

    except Exception as e:
        logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.error(traceback.format_exc())
        return False

if __name__ == "__main__":
    # ë¹„ë™ê¸° ì‹¤í–‰
    import platform
    if platform.system() == "Windows":
        # Windowsì—ì„œ ProactorEventLoop ì‚¬ìš©
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

    success = asyncio.run(main())
    sys.exit(0 if success else 1)