#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ìµœì¢… ë¬¸ì„œ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë¬¸ì„œ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ì˜ ì „ì²´ ê³¼ì •ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤:
1. íŒŒì¼ í¬ë§· ì§€ì› í…ŒìŠ¤íŠ¸ (PDF, TXT, MD, HTML, CSV, JSON)
2. í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì²­í‚¹
3. ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
4. PII í•„í„°ë§ (í•œêµ­ì–´ ì§€ì›)
5. ì„ë² ë”© ìƒì„± (MockService ì‚¬ìš©)
6. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹œë®¬ë ˆì´ì…˜
7. ì˜¤ë¥˜ ì²˜ë¦¬
8. ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥

í•œê¸€ ì²˜ë¦¬ì™€ ì‹¤ì œ íŒŒì¼ ê²€ì¦ì— ì¤‘ì ì„ ë‘¡ë‹ˆë‹¤.
"""

import os
import sys
import json
import time
import tempfile
import shutil
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import asyncio
from datetime import datetime
import traceback
import re
from dataclasses import dataclass, field

# í•œê¸€ ì¶œë ¥ì„ ìœ„í•œ ì„¤ì •
if sys.platform.startswith('win'):
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer)
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer)

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# ë¡œê¹… ì„¤ì • (UTF-8 ì§€ì›)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('document_ingestion_final_test.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class TestResult:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    success: bool
    message: str
    details: Dict[str, Any] = field(default_factory=dict)
    processing_time: float = 0.0
    error: Optional[str] = None

class DocumentIngestionTester:
    """ë¬¸ì„œ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ì¢…í•© í…ŒìŠ¤í„°"""

    def __init__(self):
        self.temp_dir = Path(tempfile.mkdtemp(prefix="ingestion_final_test_"))
        self.start_time = time.time()
        self.test_results = {}

        logger.info(f"í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬: {self.temp_dir}")

    def __del__(self):
        """ì •ë¦¬"""
        if hasattr(self, 'temp_dir') and self.temp_dir.exists():
            shutil.rmtree(self.temp_dir, ignore_errors=True)

    def create_test_files(self) -> Dict[str, Path]:
        """ë‹¤ì–‘í•œ í¬ë§·ì˜ í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±"""
        logger.info("í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„± ì¤‘...")
        files = {}

        # 1. í•œê¸€ í…ìŠ¤íŠ¸ íŒŒì¼
        txt_content = """Dynamic Taxonomy RAG ì‹œìŠ¤í…œ ë¬¸ì„œ

ê°œì¸ì •ë³´ í…ŒìŠ¤íŠ¸:
- ì´ë©”ì¼: hong@example.com, kim.test@company.co.kr
- ì „í™”ë²ˆí˜¸: 010-1234-5678, 02-987-6543
- ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸: 123456-1234567, 890123-2345678
- ì‹ ìš©ì¹´ë“œ: 1234-5678-9012-3456

ê¸°ìˆ  ë‚´ìš©:
ì´ ì‹œìŠ¤í…œì€ ë™ì  ë¶„ë¥˜ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ìë™ìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
RAG(Retrieval-Augmented Generation) ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°,
ë²¡í„° ê²€ìƒ‰ê³¼ í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ì§€ì›í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. ë¬¸ì„œ ìë™ ë¶„ë¥˜
2. ì˜ë¯¸ì  ê²€ìƒ‰
3. ê³„ì¸µì  ë¶„ë¥˜ì²´ê³„
4. ì‹¤ì‹œê°„ ì„ë² ë”© ìƒì„±

English content is also supported for multilingual processing.
The system handles various document formats including PDF, HTML, and plain text.
"""

        txt_file = self.temp_dir / "test_korean.txt"
        with open(txt_file, 'w', encoding='utf-8') as f:
            f.write(txt_content)
        files['txt'] = txt_file

        # 2. Markdown íŒŒì¼
        md_content = """# í•œê¸€ ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ í…ŒìŠ¤íŠ¸

## ê°œìš”
ì´ê²ƒì€ **Dynamic Taxonomy RAG** ì‹œìŠ¤í…œì˜ ë§ˆí¬ë‹¤ìš´ í…ŒìŠ¤íŠ¸ íŒŒì¼ì…ë‹ˆë‹¤.

### ê°œì¸ì •ë³´ ì„¹ì…˜
- ë‹´ë‹¹ì: ê¹€ê°œë°œ (kim.dev@company.com)
- ì—°ë½ì²˜: 010-9999-8888
- ë¶€ì„œ: AIê°œë°œíŒ€

### ê¸°ìˆ  ìŠ¤í™
```python
class DocumentProcessor:
    def __init__(self):
        self.chunker = ChunkingStrategy()
        self.pii_filter = PIIFilter()

    async def process(self, document):
        chunks = await self.chunker.chunk(document)
        filtered_chunks = await self.pii_filter.filter(chunks)
        return filtered_chunks
```

### ì²˜ë¦¬ í”Œë¡œìš°
1. **ë¬¸ì„œ íŒŒì‹±** â†’ í…ìŠ¤íŠ¸ ì¶”ì¶œ
2. **ì²­í‚¹** â†’ ì ì ˆí•œ í¬ê¸°ë¡œ ë¶„í• 
3. **PII í•„í„°ë§** â†’ ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹
4. **ì„ë² ë”©** â†’ ë²¡í„° ìƒì„±
5. **ì €ì¥** â†’ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥

> ì¤‘ìš”: ëª¨ë“  ê°œì¸ì •ë³´ëŠ” GDPR ë° PIPA ê·œì •ì— ë”°ë¼ ì²˜ë¦¬ë©ë‹ˆë‹¤.

[ì‹œìŠ¤í…œ ë¬¸ì„œ](https://docs.example.com)
"""

        md_file = self.temp_dir / "test_korean.md"
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(md_content)
        files['md'] = md_file

        # 3. HTML íŒŒì¼
        html_content = """<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dynamic Taxonomy RAG ì‹œìŠ¤í…œ</title>
</head>
<body>
    <header>
        <h1>í•œê¸€ HTML ë¬¸ì„œ í…ŒìŠ¤íŠ¸</h1>
        <nav>
            <ul>
                <li><a href="#intro">ì†Œê°œ</a></li>
                <li><a href="#features">ê¸°ëŠ¥</a></li>
                <li><a href="#contact">ì—°ë½ì²˜</a></li>
            </ul>
        </nav>
    </header>

    <main>
        <section id="intro">
            <h2>ì‹œìŠ¤í…œ ì†Œê°œ</h2>
            <p>Dynamic Taxonomy RAGëŠ” <strong>ì°¨ì„¸ëŒ€ ë¬¸ì„œ ì²˜ë¦¬ ì‹œìŠ¤í…œ</strong>ì…ë‹ˆë‹¤.</p>
            <p>ë‹¤ì–‘í•œ í¬ë§·ì˜ ë¬¸ì„œë¥¼ ìë™ìœ¼ë¡œ ë¶„ë¥˜í•˜ê³  ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
        </section>

        <section id="features">
            <h2>ì£¼ìš” ê¸°ëŠ¥</h2>
            <ul>
                <li>ë‹¤ì¤‘ í¬ë§· ì§€ì› (PDF, HTML, TXT, MD)</li>
                <li>ì‹¤ì‹œê°„ PII íƒì§€ ë° ë§ˆìŠ¤í‚¹</li>
                <li>ì˜ë¯¸ì  ê²€ìƒ‰ ì§€ì›</li>
                <li>ê³„ì¸µì  ë¶„ë¥˜ ì²´ê³„</li>
            </ul>
        </section>

        <section id="contact">
            <h2>ì—°ë½ì²˜ ì •ë³´</h2>
            <div class="contact-info">
                <p>ì´ë©”ì¼: contact@rag-system.com</p>
                <p>ì „í™”: 02-1234-5678</p>
                <p>ë‹´ë‹¹ì: ë°•ì‹œìŠ¤í…œ (park.system@company.kr)</p>
                <p>ê³ ê°ì„¼í„°: 1588-1234</p>
            </div>
        </section>
    </main>

    <footer>
        <p>&copy; 2024 Dynamic Taxonomy RAG System. All rights reserved.</p>
    </footer>

    <script>
        // ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” íŒŒì‹± ì‹œ ì œê±°ë˜ì–´ì•¼ í•¨
        console.log("JavaScript content should be removed");
        document.addEventListener('DOMContentLoaded', function() {
            alert('ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë³´ì´ì§€ ì•Šì•„ì•¼ í•©ë‹ˆë‹¤');
        });
    </script>
</body>
</html>"""

        html_file = self.temp_dir / "test_korean.html"
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        files['html'] = html_file

        # 4. CSV íŒŒì¼
        csv_content = """ì´ë¦„,ì´ë©”ì¼,ì „í™”ë²ˆí˜¸,ë¶€ì„œ,ì…ì‚¬ì¼
ê¹€ë¯¼ìˆ˜,kim.min@company.com,010-1111-2222,ê°œë°œíŒ€,2023-01-15
ì´ì˜í¬,lee.young@company.com,010-3333-4444,ë§ˆì¼€íŒ…íŒ€,2023-02-01
ë°•ì² ìˆ˜,park.chul@company.com,010-5555-6666,ì˜ì—…íŒ€,2023-03-10
ìµœì§€í›ˆ,choi.ji@company.com,010-7777-8888,ê¸°íšíŒ€,2023-04-05
ì •ë¯¸ë˜,jung.mi@company.com,010-9999-0000,ì¸ì‚¬íŒ€,2023-05-20"""

        csv_file = self.temp_dir / "test_korean.csv"
        with open(csv_file, 'w', encoding='utf-8') as f:
            f.write(csv_content)
        files['csv'] = csv_file

        # 5. JSON íŒŒì¼
        json_data = {
            "system_info": {
                "name": "Dynamic Taxonomy RAG",
                "version": "1.8.1",
                "description": "ì§€ëŠ¥í˜• ë¬¸ì„œ ë¶„ë¥˜ ë° ê²€ìƒ‰ ì‹œìŠ¤í…œ"
            },
            "configuration": {
                "chunk_size": 500,
                "overlap_size": 128,
                "embedding_model": "text-embedding-ada-002",
                "languages": ["ko", "en"]
            },
            "test_users": [
                {
                    "name": "í™ê¸¸ë™",
                    "email": "hong@test.example.com",
                    "phone": "010-1234-0000",
                    "role": "ê´€ë¦¬ì",
                    "created_at": "2024-01-01T09:00:00Z"
                },
                {
                    "name": "ê¹€ì˜ìˆ˜",
                    "email": "kim.young@test.example.com",
                    "phone": "010-5678-0000",
                    "role": "ì‚¬ìš©ì",
                    "created_at": "2024-01-02T10:30:00Z"
                }
            ],
            "sensitive_data": {
                "api_key": "sk-test-api-key-1234567890",
                "database_url": "postgresql://user:pass@localhost:5432/db",
                "credit_card": "4532-1234-5678-9012"
            },
            "features": {
                "document_parsing": {
                    "supported_formats": ["pdf", "html", "txt", "md", "csv", "json"],
                    "encoding_support": ["utf-8", "cp949", "iso-8859-1"],
                    "korean_support": True
                },
                "pii_detection": {
                    "email_patterns": True,
                    "phone_patterns": True,
                    "ssn_patterns": True,
                    "credit_card_patterns": True,
                    "korean_specific": True
                }
            }
        }

        json_file = self.temp_dir / "test_korean.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        files['json'] = json_file

        # 6. ë¹ˆ íŒŒì¼ (ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸)
        empty_file = self.temp_dir / "empty.txt"
        empty_file.touch()
        files['empty'] = empty_file

        # 7. ì†ìƒëœ íŒŒì¼ (ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸)
        corrupted_file = self.temp_dir / "corrupted.pdf"
        with open(corrupted_file, 'wb') as f:
            f.write(b"This is not a valid PDF file content - corrupted data \x00\xFF\xFE")
        files['corrupted'] = corrupted_file

        # 8. ëŒ€ìš©ëŸ‰ íŒŒì¼ (ì„±ëŠ¥ í…ŒìŠ¤íŠ¸)
        large_content = "í•œê¸€ ëŒ€ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ì½˜í…ì¸ ì…ë‹ˆë‹¤. " * 2000 + \
                       "ì´ë©”ì¼: large@test.com, ì „í™”: 010-0000-1111 " * 100
        large_file = self.temp_dir / "large_test.txt"
        with open(large_file, 'w', encoding='utf-8') as f:
            f.write(large_content)
        files['large'] = large_file

        logger.info(f"ì´ {len(files)}ê°œì˜ í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„± ì™„ë£Œ")
        return files

    def test_file_parsing(self, files: Dict[str, Path]) -> TestResult:
        """íŒŒì¼ íŒŒì‹± í…ŒìŠ¤íŠ¸"""
        logger.info("=== íŒŒì¼ íŒŒì‹± í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
        start_time = time.time()

        results = {}

        try:
            # ê¸°ë³¸ import í…ŒìŠ¤íŠ¸
            from apps.ingestion.document_parser import get_parser_factory

            parser_factory = get_parser_factory()

            for file_type, file_path in files.items():
                if file_type in ['corrupted', 'empty']:
                    continue

                try:
                    parser = parser_factory.get_parser(file_path)
                    if parser:
                        # ë™ê¸° ë°©ì‹ìœ¼ë¡œ ë³€ê²½
                        import asyncio
                        try:
                            # ê¸°ì¡´ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆëŠ”ì§€ í™•ì¸
                            loop = asyncio.get_event_loop()
                            if loop.is_running():
                                # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìˆìœ¼ë©´ ìƒˆ íƒœìŠ¤í¬ë¡œ ì‹¤í–‰
                                parsed_doc = loop.run_until_complete(parser.parse(file_path))
                            else:
                                parsed_doc = asyncio.run(parser.parse(file_path))
                        except RuntimeError:
                            # ìƒˆ ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„±
                            new_loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(new_loop)
                            try:
                                parsed_doc = new_loop.run_until_complete(parser.parse(file_path))
                            finally:
                                new_loop.close()

                        results[file_type] = {
                            'success': True,
                            'content_length': len(parsed_doc.content),
                            'metadata_keys': list(parsed_doc.metadata.keys()) if parsed_doc.metadata else [],
                            'has_korean': any(ord(c) > 127 for c in parsed_doc.content[:100]),
                            'sample_content': parsed_doc.content[:200] + "..." if len(parsed_doc.content) > 200 else parsed_doc.content
                        }
                        logger.info(f"{file_type} íŒŒì‹± ì„±ê³µ: {len(parsed_doc.content)} ë¬¸ì")
                    else:
                        results[file_type] = {
                            'success': False,
                            'error': 'Parser not available for this format'
                        }

                except Exception as e:
                    results[file_type] = {
                        'success': False,
                        'error': str(e)
                    }
                    logger.error(f"{file_type} íŒŒì‹± ì‹¤íŒ¨: {e}")

            processing_time = time.time() - start_time
            success_count = sum(1 for r in results.values() if r.get('success', False))

            return TestResult(
                success=success_count > 0,
                message=f"íŒŒì¼ íŒŒì‹± í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {success_count}/{len(results)}ê°œ ì„±ê³µ",
                details=results,
                processing_time=processing_time
            )

        except Exception as e:
            return TestResult(
                success=False,
                message="íŒŒì¼ íŒŒì‹± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨",
                error=str(e),
                processing_time=time.time() - start_time
            )

    def test_chunking(self, files: Dict[str, Path]) -> TestResult:
        """ì²­í‚¹ í…ŒìŠ¤íŠ¸"""
        logger.info("=== ì²­í‚¹ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
        start_time = time.time()

        results = {}

        try:
            from apps.ingestion.chunking_strategy import default_chunker
            from apps.ingestion.document_parser import get_parser_factory

            parser_factory = get_parser_factory()

            # ëŒ€í‘œì ì¸ íŒŒì¼ë“¤ë¡œ í…ŒìŠ¤íŠ¸
            test_files = {k: v for k, v in files.items() if k in ['txt', 'md', 'html']}

            for file_type, file_path in test_files.items():
                try:
                    parser = parser_factory.get_parser(file_path)
                    if not parser:
                        continue

                    # ë¹„ë™ê¸° ì²˜ë¦¬
                    import asyncio
                    try:
                        loop = asyncio.get_event_loop()
                        if loop.is_running():
                            parsed_doc = loop.run_until_complete(parser.parse(file_path))
                            chunks = loop.run_until_complete(default_chunker.chunk_document(parsed_doc))
                        else:
                            parsed_doc = asyncio.run(parser.parse(file_path))
                            chunks = asyncio.run(default_chunker.chunk_document(parsed_doc))
                    except RuntimeError:
                        new_loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(new_loop)
                        try:
                            parsed_doc = new_loop.run_until_complete(parser.parse(file_path))
                            chunks = new_loop.run_until_complete(default_chunker.chunk_document(parsed_doc))
                        finally:
                            new_loop.close()

                    chunk_info = []
                    for i, chunk in enumerate(chunks):
                        chunk_info.append({
                            'id': i,
                            'length': len(chunk.content),
                            'start_pos': chunk.start_pos,
                            'end_pos': chunk.end_pos,
                            'sample': chunk.content[:100] + "..." if len(chunk.content) > 100 else chunk.content
                        })

                    results[file_type] = {
                        'success': True,
                        'original_length': len(parsed_doc.content),
                        'chunk_count': len(chunks),
                        'chunks': chunk_info,
                        'avg_chunk_size': sum(len(c.content) for c in chunks) / len(chunks) if chunks else 0
                    }

                    logger.info(f"{file_type} ì²­í‚¹ ì„±ê³µ: {len(chunks)}ê°œ ì²­í¬")

                except Exception as e:
                    results[file_type] = {
                        'success': False,
                        'error': str(e)
                    }
                    logger.error(f"{file_type} ì²­í‚¹ ì‹¤íŒ¨: {e}")

            processing_time = time.time() - start_time
            success_count = sum(1 for r in results.values() if r.get('success', False))

            return TestResult(
                success=success_count > 0,
                message=f"ì²­í‚¹ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {success_count}/{len(results)}ê°œ ì„±ê³µ",
                details=results,
                processing_time=processing_time
            )

        except Exception as e:
            return TestResult(
                success=False,
                message="ì²­í‚¹ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨",
                error=str(e),
                processing_time=time.time() - start_time
            )

    def test_pii_filtering(self, files: Dict[str, Path]) -> TestResult:
        """PII í•„í„°ë§ í…ŒìŠ¤íŠ¸"""
        logger.info("=== PII í•„í„°ë§ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
        start_time = time.time()

        results = {}

        try:
            from apps.ingestion.pii_filter import default_pii_filter, MaskingStrategy
            from apps.ingestion.document_parser import get_parser_factory

            parser_factory = get_parser_factory()

            # PIIê°€ í¬í•¨ëœ íŒŒì¼ë“¤ë¡œ í…ŒìŠ¤íŠ¸
            test_files = {k: v for k, v in files.items() if k in ['txt', 'html', 'csv', 'json']}

            for file_type, file_path in test_files.items():
                try:
                    parser = parser_factory.get_parser(file_path)
                    if not parser:
                        continue

                    # ë¹„ë™ê¸° ì²˜ë¦¬
                    import asyncio
                    try:
                        loop = asyncio.get_event_loop()
                        if loop.is_running():
                            parsed_doc = loop.run_until_complete(parser.parse(file_path))
                            filter_result = loop.run_until_complete(
                                default_pii_filter.filter_text(parsed_doc.content, MaskingStrategy.MASK)
                            )
                        else:
                            parsed_doc = asyncio.run(parser.parse(file_path))
                            filter_result = asyncio.run(
                                default_pii_filter.filter_text(parsed_doc.content, MaskingStrategy.MASK)
                            )
                    except RuntimeError:
                        new_loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(new_loop)
                        try:
                            parsed_doc = new_loop.run_until_complete(parser.parse(file_path))
                            filter_result = new_loop.run_until_complete(
                                default_pii_filter.filter_text(parsed_doc.content, MaskingStrategy.MASK)
                            )
                        finally:
                            new_loop.close()

                    detections = []
                    for detection in filter_result.detections:
                        detections.append({
                            'type': detection.pii_type.value,
                            'confidence': detection.confidence,
                            'original': detection.original_text,
                            'masked': detection.masked_text,
                            'position': f"{detection.start_pos}-{detection.end_pos}"
                        })

                    results[file_type] = {
                        'success': True,
                        'original_length': len(parsed_doc.content),
                        'filtered_length': len(filter_result.filtered_text),
                        'detections_count': len(filter_result.detections),
                        'detections': detections,
                        'is_compliant': filter_result.compliance_flags.get('gdpr_compliant', False),
                        'sample_filtered': filter_result.filtered_text[:300] + "..." if len(filter_result.filtered_text) > 300 else filter_result.filtered_text
                    }

                    logger.info(f"{file_type} PII í•„í„°ë§ ì„±ê³µ: {len(filter_result.detections)}ê°œ íƒì§€")

                except Exception as e:
                    results[file_type] = {
                        'success': False,
                        'error': str(e)
                    }
                    logger.error(f"{file_type} PII í•„í„°ë§ ì‹¤íŒ¨: {e}")

            processing_time = time.time() - start_time
            success_count = sum(1 for r in results.values() if r.get('success', False))
            total_pii = sum(r.get('detections_count', 0) for r in results.values() if r.get('success', False))

            return TestResult(
                success=success_count > 0,
                message=f"PII í•„í„°ë§ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {success_count}/{len(results)}ê°œ ì„±ê³µ, ì´ {total_pii}ê°œ PII íƒì§€",
                details=results,
                processing_time=processing_time
            )

        except Exception as e:
            return TestResult(
                success=False,
                message="PII í•„í„°ë§ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨",
                error=str(e),
                processing_time=time.time() - start_time
            )

    def test_mock_embedding(self, files: Dict[str, Path]) -> TestResult:
        """Mock ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸"""
        logger.info("=== Mock ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
        start_time = time.time()

        results = {}

        try:
            import numpy as np
            import hashlib

            class MockEmbeddingService:
                """Mock ì„ë² ë”© ì„œë¹„ìŠ¤"""

                def __init__(self, dimension=1536):
                    self.dimension = dimension

                async def embed(self, text: str) -> List[float]:
                    """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (Mock)"""
                    # í…ìŠ¤íŠ¸ì˜ í•´ì‹œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²°ì •ì  ì„ë² ë”© ìƒì„±
                    hash_object = hashlib.md5(text.encode('utf-8'))
                    hash_hex = hash_object.hexdigest()

                    # í•´ì‹œë¥¼ ì‹œë“œë¡œ ì‚¬ìš©í•˜ì—¬ ì¼ê´€ëœ ì„ë² ë”© ìƒì„±
                    np.random.seed(int(hash_hex[:8], 16))
                    embedding = np.random.normal(0, 1, self.dimension)

                    # ì •ê·œí™”
                    norm = np.linalg.norm(embedding)
                    if norm > 0:
                        embedding = embedding / norm

                    return embedding.tolist()

            mock_embedding_service = MockEmbeddingService()

            # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ë“¤ë¡œ í…ŒìŠ¤íŠ¸
            test_texts = [
                "ì•ˆë…•í•˜ì„¸ìš”. í•œê¸€ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.",
                "This is English text.",
                "Dynamic Taxonomy RAG ì‹œìŠ¤í…œ",
                "ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸ìš© ê¸´ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. " * 10
            ]

            for i, text in enumerate(test_texts):
                try:
                    import asyncio
                    try:
                        loop = asyncio.get_event_loop()
                        if loop.is_running():
                            embedding = loop.run_until_complete(mock_embedding_service.embed(text))
                        else:
                            embedding = asyncio.run(mock_embedding_service.embed(text))
                    except RuntimeError:
                        new_loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(new_loop)
                        try:
                            embedding = new_loop.run_until_complete(mock_embedding_service.embed(text))
                        finally:
                            new_loop.close()

                    # ì„ë² ë”© í’ˆì§ˆ ê²€ì¦
                    embedding_norm = sum(x*x for x in embedding) ** 0.5

                    results[f'text_{i}'] = {
                        'success': True,
                        'text_length': len(text),
                        'embedding_dimension': len(embedding),
                        'embedding_norm': embedding_norm,
                        'sample_values': embedding[:5],  # ì²˜ìŒ 5ê°œ ê°’
                        'text_sample': text[:50] + "..." if len(text) > 50 else text
                    }

                    logger.info(f"í…ìŠ¤íŠ¸ {i} ì„ë² ë”© ì„±ê³µ: {len(embedding)}ì°¨ì›")

                except Exception as e:
                    results[f'text_{i}'] = {
                        'success': False,
                        'error': str(e)
                    }
                    logger.error(f"í…ìŠ¤íŠ¸ {i} ì„ë² ë”© ì‹¤íŒ¨: {e}")

            processing_time = time.time() - start_time
            success_count = sum(1 for r in results.values() if r.get('success', False))

            return TestResult(
                success=success_count > 0,
                message=f"ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {success_count}/{len(results)}ê°œ ì„±ê³µ",
                details=results,
                processing_time=processing_time
            )

        except Exception as e:
            return TestResult(
                success=False,
                message="ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨",
                error=str(e),
                processing_time=time.time() - start_time
            )

    def test_error_handling(self, files: Dict[str, Path]) -> TestResult:
        """ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        logger.info("=== ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
        start_time = time.time()

        results = {}

        try:
            from apps.ingestion.document_parser import get_parser_factory

            parser_factory = get_parser_factory()

            # 1. ë¹ˆ íŒŒì¼ í…ŒìŠ¤íŠ¸
            empty_file = files.get('empty')
            if empty_file:
                try:
                    parser = parser_factory.get_parser(empty_file)
                    if parser:
                        import asyncio
                        try:
                            loop = asyncio.get_event_loop()
                            if loop.is_running():
                                parsed_doc = loop.run_until_complete(parser.parse(empty_file))
                            else:
                                parsed_doc = asyncio.run(parser.parse(empty_file))
                        except RuntimeError:
                            new_loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(new_loop)
                            try:
                                parsed_doc = new_loop.run_until_complete(parser.parse(empty_file))
                            finally:
                                new_loop.close()

                        results['empty_file'] = {
                            'success': True,
                            'handled_gracefully': len(parsed_doc.content) == 0,
                            'content_length': len(parsed_doc.content)
                        }
                    else:
                        results['empty_file'] = {
                            'success': False,
                            'error': 'No parser available'
                        }
                except Exception as e:
                    results['empty_file'] = {
                        'success': True,  # ì˜¤ë¥˜ê°€ ì ì ˆíˆ ì²˜ë¦¬ë¨
                        'handled_gracefully': True,
                        'error_handled': str(e)
                    }

            # 2. ì†ìƒëœ íŒŒì¼ í…ŒìŠ¤íŠ¸
            corrupted_file = files.get('corrupted')
            if corrupted_file:
                try:
                    parser = parser_factory.get_parser(corrupted_file)
                    if parser:
                        import asyncio
                        try:
                            loop = asyncio.get_event_loop()
                            if loop.is_running():
                                parsed_doc = loop.run_until_complete(parser.parse(corrupted_file))
                            else:
                                parsed_doc = asyncio.run(parser.parse(corrupted_file))
                        except RuntimeError:
                            new_loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(new_loop)
                            try:
                                parsed_doc = new_loop.run_until_complete(parser.parse(corrupted_file))
                            finally:
                                new_loop.close()

                        results['corrupted_file'] = {
                            'success': True,
                            'handled_gracefully': True,
                            'content_length': len(parsed_doc.content)
                        }
                    else:
                        results['corrupted_file'] = {
                            'success': False,
                            'error': 'No parser available'
                        }
                except Exception as e:
                    results['corrupted_file'] = {
                        'success': True,  # ì˜¤ë¥˜ê°€ ì ì ˆíˆ ì²˜ë¦¬ë¨
                        'handled_gracefully': True,
                        'error_handled': str(e)
                    }

            # 3. ì¡´ì¬í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í…ŒìŠ¤íŠ¸
            nonexistent_file = self.temp_dir / "nonexistent.txt"
            try:
                parser = parser_factory.get_parser(nonexistent_file)
                if parser:
                    import asyncio
                    try:
                        loop = asyncio.get_event_loop()
                        if loop.is_running():
                            parsed_doc = loop.run_until_complete(parser.parse(nonexistent_file))
                        else:
                            parsed_doc = asyncio.run(parser.parse(nonexistent_file))
                    except RuntimeError:
                        new_loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(new_loop)
                        try:
                            parsed_doc = new_loop.run_until_complete(parser.parse(nonexistent_file))
                        finally:
                            new_loop.close()

                    results['nonexistent_file'] = {
                        'success': False,
                        'unexpected_success': True
                    }
                else:
                    results['nonexistent_file'] = {
                        'success': True,
                        'handled_gracefully': True,
                        'error_handled': 'No parser for nonexistent file'
                    }
            except Exception as e:
                results['nonexistent_file'] = {
                    'success': True,  # ì˜¤ë¥˜ê°€ ì ì ˆíˆ ì²˜ë¦¬ë¨
                    'handled_gracefully': True,
                    'error_handled': str(e)
                }

            processing_time = time.time() - start_time
            handled_gracefully = sum(1 for r in results.values() if r.get('handled_gracefully', False))

            return TestResult(
                success=handled_gracefully > 0,
                message=f"ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {handled_gracefully}/{len(results)}ê°œ ì ì ˆíˆ ì²˜ë¦¬ë¨",
                details=results,
                processing_time=processing_time
            )

        except Exception as e:
            return TestResult(
                success=False,
                message="ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨",
                error=str(e),
                processing_time=time.time() - start_time
            )

    def test_performance_benchmark(self, files: Dict[str, Path]) -> TestResult:
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
        logger.info("=== ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
        start_time = time.time()

        results = {}

        try:
            from apps.ingestion.document_parser import get_parser_factory

            parser_factory = get_parser_factory()

            # ëŒ€ìš©ëŸ‰ íŒŒì¼ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            large_file = files.get('large')
            if large_file:
                file_start = time.time()
                try:
                    parser = parser_factory.get_parser(large_file)
                    if parser:
                        import asyncio
                        try:
                            loop = asyncio.get_event_loop()
                            if loop.is_running():
                                parsed_doc = loop.run_until_complete(parser.parse(large_file))
                            else:
                                parsed_doc = asyncio.run(parser.parse(large_file))
                        except RuntimeError:
                            new_loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(new_loop)
                            try:
                                parsed_doc = new_loop.run_until_complete(parser.parse(large_file))
                            finally:
                                new_loop.close()

                        file_time = time.time() - file_start

                        results['large_file'] = {
                            'success': True,
                            'file_size_chars': len(parsed_doc.content),
                            'processing_time': file_time,
                            'chars_per_second': len(parsed_doc.content) / file_time if file_time > 0 else 0,
                            'throughput_mb_per_sec': (len(parsed_doc.content.encode('utf-8')) / 1024 / 1024) / file_time if file_time > 0 else 0
                        }

                        logger.info(f"ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬: {len(parsed_doc.content):,} ë¬¸ì, {file_time:.3f}ì´ˆ")
                    else:
                        results['large_file'] = {
                            'success': False,
                            'error': 'No parser available'
                        }
                except Exception as e:
                    results['large_file'] = {
                        'success': False,
                        'error': str(e)
                    }

            # ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            batch_start = time.time()
            batch_results = {
                'total_files': 0,
                'successful_files': 0,
                'total_chars': 0,
                'total_time': 0
            }

            test_files = {k: v for k, v in files.items() if k not in ['corrupted', 'empty', 'large']}

            for file_type, file_path in test_files.items():
                file_start = time.time()
                try:
                    parser = parser_factory.get_parser(file_path)
                    if parser:
                        import asyncio
                        try:
                            loop = asyncio.get_event_loop()
                            if loop.is_running():
                                parsed_doc = loop.run_until_complete(parser.parse(file_path))
                            else:
                                parsed_doc = asyncio.run(parser.parse(file_path))
                        except RuntimeError:
                            new_loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(new_loop)
                            try:
                                parsed_doc = new_loop.run_until_complete(parser.parse(file_path))
                            finally:
                                new_loop.close()

                        batch_results['successful_files'] += 1
                        batch_results['total_chars'] += len(parsed_doc.content)

                except Exception as e:
                    logger.warning(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ {file_type} ì‹¤íŒ¨: {e}")

                batch_results['total_files'] += 1

            batch_time = time.time() - batch_start
            batch_results['total_time'] = batch_time

            results['batch_processing'] = {
                'success': batch_results['successful_files'] > 0,
                **batch_results,
                'success_rate': batch_results['successful_files'] / batch_results['total_files'] * 100 if batch_results['total_files'] > 0 else 0,
                'files_per_second': batch_results['total_files'] / batch_time if batch_time > 0 else 0,
                'chars_per_second': batch_results['total_chars'] / batch_time if batch_time > 0 else 0
            }

            logger.info(f"ë°°ì¹˜ ì²˜ë¦¬: {batch_results['successful_files']}/{batch_results['total_files']}ê°œ íŒŒì¼, {batch_time:.3f}ì´ˆ")

            processing_time = time.time() - start_time

            return TestResult(
                success=True,
                message="ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ",
                details=results,
                processing_time=processing_time
            )

        except Exception as e:
            return TestResult(
                success=False,
                message="ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨",
                error=str(e),
                processing_time=time.time() - start_time
            )

    def generate_final_report(self) -> Dict[str, Any]:
        """ìµœì¢… ë³´ê³ ì„œ ìƒì„±"""
        total_time = time.time() - self.start_time

        # ì „ì²´ í†µê³„ ê³„ì‚°
        total_tests = len(self.test_results)
        successful_tests = sum(1 for result in self.test_results.values() if result.success)

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        total_processing_time = sum(result.processing_time for result in self.test_results.values())

        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = []

        if successful_tests < total_tests:
            failed_tests = [name for name, result in self.test_results.items() if not result.success]
            recommendations.append(f"ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ë“¤ì„ ê²€í† í•˜ì„¸ìš”: {', '.join(failed_tests)}")

        if total_processing_time > 30:
            recommendations.append("ì „ì²´ ì²˜ë¦¬ ì‹œê°„ì´ 30ì´ˆë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì„±ëŠ¥ ìµœì í™”ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")

        # PII íƒì§€ ê²°ê³¼ í™•ì¸
        pii_result = self.test_results.get('pii_filtering')
        if pii_result and pii_result.success:
            total_pii = sum(
                details.get('detections_count', 0)
                for details in pii_result.details.values()
                if isinstance(details, dict)
            )
            if total_pii == 0:
                recommendations.append("PIIê°€ íƒì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. PII íŒ¨í„´ì„ í™•ì¸í•˜ì„¸ìš”.")
            elif total_pii > 20:
                recommendations.append(f"ë§ì€ PIIê°€ íƒì§€ë˜ì—ˆìŠµë‹ˆë‹¤ ({total_pii}ê°œ). ë°ì´í„° ë³´ì•ˆì„ ê°•í™”í•˜ì„¸ìš”.")

        if not recommendations:
            recommendations.append("ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤.")

        return {
            "summary": {
                "test_duration": total_time,
                "total_tests": total_tests,
                "successful_tests": successful_tests,
                "failed_tests": total_tests - successful_tests,
                "success_rate": (successful_tests / total_tests * 100) if total_tests > 0 else 0,
                "total_processing_time": total_processing_time
            },
            "test_results": {name: {
                "success": result.success,
                "message": result.message,
                "processing_time": result.processing_time,
                "error": result.error,
                "details": result.details
            } for name, result in self.test_results.items()},
            "recommendations": recommendations,
            "timestamp": datetime.now().isoformat()
        }

    def run_all_tests(self) -> Dict[str, Any]:
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸš€ ìµœì¢… ë¬¸ì„œ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")

        try:
            # í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
            test_files = self.create_test_files()

            # 1. íŒŒì¼ íŒŒì‹± í…ŒìŠ¤íŠ¸
            logger.info("1ï¸âƒ£ íŒŒì¼ íŒŒì‹± í…ŒìŠ¤íŠ¸")
            self.test_results['file_parsing'] = self.test_file_parsing(test_files)

            # 2. ì²­í‚¹ í…ŒìŠ¤íŠ¸
            logger.info("2ï¸âƒ£ ì²­í‚¹ í…ŒìŠ¤íŠ¸")
            self.test_results['chunking'] = self.test_chunking(test_files)

            # 3. PII í•„í„°ë§ í…ŒìŠ¤íŠ¸
            logger.info("3ï¸âƒ£ PII í•„í„°ë§ í…ŒìŠ¤íŠ¸")
            self.test_results['pii_filtering'] = self.test_pii_filtering(test_files)

            # 4. Mock ì„ë² ë”© í…ŒìŠ¤íŠ¸
            logger.info("4ï¸âƒ£ Mock ì„ë² ë”© í…ŒìŠ¤íŠ¸")
            self.test_results['embedding'] = self.test_mock_embedding(test_files)

            # 5. ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
            logger.info("5ï¸âƒ£ ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
            self.test_results['error_handling'] = self.test_error_handling(test_files)

            # 6. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
            logger.info("6ï¸âƒ£ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸")
            self.test_results['performance'] = self.test_performance_benchmark(test_files)

            logger.info("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            self.test_results['fatal_error'] = TestResult(
                success=False,
                message="ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ",
                error=str(e),
                details={'traceback': traceback.format_exc()}
            )

        return self.generate_final_report()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ“‹ Dynamic Taxonomy RAG - ë¬¸ì„œ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ ì¢…í•© í…ŒìŠ¤íŠ¸")
    print("=" * 70)

    tester = DocumentIngestionTester()

    try:
        # ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        report = tester.run_all_tests()

        # ê²°ê³¼ ì €ì¥
        report_file = Path("document_ingestion_final_report.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        # ê²°ê³¼ ì¶œë ¥
        print("\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("-" * 40)

        summary = report['summary']
        print(f"ğŸ• ì´ í…ŒìŠ¤íŠ¸ ì‹œê°„: {summary['test_duration']:.2f}ì´ˆ")
        print(f"ğŸ“ ì´ í…ŒìŠ¤íŠ¸ ìˆ˜: {summary['total_tests']}ê°œ")
        print(f"âœ… ì„±ê³µ: {summary['successful_tests']}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {summary['failed_tests']}ê°œ")
        print(f"ğŸ“ˆ ì„±ê³µë¥ : {summary['success_rate']:.1f}%")
        print(f"âš¡ ì²˜ë¦¬ ì‹œê°„: {summary['total_processing_time']:.2f}ì´ˆ")

        # ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼
        print(f"\nğŸ“‹ ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        for test_name, result in report['test_results'].items():
            status = "âœ…" if result['success'] else "âŒ"
            print(f"{status} {test_name}: {result['message']} ({result['processing_time']:.3f}ì´ˆ)")
            if result.get('error'):
                print(f"   ì˜¤ë¥˜: {result['error']}")

        # ê¶Œì¥ì‚¬í•­
        if report['recommendations']:
            print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
            for i, rec in enumerate(report['recommendations'], 1):
                print(f"{i}. {rec}")

        print(f"\nğŸ“„ ìƒì„¸ ë³´ê³ ì„œ: {report_file.absolute()}")

        # ì„±ê³µ ì—¬ë¶€ íŒë‹¨
        success_rate = summary['success_rate']
        if success_rate >= 90:
            print("\nğŸ‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ! ë¬¸ì„œ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.")
            return 0
        elif success_rate >= 70:
            print("\nâš ï¸ í…ŒìŠ¤íŠ¸ ë¶€ë¶„ ì„±ê³µ. ì¼ë¶€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            return 1
        else:
            print("\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ì‹¬ê°í•œ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
            return 2

    except Exception as e:
        print(f"\nğŸ’¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return 3

    finally:
        # ì •ë¦¬
        if hasattr(tester, 'temp_dir') and tester.temp_dir.exists():
            shutil.rmtree(tester.temp_dir, ignore_errors=True)

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)