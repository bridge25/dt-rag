{
  "dataset_name": "ai_rag_qa",
  "description": "AI/RAG domain questions and answers for evaluation",
  "version": "1.0",
  "created_at": "2025-09-19T14:00:00Z",
  "data_points": [
    {
      "id": "ai_rag_001",
      "query": "What is Retrieval-Augmented Generation (RAG)?",
      "expected_answer": "Retrieval-Augmented Generation (RAG) is a framework that combines information retrieval with text generation. It retrieves relevant documents from a knowledge base and uses them as context to generate more accurate and informed responses. RAG consists of two main components: a retriever that finds relevant documents and a generator that creates answers based on the retrieved context.",
      "expected_contexts": [
        "RAG is a hybrid approach that leverages both parametric and non-parametric knowledge",
        "The retriever component uses vector embeddings to find semantically similar documents",
        "The generator component is typically a large language model that conditions on retrieved context"
      ],
      "taxonomy_path": ["AI", "NLP", "RAG", "Architecture"],
      "difficulty_level": "easy",
      "domain": "AI/RAG",
      "metadata": {
        "topic": "basic_concepts",
        "expected_keywords": ["retrieval", "generation", "context", "knowledge base"]
      }
    },
    {
      "id": "ai_rag_002",
      "query": "How does vector search work in RAG systems?",
      "expected_answer": "Vector search in RAG systems works by converting text into high-dimensional vector embeddings that capture semantic meaning. Documents are pre-processed and embedded using models like BERT or sentence transformers. At query time, the query is also embedded, and similarity measures (typically cosine similarity) are used to find the most relevant documents. This enables semantic search that goes beyond keyword matching.",
      "expected_contexts": [
        "Vector embeddings represent text in high-dimensional space where semantically similar content clusters together",
        "Cosine similarity is commonly used to measure the angle between query and document vectors",
        "Modern embedding models like BERT, RoBERTa, or specialized sentence transformers create these representations"
      ],
      "taxonomy_path": ["AI", "Information Retrieval", "Vector Search", "Embeddings"],
      "difficulty_level": "medium",
      "domain": "AI/RAG",
      "metadata": {
        "topic": "technical_implementation",
        "expected_keywords": ["embeddings", "vector", "cosine similarity", "semantic search"]
      }
    },
    {
      "id": "ai_rag_003",
      "query": "What are the main components of a hybrid search system?",
      "expected_answer": "A hybrid search system typically combines three main components: (1) Lexical search (like BM25) that matches exact keywords and terms, (2) Semantic search using vector embeddings for meaning-based retrieval, and (3) A fusion mechanism that combines and ranks results from both approaches. Some systems also include a re-ranking component using cross-encoders for final result optimization.",
      "expected_contexts": [
        "BM25 is a probabilistic ranking function that scores documents based on term frequency and inverse document frequency",
        "Vector search captures semantic relationships that keyword search might miss",
        "Fusion techniques like Reciprocal Rank Fusion (RRF) or learned fusion combine different search modalities",
        "Cross-encoders can re-rank initial results by directly computing query-document interaction scores"
      ],
      "taxonomy_path": ["AI", "Information Retrieval", "Hybrid Search", "Components"],
      "difficulty_level": "medium",
      "domain": "AI/RAG",
      "metadata": {
        "topic": "system_architecture",
        "expected_keywords": ["BM25", "vector search", "fusion", "re-ranking", "cross-encoder"]
      }
    },
    {
      "id": "ai_rag_004",
      "query": "How do you evaluate RAG system performance?",
      "expected_answer": "RAG system performance is evaluated using multiple metrics: (1) Retrieval metrics like Precision@K, Recall@K, and NDCG measure how well relevant documents are found, (2) Generation quality metrics assess answer accuracy and relevance, (3) RAGAS framework provides specialized metrics like faithfulness, answer relevancy, context precision, and context recall, (4) End-to-end metrics evaluate the complete pipeline performance.",
      "expected_contexts": [
        "Retrieval evaluation focuses on whether the right documents are found and ranked properly",
        "Generation evaluation assesses whether answers are accurate, relevant, and well-grounded in retrieved context",
        "RAGAS (RAG Assessment) provides a comprehensive framework specifically designed for RAG evaluation",
        "Human evaluation and user satisfaction metrics provide additional quality insights"
      ],
      "taxonomy_path": ["AI", "RAG", "Evaluation", "Metrics"],
      "difficulty_level": "hard",
      "domain": "AI/RAG",
      "metadata": {
        "topic": "evaluation_methods",
        "expected_keywords": ["RAGAS", "precision", "recall", "faithfulness", "relevancy"]
      }
    },
    {
      "id": "ai_rag_005",
      "query": "What is faithfulness in RAGAS evaluation?",
      "expected_answer": "Faithfulness in RAGAS evaluation measures how well the generated answer is grounded in and supported by the retrieved context documents. It assesses whether the factual claims in the answer can be verified from the provided context, helping ensure that the RAG system doesn't hallucinate or generate information not present in the source documents. High faithfulness scores indicate that answers are well-supported by evidence.",
      "expected_contexts": [
        "Faithfulness prevents hallucination by ensuring answers are grounded in retrieved context",
        "It measures the proportion of claims in the answer that can be inferred from the context",
        "Low faithfulness indicates the model is generating unsupported information",
        "This metric is crucial for applications requiring factual accuracy and verifiability"
      ],
      "taxonomy_path": ["AI", "RAG", "Evaluation", "RAGAS", "Faithfulness"],
      "difficulty_level": "medium",
      "domain": "AI/RAG",
      "metadata": {
        "topic": "evaluation_metrics",
        "expected_keywords": ["faithfulness", "grounding", "hallucination", "context support"]
      }
    },
    {
      "id": "ai_rag_006",
      "query": "How does dynamic taxonomy classification improve RAG systems?",
      "expected_answer": "Dynamic taxonomy classification improves RAG systems by automatically organizing and categorizing content into hierarchical structures that evolve with the knowledge base. This enables more precise retrieval by allowing searches within specific categories, improves context relevance by understanding document topics, and supports better filtering and routing of queries to relevant content sections. The dynamic aspect allows the taxonomy to adapt as new content is added.",
      "expected_contexts": [
        "Taxonomies provide hierarchical organization that mirrors how humans categorize knowledge",
        "Dynamic classification adapts to new content patterns and domains automatically",
        "Taxonomic filtering can significantly improve retrieval precision by narrowing search scope",
        "Classification metadata can be used for routing queries to domain-specific retrieval strategies"
      ],
      "taxonomy_path": ["AI", "RAG", "Classification", "Dynamic Taxonomy"],
      "difficulty_level": "hard",
      "domain": "AI/RAG",
      "metadata": {
        "topic": "advanced_features",
        "expected_keywords": ["taxonomy", "classification", "hierarchical", "dynamic", "categorization"]
      }
    },
    {
      "id": "ai_rag_007",
      "query": "What are the benefits of caching in RAG systems?",
      "expected_answer": "Caching in RAG systems provides several benefits: (1) Improved response times by storing frequently accessed embeddings and search results, (2) Reduced computational costs by avoiding repeated embedding calculations, (3) Better scalability by handling more concurrent requests, (4) Enhanced user experience through faster responses, and (5) Reduced load on external APIs and embedding models. Cache strategies should balance memory usage with hit rates.",
      "expected_contexts": [
        "Embedding computation is often the most expensive part of RAG pipelines",
        "Query result caching can serve repeated or similar questions instantly",
        "Cache invalidation strategies ensure freshness when underlying documents change",
        "Distributed caching systems like Redis enable sharing cached data across multiple instances"
      ],
      "taxonomy_path": ["AI", "RAG", "Performance", "Caching"],
      "difficulty_level": "medium",
      "domain": "AI/RAG",
      "metadata": {
        "topic": "performance_optimization",
        "expected_keywords": ["caching", "performance", "response time", "scalability", "Redis"]
      }
    },
    {
      "id": "ai_rag_008",
      "query": "How do you handle document chunking for optimal RAG performance?",
      "expected_answer": "Optimal document chunking for RAG involves several strategies: (1) Size-based chunking (typically 200-1000 tokens) to balance context richness with processing speed, (2) Semantic chunking that preserves topic boundaries and meaningful units, (3) Overlapping chunks to maintain context continuity, (4) Metadata preservation to maintain document structure information, and (5) Adaptive chunking based on document type and content structure. The goal is maximizing relevant information density while maintaining coherence.",
      "expected_contexts": [
        "Chunk size affects both retrieval granularity and context window utilization",
        "Semantic boundaries help preserve complete thoughts and concepts within chunks",
        "Overlap between chunks ensures important information isn't lost at boundaries",
        "Different document types (papers, articles, documentation) may require different chunking strategies"
      ],
      "taxonomy_path": ["AI", "RAG", "Data Processing", "Chunking"],
      "difficulty_level": "hard",
      "domain": "AI/RAG",
      "metadata": {
        "topic": "data_processing",
        "expected_keywords": ["chunking", "semantic boundaries", "overlap", "token limits", "metadata"]
      }
    },
    {
      "id": "ai_rag_009",
      "query": "What is context precision in RAGAS and why is it important?",
      "expected_answer": "Context precision in RAGAS measures how many of the retrieved documents are actually relevant to answering the query. It evaluates the quality of the retrieval component by calculating the proportion of retrieved contexts that contain useful information for the specific question. High context precision means the retrieval system is good at filtering out irrelevant documents and focusing on truly useful content, which leads to better answer quality and reduced noise.",
      "expected_contexts": [
        "Context precision focuses on the quality rather than quantity of retrieved documents",
        "It helps identify when retrieval systems are returning too much irrelevant information",
        "Higher precision reduces noise that could confuse the generation model",
        "This metric is complementary to context recall, which measures coverage of relevant information"
      ],
      "taxonomy_path": ["AI", "RAG", "Evaluation", "RAGAS", "Context Precision"],
      "difficulty_level": "medium",
      "domain": "AI/RAG",
      "metadata": {
        "topic": "evaluation_metrics",
        "expected_keywords": ["context precision", "retrieval quality", "relevance", "noise reduction"]
      }
    },
    {
      "id": "ai_rag_010",
      "query": "How does answer relevancy differ from faithfulness in RAG evaluation?",
      "expected_answer": "Answer relevancy and faithfulness measure different aspects of RAG quality: Faithfulness evaluates whether the answer is grounded in and supported by the retrieved context documents, focusing on factual accuracy and avoiding hallucination. Answer relevancy evaluates whether the answer actually addresses the specific question asked, focusing on topical alignment and completeness. An answer can be faithful (well-grounded) but not relevant (off-topic), or relevant (on-topic) but not faithful (contains unsupported claims).",
      "expected_contexts": [
        "Faithfulness is about grounding in source material - does the context support the answer claims?",
        "Relevancy is about addressing the question - does the answer respond to what was actually asked?",
        "These metrics can diverge: a well-grounded answer might not address the specific question",
        "Both metrics are needed for comprehensive evaluation of RAG system quality"
      ],
      "taxonomy_path": ["AI", "RAG", "Evaluation", "RAGAS", "Metrics Comparison"],
      "difficulty_level": "hard",
      "domain": "AI/RAG",
      "metadata": {
        "topic": "evaluation_concepts",
        "expected_keywords": ["faithfulness", "relevancy", "grounding", "hallucination", "topical alignment"]
      }
    },
    {
      "id": "ai_rag_011",
      "query": "What role does monitoring play in production RAG systems?",
      "expected_answer": "Monitoring in production RAG systems is crucial for maintaining quality and performance. Key monitoring aspects include: (1) Performance metrics like response time, throughput, and error rates, (2) Quality metrics such as RAGAS scores tracked over time, (3) Resource utilization monitoring for scaling decisions, (4) User feedback and satisfaction tracking, (5) Drift detection for model and data quality degradation, and (6) Cost monitoring for API usage and computational resources. Effective monitoring enables proactive maintenance and continuous improvement.",
      "expected_contexts": [
        "Production RAG systems need real-time monitoring to detect quality degradation",
        "Performance monitoring helps identify bottlenecks and scaling needs",
        "Quality metrics should be tracked continuously to catch drift or regression",
        "User behavior and feedback provide valuable signals for system optimization"
      ],
      "taxonomy_path": ["AI", "RAG", "Production", "Monitoring"],
      "difficulty_level": "medium",
      "domain": "AI/RAG",
      "metadata": {
        "topic": "production_systems",
        "expected_keywords": ["monitoring", "performance", "quality tracking", "drift detection", "user feedback"]
      }
    },
    {
      "id": "ai_rag_012",
      "query": "How do you implement A/B testing for RAG system improvements?",
      "expected_answer": "A/B testing for RAG systems involves: (1) Defining clear metrics to compare (RAGAS scores, user satisfaction, response time), (2) Implementing traffic splitting to route queries to different system versions, (3) Ensuring statistical significance through proper sample sizing and duration, (4) Controlling for confounding variables like query complexity and user segments, (5) Collecting both automated metrics and human evaluation data, and (6) Analyzing results with statistical tests to determine significance. Careful experimental design is crucial for valid conclusions.",
      "expected_contexts": [
        "A/B testing allows objective comparison of different RAG system configurations",
        "Statistical significance testing ensures observed differences aren't due to chance",
        "Multiple metrics should be tracked to get a comprehensive view of system performance",
        "Human evaluation complements automated metrics for quality assessment"
      ],
      "taxonomy_path": ["AI", "RAG", "Evaluation", "A/B Testing"],
      "difficulty_level": "hard",
      "domain": "AI/RAG",
      "metadata": {
        "topic": "experimentation",
        "expected_keywords": ["A/B testing", "statistical significance", "traffic splitting", "metrics comparison"]
      }
    }
  ]
}