Understanding Vector Embeddings in RAG Systems

Vector embeddings are numerical representations of text that capture semantic meaning in a high-dimensional space.
In the DT-RAG system, we use 1536-dimensional embeddings generated by OpenAI's text-embedding-3-large model.

How Vector Embeddings Work:
- Text is converted into dense vector representations where similar concepts are positioned close together
- Cosine similarity measures the angle between vectors to determine semantic relatedness
- Embeddings enable semantic search beyond exact keyword matching

Benefits of Vector Search:
1. Semantic Understanding: Finds conceptually similar content even with different wording
2. Cross-lingual Capabilities: Similar concepts in different languages map to nearby vectors
3. Context Awareness: Considers surrounding context rather than isolated keywords
4. Efficient Retrieval: IVFFlat indexing enables fast approximate nearest neighbor search

Implementation in DT-RAG:
- Chunk Size: 500 tokens with 128-token overlap to preserve context boundaries
- Storage: PostgreSQL pgvector extension with vector(1536) data type
- Indexing: IVFFlat index with 100 lists for optimal performance on our data scale
- Distance Metric: Cosine distance (1 - cosine similarity) for ranking

The hybrid search approach combines vector similarity with BM25 keyword search,
then applies cross-encoder reranking to achieve superior retrieval quality.
