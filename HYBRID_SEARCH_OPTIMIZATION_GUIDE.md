# í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ ìµœì í™” ê°€ì´ë“œ

## ğŸ¯ ê°œìš”

ì´ ê°€ì´ë“œëŠ” BM25 + Vector + Cross-Encoder í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ìµœì í™”í•˜ëŠ” ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“š ëª©ì°¨

1. [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜)
2. [BM25 ìµœì í™”](#bm25-ìµœì í™”)
3. [ë²¡í„° ê²€ìƒ‰ ìµœì í™”](#ë²¡í„°-ê²€ìƒ‰-ìµœì í™”)
4. [í•˜ì´ë¸Œë¦¬ë“œ ìœµí•© ìµœì í™”](#í•˜ì´ë¸Œë¦¬ë“œ-ìœµí•©-ìµœì í™”)
5. [Cross-Encoder ë¦¬ë­í‚¹ ìµœì í™”](#cross-encoder-ë¦¬ë­í‚¹-ìµœì í™”)
6. [ìºì‹± ë° ì„±ëŠ¥ ìµœì í™”](#ìºì‹±-ë°-ì„±ëŠ¥-ìµœì í™”)
7. [ë°°í¬ ë° ìš´ì˜ ìµœì í™”](#ë°°í¬-ë°-ìš´ì˜-ìµœì í™”)

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ì „ì²´ ì›Œí¬í”Œë¡œìš°

```
Query Input
    â†“
Query Optimization â† ìºì‹œ ì¡°íšŒ
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BM25 Search â”‚ Vector Searchâ”‚ (ë³‘ë ¬ ì‹¤í–‰)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Hybrid Score Fusion
    â†“
Cross-Encoder Reranking
    â†“
Final Results â†’ ìºì‹œ ì €ì¥
```

### í•µì‹¬ ì»´í¬ë„ŒíŠ¸

- **BM25Engine**: í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
- **VectorEngine**: ì˜ë¯¸ì  ìœ ì‚¬ë„ ê²€ìƒ‰
- **HybridFusion**: ì ìˆ˜ ìœµí•© ë° ì •ê·œí™”
- **CrossEncoderReranker**: ì •ë°€ ë¦¬ë­í‚¹
- **SearchCache**: Redis/ë¡œì»¬ ìºì‹±
- **PerformanceMonitor**: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

## ğŸ” BM25 ìµœì í™”

### 1. íŒŒë¼ë¯¸í„° íŠœë‹

```python
# ê¸°ë³¸ ì„¤ì • (ë²”ìš©)
BM25_K1 = 1.5  # Term frequency saturation (1.2-2.0)
BM25_B = 0.75   # Document length normalization (0.0-1.0)

# ì§§ì€ ë¬¸ì„œìš© (chunk ê¸°ë°˜)
BM25_K1 = 1.8  # ë†’ì€ ê°’ìœ¼ë¡œ TF ì¤‘ìš”ë„ ì¦ê°€
BM25_B = 0.6   # ë¬¸ì„œ ê¸¸ì´ ì •ê·œí™” ê°ì†Œ

# ê¸´ ë¬¸ì„œìš©
BM25_K1 = 1.2  # ë‚®ì€ ê°’ìœ¼ë¡œ TF saturation ë¹ ë¥´ê²Œ
BM25_B = 0.9   # ë¬¸ì„œ ê¸¸ì´ ì •ê·œí™” ê°•í™”
```

### 2. ì¸ë±ìŠ¤ ìµœì í™”

#### PostgreSQL with GIN Index

```sql
-- Full-text search ì¸ë±ìŠ¤
CREATE INDEX CONCURRENTLY idx_chunks_text_gin
ON chunks USING gin(to_tsvector('english', text));

-- ë³µí•© ì¸ë±ìŠ¤ (í•„í„°ë§ + ê²€ìƒ‰)
CREATE INDEX CONCURRENTLY idx_chunks_composite
ON chunks(doc_id, to_tsvector('english', text))
WHERE text IS NOT NULL;

-- ì¸ë±ìŠ¤ í†µê³„ ì—…ë°ì´íŠ¸
ANALYZE chunks;
```

#### SQLite with FTS5

```sql
-- FTS5 ê°€ìƒ í…Œì´ë¸” ìƒì„±
CREATE VIRTUAL TABLE chunks_fts USING fts5(
    chunk_id,
    text,
    title,
    content='chunks',
    tokenize='porter'
);

-- ë°ì´í„° ë™ê¸°í™”
INSERT INTO chunks_fts(chunk_id, text, title)
SELECT chunk_id, text, title FROM chunks;
```

### 3. ì¿¼ë¦¬ ìµœì í™”

```python
def optimize_bm25_query(query: str) -> str:
    """BM25ìš© ì¿¼ë¦¬ ìµœì í™”"""
    # 1. ë¶ˆìš©ì–´ ì œê±° (ì„ íƒì )
    stopwords = {'the', 'a', 'an', 'and', 'or', 'but'}
    words = [w for w in query.split() if w.lower() not in stopwords]

    # 2. ë™ì˜ì–´ í™•ì¥
    expanded_words = []
    for word in words:
        expanded_words.append(word)
        synonyms = get_synonyms(word)  # ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜
        expanded_words.extend(synonyms[:2])  # ìµœëŒ€ 2ê°œ ë™ì˜ì–´

    return " ".join(expanded_words)
```

## ğŸ¯ ë²¡í„° ê²€ìƒ‰ ìµœì í™”

### 1. ì„ë² ë”© ëª¨ë¸ ì„ íƒ

```python
# ì„±ëŠ¥ vs í’ˆì§ˆ íŠ¸ë ˆì´ë“œì˜¤í”„
EMBEDDING_MODELS = {
    # ê³ ì„±ëŠ¥ (ë¹ ë¦„, ì‘ì€ ì°¨ì›)
    'sentence-transformers/all-MiniLM-L6-v2': {
        'dimensions': 384,
        'speed': 'fast',
        'quality': 'good'
    },

    # ê· í˜•
    'text-embedding-ada-002': {
        'dimensions': 1536,
        'speed': 'medium',
        'quality': 'excellent'
    },

    # ê³ í’ˆì§ˆ (ëŠë¦¼, í° ì°¨ì›)
    'sentence-transformers/all-mpnet-base-v2': {
        'dimensions': 768,
        'speed': 'slow',
        'quality': 'excellent'
    }
}
```

### 2. ë²¡í„° ì¸ë±ìŠ¤ ìµœì í™”

#### pgvector with HNSW

```sql
-- HNSW ì¸ë±ìŠ¤ (ë†’ì€ í’ˆì§ˆ)
CREATE INDEX CONCURRENTLY idx_chunks_embedding_hnsw
ON chunks USING hnsw (embedding vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

-- IVFFlat ì¸ë±ìŠ¤ (ë¹ ë¥¸ ì†ë„)
CREATE INDEX CONCURRENTLY idx_chunks_embedding_ivfflat
ON chunks USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);

-- ëŸ°íƒ€ì„ ì„¤ì •
SET hnsw.ef_search = 40;  -- ê²€ìƒ‰ ì •í™•ë„ vs ì†ë„
SET ivfflat.probes = 10;  -- íƒìƒ‰í•  í´ëŸ¬ìŠ¤í„° ìˆ˜
```

### 3. ì„ë² ë”© ìºì‹±

```python
class EmbeddingCache:
    async def get_or_create_embedding(self, text: str) -> np.ndarray:
        # 1. í…ìŠ¤íŠ¸ í•´ì‹œ ìƒì„±
        text_hash = hashlib.md5(text.encode()).hexdigest()

        # 2. ìºì‹œ ì¡°íšŒ
        cached = await self.cache.get(f"emb:{text_hash}")
        if cached:
            return np.frombuffer(cached, dtype=np.float32)

        # 3. ì„ë² ë”© ìƒì„±
        embedding = await self.embedding_service.generate(text)

        # 4. ìºì‹œ ì €ì¥ (24ì‹œê°„)
        await self.cache.setex(
            f"emb:{text_hash}",
            86400,
            embedding.tobytes()
        )

        return embedding
```

### 4. ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”

```python
async def batch_vector_search(
    self,
    queries: List[str],
    batch_size: int = 10
) -> List[List[Dict]]:
    """ë²¡í„° ê²€ìƒ‰ ë°°ì¹˜ ì²˜ë¦¬"""

    # 1. ì„ë² ë”© ë°°ì¹˜ ìƒì„±
    embeddings = await self.batch_generate_embeddings(queries)

    # 2. ë°°ì¹˜ ê²€ìƒ‰ ì‹¤í–‰
    results = []
    for i in range(0, len(embeddings), batch_size):
        batch_embeddings = embeddings[i:i + batch_size]
        batch_results = await self.batch_search_vectors(batch_embeddings)
        results.extend(batch_results)

    return results
```

## âš–ï¸ í•˜ì´ë¸Œë¦¬ë“œ ìœµí•© ìµœì í™”

### 1. ì ìˆ˜ ì •ê·œí™” ë°©ë²• ì„ íƒ

```python
# ìƒí™©ë³„ ì •ê·œí™” ë°©ë²•
NORMALIZATION_STRATEGIES = {
    # ì ìˆ˜ ë²”ìœ„ê°€ ë‹¤ë¥¼ ë•Œ
    'min_max': MinMaxNormalization,

    # ì ìˆ˜ ë¶„í¬ê°€ ì •ê·œë¶„í¬ì— ê°€ê¹Œìš¸ ë•Œ
    'z_score': ZScoreNormalization,

    # ìˆœìœ„ê°€ ì¤‘ìš”í•  ë•Œ
    'rank_based': RankBasedNormalization,

    # ìƒìœ„ ê²°ê³¼ì— ì§‘ì¤‘í•  ë•Œ
    'reciprocal_rank': ReciprocalRankNormalization
}
```

### 2. ìœµí•© ë°©ë²• ìµœì í™”

```python
def select_fusion_method(query_characteristics: Dict) -> FusionMethod:
    """ì¿¼ë¦¬ íŠ¹ì„±ì— ë”°ë¥¸ ìœµí•© ë°©ë²• ì„ íƒ"""

    # ì§§ì€ í‚¤ì›Œë“œ ì¿¼ë¦¬ â†’ BM25 ìš°ì„ 
    if query_characteristics['word_count'] <= 2:
        return FusionMethod.WEIGHTED_SUM  # BM25 ê°€ì¤‘ì¹˜ ë†’ê²Œ

    # ê¸´ ìì—°ì–´ ì¿¼ë¦¬ â†’ Vector ìš°ì„ 
    elif query_characteristics['word_count'] >= 6:
        return FusionMethod.RRF  # Reciprocal Rank Fusion

    # êµ¬ë¬¸ ê²€ìƒ‰ â†’ BM25 ìš°ì„ 
    elif query_characteristics['has_quotes']:
        return FusionMethod.WEIGHTED_SUM  # BM25 ê°€ì¤‘ì¹˜ ë†’ê²Œ

    # ê¸°ìˆ  ìš©ì–´ â†’ Vector ìš°ì„ 
    elif query_characteristics['has_technical_terms']:
        return FusionMethod.RRF

    # ê¸°ë³¸ê°’
    else:
        return FusionMethod.WEIGHTED_SUM
```

### 3. ê°€ì¤‘ì¹˜ ìë™ ì¡°ì •

```python
class AdaptiveWeighting:
    def __init__(self):
        self.performance_history = defaultdict(list)

    def adjust_weights(
        self,
        query: str,
        user_feedback: float
    ) -> Tuple[float, float]:
        """ì‚¬ìš©ì í”¼ë“œë°± ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì¡°ì •"""

        query_type = self.classify_query(query)

        # ì„±ëŠ¥ ì´ë ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°
        if query_type in self.performance_history:
            # ìµœê·¼ ì„±ëŠ¥ì´ ì¢‹ì•˜ë˜ ì„¤ì • ìš°ì„ 
            recent_performance = self.performance_history[query_type][-10:]
            best_config = max(recent_performance, key=lambda x: x['score'])
            return best_config['bm25_weight'], best_config['vector_weight']

        # ê¸°ë³¸ ê°€ì¤‘ì¹˜
        return 0.5, 0.5
```

## ğŸ­ Cross-Encoder ë¦¬ë­í‚¹ ìµœì í™”

### 1. ëª¨ë¸ ì„ íƒ

```python
# ì„±ëŠ¥ë³„ ëª¨ë¸ ì¶”ì²œ
RERANKING_MODELS = {
    # ë¹ ë¥¸ ì†ë„ (ì¶”ì²œ)
    'cross-encoder/ms-marco-MiniLM-L-6-v2': {
        'speed': 'fast',
        'quality': 'good',
        'max_length': 512
    },

    # ê· í˜•
    'cross-encoder/ms-marco-MiniLM-L-12-v2': {
        'speed': 'medium',
        'quality': 'excellent',
        'max_length': 512
    },

    # ìµœê³  í’ˆì§ˆ
    'cross-encoder/ms-marco-electra-base': {
        'speed': 'slow',
        'quality': 'best',
        'max_length': 512
    }
}
```

### 2. ë‹¤ë‹¨ê³„ ë¦¬ë­í‚¹

```python
async def multi_stage_rerank(
    self,
    query: str,
    candidates: List[Dict],
    final_k: int = 5
) -> List[Dict]:
    """3ë‹¨ê³„ ë¦¬ë­í‚¹"""

    # Stage 1: ë¹ ë¥¸ í•„í„°ë§ (50 â†’ 20)
    stage1 = await self.fast_rerank(query, candidates, 20)

    # Stage 2: Cross-encoder (20 â†’ 10)
    stage2 = await self.cross_encoder_rerank(query, stage1, 10)

    # Stage 3: ë‹¤ì–‘ì„± ë³´ì¥ (10 â†’ 5)
    final = self.diversity_rerank(stage2, final_k)

    return final
```

### 3. ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”

```python
class OptimizedCrossEncoder:
    def __init__(self, batch_size: int = 32):
        self.batch_size = batch_size
        self.model_cache = {}

    async def batch_rerank(
        self,
        query: str,
        candidates: List[Dict]
    ) -> List[Dict]:
        """ë°°ì¹˜ ë‹¨ìœ„ ë¦¬ë­í‚¹"""

        # ì¿¼ë¦¬-ë¬¸ì„œ ìŒ ìƒì„±
        pairs = [(query, doc['text'][:500]) for doc in candidates]

        # ë°°ì¹˜ ì²˜ë¦¬
        scores = []
        for i in range(0, len(pairs), self.batch_size):
            batch = pairs[i:i + self.batch_size]
            batch_scores = await self.model.predict(batch)
            scores.extend(batch_scores)

        # ì ìˆ˜ í• ë‹¹ ë° ì •ë ¬
        for doc, score in zip(candidates, scores):
            doc['rerank_score'] = float(score)

        return sorted(candidates, key=lambda x: x['rerank_score'], reverse=True)
```

## ğŸš€ ìºì‹± ë° ì„±ëŠ¥ ìµœì í™”

### 1. Redis ìºì‹± ì„¤ì •

```python
# Redis ì„¤ì • ìµœì í™”
REDIS_CONFIG = {
    'decode_responses': True,
    'health_check_interval': 30,
    'socket_keepalive': True,
    'socket_keepalive_options': {},
    'connection_pool_kwargs': {
        'max_connections': 50,
        'retry_on_timeout': True
    }
}

# ìºì‹œ TTL ì„¤ì •
CACHE_TTL = {
    'search_results': 3600,      # 1ì‹œê°„
    'embeddings': 86400 * 7,     # 1ì£¼ì¼
    'query_suggestions': 3600,   # 1ì‹œê°„
    'user_preferences': 86400    # 1ì¼
}
```

### 2. ë©”ëª¨ë¦¬ ìµœì í™”

```python
class MemoryOptimizedSearch:
    def __init__(self):
        # ì„ë² ë”© ì–‘ìí™” (ë©”ëª¨ë¦¬ 50% ì ˆì•½)
        self.use_quantization = True
        self.quantization_bits = 8

        # ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°
        self.enable_streaming = True
        self.stream_batch_size = 10

    async def quantize_embeddings(self, embeddings: np.ndarray) -> np.ndarray:
        """ì„ë² ë”© ì–‘ìí™”"""
        if self.use_quantization:
            # Float32 â†’ Int8 ë³€í™˜
            min_val, max_val = embeddings.min(), embeddings.max()
            scale = (max_val - min_val) / 255
            quantized = ((embeddings - min_val) / scale).astype(np.uint8)
            return quantized, scale, min_val
        return embeddings
```

### 3. ë¹„ë™ê¸° ìµœì í™”

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class AsyncOptimizedEngine:
    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.semaphore = asyncio.Semaphore(10)  # ë™ì‹œ ìš”ì²­ ì œí•œ

    async def search_with_concurrency_limit(
        self,
        session: AsyncSession,
        query: str
    ) -> SearchResponse:
        async with self.semaphore:
            # BM25ì™€ Vector ê²€ìƒ‰ ë³‘ë ¬ ì‹¤í–‰
            bm25_task = self.bm25_search(session, query)
            vector_task = self.vector_search(session, query)

            bm25_results, vector_results = await asyncio.gather(
                bm25_task, vector_task
            )

            # CPU ì§‘ì•½ì  ì‘ì—…ì€ ThreadPoolì—ì„œ ì‹¤í–‰
            fusion_task = asyncio.get_event_loop().run_in_executor(
                self.executor,
                self.fusion_engine.fuse_results,
                bm25_results,
                vector_results
            )

            fused_results = await fusion_task
            return fused_results
```

## ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”

### 1. ì—°ê²° í’€ ì„¤ì •

```python
# SQLAlchemy ì—°ê²° í’€ ìµœì í™”
ENGINE_CONFIG = {
    'pool_size': 20,           # ê¸°ë³¸ ì—°ê²° ìˆ˜
    'max_overflow': 30,        # ì¶”ê°€ ì—°ê²° ìˆ˜
    'pool_timeout': 30,        # ì—°ê²° ëŒ€ê¸° ì‹œê°„
    'pool_recycle': 3600,      # ì—°ê²° ì¬ì‚¬ìš© ì‹œê°„
    'pool_pre_ping': True,     # ì—°ê²° ìƒíƒœ í™•ì¸
}

# PostgreSQL ì „ìš© ì„¤ì •
POSTGRESQL_CONFIG = {
    'statement_timeout': 30000,    # 30ì´ˆ
    'lock_timeout': 10000,         # 10ì´ˆ
    'idle_in_transaction_session_timeout': 60000,  # 1ë¶„
}
```

### 2. ì¿¼ë¦¬ ìµœì í™”

```sql
-- ë³µí•© ì¸ë±ìŠ¤ í™œìš©
CREATE INDEX CONCURRENTLY idx_chunks_search_optimized
ON chunks(doc_id, LENGTH(text), to_tsvector('english', text))
WHERE text IS NOT NULL AND LENGTH(text) > 50;

-- íŒŒí‹°ì…”ë‹ (ëŒ€ìš©ëŸ‰ ë°ì´í„°)
CREATE TABLE chunks_partitioned (
    LIKE chunks INCLUDING ALL
) PARTITION BY RANGE (doc_id);

-- í†µê³„ ìµœì í™”
ALTER TABLE chunks ALTER COLUMN text SET STATISTICS 1000;
ANALYZE chunks;
```

### 3. ë°°ì¹˜ Insert ìµœì í™”

```python
async def bulk_insert_embeddings(
    session: AsyncSession,
    embeddings_data: List[Dict]
) -> None:
    """ë°°ì¹˜ ì„ë² ë”© ì‚½ì…"""

    # ë°°ì¹˜ í¬ê¸° ì„¤ì •
    batch_size = 1000

    for i in range(0, len(embeddings_data), batch_size):
        batch = embeddings_data[i:i + batch_size]

        # PostgreSQL COPY ì‚¬ìš©
        if "postgresql" in str(session.bind.url):
            await session.execute(
                text("""
                COPY chunks(chunk_id, text, embedding)
                FROM STDIN WITH CSV
                """),
                batch
            )
        else:
            # SQLite executemany ì‚¬ìš©
            await session.execute(
                insert(Chunk),
                batch
            )

        await session.commit()
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ì•ŒëŒ

### 1. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì 

```python
class SearchMetrics:
    def __init__(self):
        self.metrics = {
            'search_latency_p95': Histogram('search_latency_p95'),
            'search_throughput': Counter('search_requests_total'),
            'cache_hit_rate': Gauge('cache_hit_rate'),
            'error_rate': Gauge('search_error_rate'),
            'result_quality': Histogram('result_quality_score')
        }

    def record_search(
        self,
        latency: float,
        result_count: int,
        cache_hit: bool,
        error: bool = False
    ):
        self.metrics['search_latency_p95'].observe(latency)
        self.metrics['search_throughput'].inc()

        if cache_hit:
            self.metrics['cache_hit_rate'].inc()

        if error:
            self.metrics['error_rate'].inc()
```

### 2. ì•ŒëŒ ì„¤ì •

```python
# ì„±ëŠ¥ ì„ê³„ì¹˜
PERFORMANCE_THRESHOLDS = {
    'p95_latency_ms': 1000,      # 1ì´ˆ
    'error_rate_percent': 5,      # 5%
    'cache_hit_rate_percent': 70, # 70%
    'throughput_qps': 10          # 10 QPS
}

async def check_performance_alerts():
    """ì„±ëŠ¥ ì•ŒëŒ í™•ì¸"""
    metrics = get_current_metrics()

    alerts = []

    if metrics['p95_latency'] > PERFORMANCE_THRESHOLDS['p95_latency_ms']:
        alerts.append(f"High latency: {metrics['p95_latency']}ms")

    if metrics['error_rate'] > PERFORMANCE_THRESHOLDS['error_rate_percent']:
        alerts.append(f"High error rate: {metrics['error_rate']}%")

    if alerts:
        await send_alert_notification(alerts)
```

## ğŸš€ ë°°í¬ ìµœì í™”

### 1. Docker ìµœì í™”

```dockerfile
# Multi-stage build
FROM python:3.11-slim as builder
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

FROM python:3.11-slim
WORKDIR /app

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# Python íŒ¨í‚¤ì§€ ë³µì‚¬
COPY --from=builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ
COPY . .

# í™˜ê²½ ë³€ìˆ˜
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1

# í—¬ìŠ¤ì²´í¬
HEALTHCHECK --interval=30s --timeout=10s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 2. Kubernetes ì„¤ì •

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hybrid-search
spec:
  replicas: 3
  selector:
    matchLabels:
      app: hybrid-search
  template:
    metadata:
      labels:
        app: hybrid-search
    spec:
      containers:
      - name: hybrid-search
        image: hybrid-search:latest
        ports:
        - containerPort: 8000
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: url
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "2000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: hybrid-search-service
spec:
  selector:
    app: hybrid-search
  ports:
  - port: 80
    targetPort: 8000
  type: LoadBalancer
```

### 3. ì˜¤í† ìŠ¤ì¼€ì¼ë§

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: hybrid-search-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: hybrid-search
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

## ğŸ“ˆ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### ëª©í‘œ ì„±ëŠ¥ ì§€í‘œ

| ë©”íŠ¸ë¦­ | ëª©í‘œ ê°’ | ì„¤ëª… |
|--------|---------|------|
| **Recall@10** | â‰¥ 0.85 | ìƒìœ„ 10ê°œ ê²°ê³¼ ì¤‘ ê´€ë ¨ ë¬¸ì„œ ë¹„ìœ¨ |
| **P95 Latency** | â‰¤ 1s | 95%ì˜ ì¿¼ë¦¬ê°€ 1ì´ˆ ë‚´ ì™„ë£Œ |
| **Throughput** | â‰¥ 50 QPS | ì´ˆë‹¹ 50ê°œ ì¿¼ë¦¬ ì²˜ë¦¬ |
| **Cache Hit Rate** | â‰¥ 70% | ìºì‹œ ì ì¤‘ë¥  70% ì´ìƒ |
| **Error Rate** | â‰¤ 1% | ì˜¤ë¥˜ìœ¨ 1% ì´í•˜ |
| **Cost per Search** | â‰¤ â‚©3 | ê²€ìƒ‰ë‹¹ ë¹„ìš© 3ì› ì´í•˜ |

### ì‹¤ì œ ì„±ëŠ¥ ì¸¡ì •

```bash
# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python -m pytest tests/test_hybrid_search_performance.py -v

# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
python examples/benchmark_search_performance.py \
    --queries 1000 \
    --concurrent-users 10 \
    --duration 300

# ê²°ê³¼ ë¶„ì„
python scripts/analyze_performance_results.py \
    --results-file benchmark_results.json
```

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

#### 1. ë†’ì€ ì§€ì—°ì‹œê°„
```python
# ì›ì¸ ë¶„ì„
async def diagnose_latency():
    # BM25 vs Vector ì‹œê°„ ë¹„êµ
    # ì¸ë±ìŠ¤ ìƒíƒœ í™•ì¸
    # ìºì‹œ íˆíŠ¸ìœ¨ í™•ì¸
    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒíƒœ í™•ì¸
```

#### 2. ë‚®ì€ ì •í™•ë„
```python
# ê°œì„  ë°©ë²•
async def improve_accuracy():
    # BM25 íŒŒë¼ë¯¸í„° íŠœë‹
    # ì„ë² ë”© ëª¨ë¸ ë³€ê²½
    # ìœµí•© ê°€ì¤‘ì¹˜ ì¡°ì •
    # ë¦¬ë­í‚¹ ëª¨ë¸ ì—…ê·¸ë ˆì´ë“œ
```

#### 3. ë©”ëª¨ë¦¬ ë¶€ì¡±
```python
# ë©”ëª¨ë¦¬ ìµœì í™”
async def optimize_memory():
    # ì„ë² ë”© ì–‘ìí™”
    # ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ
    # ìºì‹œ í¬ê¸° ì¡°ì •
    # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìµœì í™”
```

## ğŸ“š ì¶”ê°€ ìë£Œ

- [BM25 ì•Œê³ ë¦¬ì¦˜ ìƒì„¸ ì„¤ëª…](https://en.wikipedia.org/wiki/Okapi_BM25)
- [pgvector ê³µì‹ ë¬¸ì„œ](https://github.com/pgvector/pgvector)
- [Sentence Transformers ëª¨ë¸ ëª©ë¡](https://www.sbert.net/docs/pretrained_models.html)
- [Cross-Encoder ëª¨ë¸ í—ˆë¸Œ](https://huggingface.co/cross-encoder)

---

## ğŸ‰ ê²°ë¡ 

ì´ ê°€ì´ë“œë¥¼ ë”°ë¼ êµ¬í˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- **ê²€ìƒ‰ ì •í™•ë„**: Recall@10 â‰¥ 85%
- **ê²€ìƒ‰ ì†ë„**: P95 â‰¤ 1ì´ˆ
- **ì²˜ë¦¬ëŸ‰**: 50+ QPS
- **ë¹„ìš© íš¨ìœ¨ì„±**: â‰¤ â‚©3/ê²€ìƒ‰

ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ê³¼ ìµœì í™”ë¥¼ í†µí•´ ë”ìš± í–¥ìƒëœ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.