#!/usr/bin/env python3
"""
ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤íŠ¸
Redis ìºì‹±, ì„±ëŠ¥ ë©”íŠ¸ë¦­, í—¬ìŠ¤ ì²´í¬, ëŒ€ì‹œë³´ë“œ í…ŒìŠ¤íŠ¸
"""

import asyncio
import json
import time
import logging
from pathlib import Path
import sys
import os

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root / "apps"))

from apps.api.monitoring.metrics import MetricsCollector, PerformanceSnapshot
from apps.api.monitoring.health_check import HealthChecker
from apps.api.monitoring.dashboard import MonitoringDashboard
from apps.api.cache.redis_manager import RedisManager, RedisConfig

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class MonitoringSystemTester:
    """ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤í„°"""

    def __init__(self):
        self.results = {}
        self.redis_enabled = os.getenv('REDIS_ENABLED', 'false').lower() == 'true'

    async def test_metrics_collector(self) -> dict:
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸° í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ§ª Testing Metrics Collector...")
        test_results = {
            "test_name": "metrics_collector",
            "success": False,
            "details": {}
        }

        try:
            # ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
            collector = MetricsCollector(enable_prometheus=True)

            # ê¸°ë³¸ ë©”íŠ¸ë¦­ ê¸°ë¡
            collector.increment_counter("test_requests", {"endpoint": "/test"})
            collector.record_latency("test_operation", 123.45, {"type": "integration"})
            collector.set_gauge("test_cpu_usage", 45.6)

            # ì„±ëŠ¥ ì¶”ì  í…ŒìŠ¤íŠ¸
            async with collector.track_operation("test_operation", {"test": "true"}):
                await asyncio.sleep(0.1)  # 100ms ì‹œë®¬ë ˆì´ì…˜

            # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            collector.update_system_metrics()

            # ì„±ëŠ¥ ìŠ¤ëƒ…ìƒ· ìƒì„±
            snapshot = collector.calculate_performance_snapshot()
            test_results["details"]["snapshot"] = {
                "p95_latency": snapshot.p95_latency,
                "total_requests": snapshot.total_requests,
                "cpu_usage": snapshot.cpu_usage
            }

            # ë©”íŠ¸ë¦­ ìš”ì•½ ì¡°íšŒ
            summary = collector.get_metrics_summary()
            test_results["details"]["summary"] = {
                "uptime_seconds": summary["uptime_seconds"],
                "prometheus_enabled": summary["prometheus_enabled"],
                "metrics_count": summary["total_metrics_collected"]
            }

            # Prometheus ë©”íŠ¸ë¦­ ìµìŠ¤í¬íŠ¸ í…ŒìŠ¤íŠ¸
            if collector.enable_prometheus:
                prometheus_data = collector.export_prometheus_metrics()
                test_results["details"]["prometheus_export"] = len(prometheus_data) > 0

            test_results["success"] = True
            logger.info("âœ… Metrics Collector test passed")

        except Exception as e:
            test_results["details"]["error"] = str(e)
            logger.error(f"âŒ Metrics Collector test failed: {e}")

        return test_results

    async def test_health_checker(self) -> dict:
        """í—¬ìŠ¤ ì²´ì»¤ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ§ª Testing Health Checker...")
        test_results = {
            "test_name": "health_checker",
            "success": False,
            "details": {}
        }

        try:
            # í—¬ìŠ¤ ì²´ì»¤ ì´ˆê¸°í™”
            health_checker = HealthChecker()

            # ê°œë³„ ì»´í¬ë„ŒíŠ¸ í—¬ìŠ¤ ì²´í¬
            system_health = await health_checker.check_single_component("system")
            database_health = await health_checker.check_single_component("database")
            storage_health = await health_checker.check_single_component("storage")

            test_results["details"]["components"] = {
                "system": {
                    "status": system_health.status.value,
                    "response_time_ms": system_health.response_time_ms
                },
                "database": {
                    "status": database_health.status.value,
                    "response_time_ms": database_health.response_time_ms
                },
                "storage": {
                    "status": storage_health.status.value,
                    "response_time_ms": storage_health.response_time_ms
                }
            }

            # ì „ì²´ ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬
            full_health = await health_checker.get_system_health()
            test_results["details"]["overall"] = {
                "status": full_health.overall_status.value,
                "uptime_seconds": full_health.uptime_seconds,
                "component_count": len(full_health.components)
            }

            # ìºì‹œëœ í—¬ìŠ¤ ì •ë³´ ì¡°íšŒ
            cached_health = health_checker.get_cached_health()
            test_results["details"]["cached_available"] = "last_check" in cached_health

            test_results["success"] = True
            logger.info("âœ… Health Checker test passed")

        except Exception as e:
            test_results["details"]["error"] = str(e)
            logger.error(f"âŒ Health Checker test failed: {e}")

        return test_results

    async def test_redis_manager(self) -> dict:
        """Redis ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ§ª Testing Redis Manager...")
        test_results = {
            "test_name": "redis_manager",
            "success": False,
            "details": {}
        }

        try:
            # Redis ì„¤ì •
            config = RedisConfig(
                host="localhost",
                port=6379,
                db=0,
                enable_compression=True,
                compression_threshold=100
            )

            # Redis ë§¤ë‹ˆì € ì´ˆê¸°í™”
            redis_manager = RedisManager(config)
            connected = await redis_manager.initialize()

            test_results["details"]["connection"] = connected

            if connected:
                # ê¸°ë³¸ SET/GET í…ŒìŠ¤íŠ¸
                test_key = "test_monitoring_key"
                test_value = {
                    "timestamp": time.time(),
                    "data": "test monitoring data",
                    "numbers": list(range(100))  # ì••ì¶• í…ŒìŠ¤íŠ¸ìš©
                }

                # ê°’ ì €ì¥
                set_success = await redis_manager.set(test_key, test_value, ttl=60)
                test_results["details"]["set_operation"] = set_success

                # ê°’ ì¡°íšŒ
                retrieved_value = await redis_manager.get(test_key)
                test_results["details"]["get_operation"] = retrieved_value is not None
                test_results["details"]["data_integrity"] = retrieved_value == test_value

                # TTL í™•ì¸
                ttl_value = await redis_manager.ttl(test_key)
                test_results["details"]["ttl_check"] = 0 < ttl_value <= 60

                # í—¬ìŠ¤ ì²´í¬
                health_info = await redis_manager.health_check()
                test_results["details"]["health_check"] = health_info

                # ì„±ëŠ¥ í†µê³„
                stats = redis_manager.get_stats()
                test_results["details"]["stats"] = stats

                # ì •ë¦¬
                await redis_manager.delete(test_key)
                await redis_manager.close()

                test_results["success"] = True
                logger.info("âœ… Redis Manager test passed")
            else:
                test_results["details"]["message"] = "Redis not available - skipping tests"
                test_results["success"] = not self.redis_enabled  # ë¹„í™œì„±í™”ëœ ê²½ìš° ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
                if self.redis_enabled:
                    logger.warning("âš ï¸ Redis Manager test skipped (Redis not available)")
                else:
                    logger.info("â„¹ï¸ Redis Manager test skipped (Redis disabled)")

        except Exception as e:
            test_results["details"]["error"] = str(e)
            logger.error(f"âŒ Redis Manager test failed: {e}")

        return test_results

    async def test_monitoring_dashboard(self) -> dict:
        """ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ§ª Testing Monitoring Dashboard...")
        test_results = {
            "test_name": "monitoring_dashboard",
            "success": False,
            "details": {}
        }

        try:
            # ëŒ€ì‹œë³´ë“œ ì´ˆê¸°í™”
            dashboard = MonitoringDashboard()

            # ì „ì²´ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì¡°íšŒ
            dashboard_data = await dashboard.get_dashboard_data()
            test_results["details"]["dashboard_data"] = {
                "has_system_health": "system_health" in dashboard_data,
                "has_current_metrics": "current_metrics" in dashboard_data,
                "has_alerts": "alerts" in dashboard_data,
                "alert_count": len(dashboard_data.get("alerts", []))
            }

            # ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì¡°íšŒ
            realtime_metrics = await dashboard.get_real_time_metrics()
            test_results["details"]["realtime_metrics"] = {
                "has_performance": "performance" in realtime_metrics,
                "has_system": "system" in realtime_metrics,
                "timestamp": realtime_metrics.get("timestamp")
            }

            # ê²€ìƒ‰ ë¶„ì„ ë°ì´í„°
            search_analytics = await dashboard.get_search_analytics()
            test_results["details"]["search_analytics"] = {
                "has_cache_performance": "cache_performance" in search_analytics,
                "has_search_metrics": "search_metrics" in search_analytics
            }

            # ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
            performance_report = await dashboard.get_performance_report(hours=1)
            test_results["details"]["performance_report"] = {
                "status": performance_report.get("status"),
                "has_slo_compliance": "slo_compliance" in performance_report,
                "has_recommendations": "recommendations" in performance_report
            }

            # ë©”íŠ¸ë¦­ ë°ì´í„° ìµìŠ¤í¬íŠ¸
            json_export = await dashboard.export_metrics_data("json")
            test_results["details"]["export"] = {
                "json_export_length": len(json_export),
                "valid_json": True  # JSON íŒŒì‹±ì´ ì„±ê³µí–ˆë‹¤ë©´ ìœ íš¨í•¨
            }

            test_results["success"] = True
            logger.info("âœ… Monitoring Dashboard test passed")

        except Exception as e:
            test_results["details"]["error"] = str(e)
            logger.error(f"âŒ Monitoring Dashboard test failed: {e}")

        return test_results

    async def test_cache_integration(self) -> dict:
        """ìºì‹œ í†µí•© í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ§ª Testing Cache Integration...")
        test_results = {
            "test_name": "cache_integration",
            "success": False,
            "details": {}
        }

        try:
            from apps.api.cache.search_cache import HybridSearchCache, CacheConfig

            # ìºì‹œ ì„¤ì •
            cache_config = CacheConfig(
                max_memory_entries=100,
                memory_ttl_seconds=300,
                redis_ttl_seconds=3600,
                enable_compression=True
            )

            # í•˜ì´ë¸Œë¦¬ë“œ ìºì‹œ ì´ˆê¸°í™”
            cache = HybridSearchCache(cache_config)

            # ê²€ìƒ‰ ê²°ê³¼ ìºì‹± í…ŒìŠ¤íŠ¸
            test_query = "ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"
            test_results_data = [
                {"chunk_id": "test_1", "score": 0.95, "text": "í…ŒìŠ¤íŠ¸ ê²°ê³¼ 1"},
                {"chunk_id": "test_2", "score": 0.88, "text": "í…ŒìŠ¤íŠ¸ ê²°ê³¼ 2"}
            ]

            # ìºì‹œì— ì €ì¥
            await cache.set_search_results(
                query=test_query,
                results=test_results_data,
                filters={"category": "test"}
            )

            # ìºì‹œì—ì„œ ì¡°íšŒ
            cached_results = await cache.get_search_results(
                query=test_query,
                filters={"category": "test"}
            )

            test_results["details"]["cache_hit"] = cached_results is not None
            test_results["details"]["data_integrity"] = cached_results == test_results_data

            # ì„ë² ë”© ìºì‹± í…ŒìŠ¤íŠ¸
            test_embedding = [0.1, 0.2, 0.3, 0.4, 0.5] * 100  # ë” í° ì„ë² ë”©
            await cache.set_embedding("í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸", test_embedding, "test_model")

            cached_embedding = await cache.get_embedding("í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸", "test_model")
            test_results["details"]["embedding_cache"] = cached_embedding == test_embedding

            # ìºì‹œ í†µê³„
            cache_stats = await cache.get_cache_stats()
            test_results["details"]["cache_stats"] = {
                "has_hit_rates": "hit_rates" in cache_stats,
                "has_operations": "operations" in cache_stats,
                "memory_cache_entries": cache_stats.get("memory_cache", {}).get("entries", 0)
            }

            # ìºì‹œ ë¬´íš¨í™” í…ŒìŠ¤íŠ¸
            await cache.invalidate_search_cache()
            invalidated_results = await cache.get_search_results(
                query=test_query,
                filters={"category": "test"}
            )
            test_results["details"]["cache_invalidation"] = invalidated_results is None

            test_results["success"] = True
            logger.info("âœ… Cache Integration test passed")

        except Exception as e:
            test_results["details"]["error"] = str(e)
            logger.error(f"âŒ Cache Integration test failed: {e}")

        return test_results

    async def test_performance_targets(self) -> dict:
        """ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ§ª Testing Performance Targets...")
        test_results = {
            "test_name": "performance_targets",
            "success": False,
            "details": {}
        }

        try:
            # ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°ë¡œ ì„±ëŠ¥ ì‹œë®¬ë ˆì´ì…˜
            collector = MetricsCollector()

            # ë‹¤ì–‘í•œ ì§€ì—°ì‹œê°„ìœ¼ë¡œ ìš”ì²­ ì‹œë®¬ë ˆì´ì…˜
            latencies = [50, 80, 120, 150, 200, 300, 500, 800, 1200]  # ms
            for i, latency_ms in enumerate(latencies):
                collector.record_latency("simulated_request", latency_ms, {"request_id": str(i)})
                collector.increment_counter("requests_total")
                if latency_ms < 1000:
                    collector.increment_counter("requests_success")
                else:
                    collector.increment_counter("requests_error")

            # ìºì‹œ íˆíŠ¸/ë¯¸ìŠ¤ ì‹œë®¬ë ˆì´ì…˜
            for i in range(100):
                if i < 75:  # 75% íˆíŠ¸ìœ¨
                    collector.increment_counter("cache_hits")
                else:
                    collector.increment_counter("cache_misses")

            # ì„±ëŠ¥ ìŠ¤ëƒ…ìƒ· ìƒì„±
            snapshot = collector.calculate_performance_snapshot()

            # ì„±ëŠ¥ ëª©í‘œ ì²´í¬
            targets = {
                "p95_latency_ms": 1000,  # ëª©í‘œ: 1ì´ˆ ì´í•˜
                "cache_hit_rate_percent": 70,  # ëª©í‘œ: 70% ì´ìƒ
                "error_rate_percent": 1,  # ëª©í‘œ: 1% ì´í•˜
            }

            # ì‹¤ì œ ì„±ëŠ¥ ê³„ì‚°
            error_rate = (snapshot.failed_requests / max(snapshot.total_requests, 1)) * 100

            performance_results = {
                "p95_latency_ms": snapshot.p95_latency,
                "cache_hit_rate_percent": snapshot.cache_hit_rate,
                "error_rate_percent": error_rate,
                "qps": snapshot.qps
            }

            # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€ í™•ì¸
            targets_met = {
                "p95_latency": snapshot.p95_latency <= targets["p95_latency_ms"],
                "cache_hit_rate": snapshot.cache_hit_rate >= targets["cache_hit_rate_percent"],
                "error_rate": error_rate <= targets["error_rate_percent"]
            }

            test_results["details"] = {
                "targets": targets,
                "actual_performance": performance_results,
                "targets_met": targets_met,
                "overall_success": all(targets_met.values())
            }

            test_results["success"] = True
            logger.info("âœ… Performance Targets test completed")

            # ê²°ê³¼ ë¡œê¹…
            for metric, met in targets_met.items():
                status = "âœ…" if met else "âŒ"
                logger.info(f"{status} {metric}: {met}")

        except Exception as e:
            test_results["details"]["error"] = str(e)
            logger.error(f"âŒ Performance Targets test failed: {e}")

        return test_results

    async def run_all_tests(self) -> dict:
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸš€ Starting Monitoring System Comprehensive Tests")
        start_time = time.time()

        # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        tests = [
            self.test_metrics_collector(),
            self.test_health_checker(),
            self.test_redis_manager(),
            self.test_cache_integration(),
            self.test_monitoring_dashboard(),
            self.test_performance_targets()
        ]

        results = await asyncio.gather(*tests, return_exceptions=True)

        # ê²°ê³¼ ì¢…í•©
        test_summary = {
            "test_suite": "monitoring_system_comprehensive",
            "timestamp": time.time(),
            "duration_seconds": time.time() - start_time,
            "total_tests": len(tests),
            "passed_tests": 0,
            "failed_tests": 0,
            "test_results": []
        }

        for result in results:
            if isinstance(result, Exception):
                test_summary["test_results"].append({
                    "test_name": "unknown",
                    "success": False,
                    "details": {"error": str(result)}
                })
                test_summary["failed_tests"] += 1
            else:
                test_summary["test_results"].append(result)
                if result["success"]:
                    test_summary["passed_tests"] += 1
                else:
                    test_summary["failed_tests"] += 1

        test_summary["success_rate"] = (test_summary["passed_tests"] / test_summary["total_tests"]) * 100
        test_summary["overall_success"] = test_summary["failed_tests"] == 0

        return test_summary

    def save_results(self, results: dict, filename: str = None):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥"""
        if filename is None:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f"monitoring_system_test_results_{timestamp}.json"

        output_path = Path(filename)

        def json_serializer(obj):
            """JSON ì§ë ¬í™”ë¥¼ ìœ„í•œ ì»¤ìŠ¤í…€ ì§ë ¬í™” í•¨ìˆ˜"""
            if hasattr(obj, 'isoformat'):  # datetime ê°ì²´
                return obj.isoformat()
            elif hasattr(obj, '__dict__'):  # ì¼ë°˜ ê°ì²´
                return str(obj)
            else:
                return str(obj)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=json_serializer)

        logger.info(f"Test results saved to: {output_path.absolute()}")
        return output_path

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=" * 60)
    print("Dynamic Taxonomy RAG v1.8.1 - Monitoring System Test")
    print("=" * 60)

    tester = MonitoringSystemTester()

    try:
        # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        results = await tester.run_all_tests()

        # ê²°ê³¼ ì¶œë ¥
        print(f"\nTest Summary:")
        print(f"  Total Tests: {results['total_tests']}")
        print(f"  Passed: {results['passed_tests']}")
        print(f"  Failed: {results['failed_tests']}")
        print(f"  Success Rate: {results['success_rate']:.1f}%")
        print(f"  Duration: {results['duration_seconds']:.2f}s")

        # ê°œë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼
        print(f"\nIndividual Test Results:")
        for test_result in results['test_results']:
            status = "PASS" if test_result['success'] else "FAIL"
            print(f"  [{status}] {test_result['test_name']}")

        # ê²°ê³¼ ì €ì¥
        output_file = tester.save_results(results)

        # ì¢…í•© íŒì •
        if results['overall_success']:
            print(f"\nAll monitoring system tests passed successfully!")
            print(f"System is ready for production monitoring!")
        else:
            print(f"\nSome tests failed. Check the detailed results in {output_file}")

        return 0 if results['overall_success'] else 1

    except Exception as e:
        logger.error(f"ğŸ’¥ Test execution failed: {e}")
        return 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)